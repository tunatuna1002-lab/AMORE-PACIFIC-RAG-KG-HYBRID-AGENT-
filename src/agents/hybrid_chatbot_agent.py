"""
Hybrid Chatbot Agent
Ontology-RAG í•˜ì´ë¸Œë¦¬ë“œ ì±—ë´‡ ì—ì´ì „íŠ¸

Flow:
1. ì¿¼ë¦¬ì—ì„œ ì—”í‹°í‹°/ì˜ë„ ì¶”ì¶œ
2. Knowledge Graphì—ì„œ ê´€ë ¨ ì‚¬ì‹¤ ì¡°íšŒ
3. Ontology Reasonerë¡œ ì¶”ë¡ 
4. RAGë¡œ ê°€ì´ë“œë¼ì¸ ê²€ìƒ‰
5. í†µí•© ì»¨í…ìŠ¤íŠ¸ë¡œ LLM ì‘ë‹µ ìƒì„±
"""

import logging
from datetime import datetime
from typing import Any

from litellm import acompletion

from src.domain.entities.relations import InferenceResult
from src.memory.context import ContextManager
from src.monitoring.logger import AgentLogger
from src.monitoring.metrics import QualityMetrics
from src.monitoring.tracer import ExecutionTracer
from src.ontology.business_rules import register_all_rules
from src.ontology.knowledge_graph import KnowledgeGraph
from src.ontology.reasoner import OntologyReasoner
from src.rag.context_builder import CompactContextBuilder, ContextBuilder
from src.rag.hybrid_retriever import HybridContext, HybridRetriever
from src.rag.query_rewriter import QueryRewriter, RewriteResult, create_rewrite_result_no_change
from src.rag.retriever import DocumentRetriever
from src.rag.router import QueryType, RAGRouter
from src.rag.templates import ResponseTemplates

logger = logging.getLogger(__name__)


class HybridChatbotAgent:
    """
    Ontology-RAG í•˜ì´ë¸Œë¦¬ë“œ ì±—ë´‡ ì—ì´ì „íŠ¸
    Implements ChatbotAgentProtocol (src.domain.interfaces.chatbot)

    ê¸°ì¡´ ChatbotAgentì™€ì˜ ì°¨ì´ì :
    - ì˜¨í†¨ë¡œì§€ ì¶”ë¡  ê²°ê³¼ ê¸°ë°˜ ì‘ë‹µ
    - ì§€ì‹ ê·¸ë˜í”„ì—ì„œ ê´€ë ¨ ì‚¬ì‹¤ ì¡°íšŒ
    - ì¶”ë¡  ê³¼ì • ì„¤ëª… ì œê³µ

    ì‚¬ìš© ì˜ˆ:
        agent = HybridChatbotAgent()
        result = await agent.chat("LANEIGE Lip Care ê²½ìŸë ¥ ë¶„ì„í•´ì¤˜")
    """

    # ì„¤ì • íŒŒì¼ ê²½ë¡œ
    CONFIG_PATH = "config/thresholds.json"

    # ë¸Œëœë“œ ì •ê·œí™” ë§¤í•‘ (ì˜ë¦° ë¸Œëœë“œëª… â†’ ì „ì²´ ë¸Œëœë“œëª…)
    BRAND_NORMALIZATION = {
        "burt's": "Burt's Bees",
        "wet": "wet n wild",
        "tree": "Tree Hut",
        "clean": "Clean Skin Club",
        "summer": "Summer Fridays",
        "rare": "Rare Beauty",
        "la": "La Roche-Posay",
        "beauty": "Beauty of Joseon",
        "tower": "Tower 28",
        "drunk": "Drunk Elephant",
        "paula's": "Paula's Choice",
        "the": "The Ordinary",
        "glow": "Glow Recipe",
        "youth": "Youth To The People",
        "first": "First Aid Beauty",
        "charlotte": "Charlotte Tilbury",
        "too": "Too Faced",
        "urban": "Urban Decay",
        "fenty": "Fenty Beauty",
        "huda": "Huda Beauty",
        "anastasia": "Anastasia Beverly Hills",
        "physicians": "Physicians Formula",
        "covergirl": "COVERGIRL",
        "medicube": "MEDICUBE",
    }

    @classmethod
    def _load_config(cls) -> dict:
        """ì„¤ì • íŒŒì¼ì—ì„œ chatbot ê´€ë ¨ ì„¤ì • ë¡œë“œ"""
        import json
        from pathlib import Path

        project_root = Path(__file__).parent.parent.parent
        config_path = project_root / cls.CONFIG_PATH

        if config_path.exists():
            try:
                with open(config_path, encoding="utf-8") as f:
                    config = json.load(f)
                    return config.get("system", {}).get("chatbot", {})
            except Exception:
                logger.warning("Suppressed Exception", exc_info=True)

        return {}  # ì„¤ì • ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©

    def __init__(
        self,
        model: str = None,
        docs_dir: str = ".",
        knowledge_graph: KnowledgeGraph | None = None,
        reasoner: OntologyReasoner | None = None,
        logger: AgentLogger | None = None,
        tracer: ExecutionTracer | None = None,
        metrics: QualityMetrics | None = None,
        context_manager: ContextManager | None = None,
    ):
        """
        Args:
            model: LLM ëª¨ë¸ëª… (Noneì´ë©´ ì„¤ì • íŒŒì¼ì—ì„œ ë¡œë“œ)
            docs_dir: RAG ë¬¸ì„œ ë””ë ‰í† ë¦¬
            knowledge_graph: ì§€ì‹ ê·¸ë˜í”„ (ê³µìœ  ê°€ëŠ¥)
            reasoner: ì¶”ë¡ ê¸° (ê³µìœ  ê°€ëŠ¥)
            logger: ë¡œê±°
            tracer: ì¶”ì ê¸°
            metrics: ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸°
            context_manager: ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ì
        """
        import os

        # ì„¤ì • íŒŒì¼ì—ì„œ chatbot ì„¤ì • ë¡œë“œ
        config = self._load_config()
        self.model = model or config.get("model", "gpt-4.1-mini")

        # Temperature: ì±—ë´‡ ì „ìš© í™˜ê²½ë³€ìˆ˜ > ì¼ë°˜ í™˜ê²½ë³€ìˆ˜ > ì„¤ì •íŒŒì¼ > ê¸°ë³¸ê°’(0.4)
        # ì±—ë´‡ì€ ì‚¬ì‹¤ì /ì¼ê´€ëœ ë‹µë³€ì„ ìœ„í•´ ë‚®ì€ temperature ì‚¬ìš© (E2E Audit - 2026-01-27)
        from src.shared.constants import CHATBOT_TEMPERATURE

        self.temperature = float(
            os.getenv(
                "LLM_CHATBOT_TEMPERATURE",
                os.getenv("LLM_TEMPERATURE", config.get("temperature", CHATBOT_TEMPERATURE)),
            )
        )
        self.max_context_tokens = config.get("max_context_tokens", 8000)

        # ì˜¨í†¨ë¡œì§€ ì»´í¬ë„ŒíŠ¸
        self.kg = knowledge_graph or KnowledgeGraph()
        self.reasoner = reasoner or OntologyReasoner(self.kg)

        # ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ë“±ë¡
        if not self.reasoner.rules:
            register_all_rules(self.reasoner)

        # RAG ì»´í¬ë„ŒíŠ¸
        self.doc_retriever = DocumentRetriever(docs_dir)
        self.router = RAGRouter()

        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸°
        self.hybrid_retriever = HybridRetriever(
            knowledge_graph=self.kg,
            reasoner=self.reasoner,
            doc_retriever=self.doc_retriever,
            auto_init_rules=False,
        )

        # ì»¨í…ìŠ¤íŠ¸ ë¹Œë”
        self.context_builder = ContextBuilder(max_tokens=3000)
        self.compact_builder = CompactContextBuilder(max_tokens=1500)

        # í…œí”Œë¦¿
        self.templates = ResponseTemplates()

        # ë©”ëª¨ë¦¬
        self.context = context_manager or ContextManager()

        # ëª¨ë‹ˆí„°ë§
        self.logger = logger or AgentLogger("hybrid_chatbot")
        self.tracer = tracer
        self.metrics = metrics

        # í˜„ì¬ ë°ì´í„° ì»¨í…ìŠ¤íŠ¸
        self._current_data: dict[str, Any] = {}

        # ë§ˆì§€ë§‰ í•˜ì´ë¸Œë¦¬ë“œ ì»¨í…ìŠ¤íŠ¸
        self._last_hybrid_context: HybridContext | None = None

        # Query Rewriter (ëŒ€í™” ë§¥ë½ ê¸°ë°˜ ì§ˆë¬¸ ì¬êµ¬ì„±)
        self.query_rewriter = QueryRewriter(model=model)

        # ì™¸ë¶€ ì‹ í˜¸ ìˆ˜ì§‘ê¸° (Tavily + RSS + Reddit)
        self._external_signal_collector = None
        self._last_external_signals: list[Any] = []

        # ì‘ë‹µ ê²€ì¦ íŒŒì´í”„ë¼ì¸ (ì§€ì—° ì´ˆê¸°í™”)
        self._verification_pipeline: Any = None
        self._enable_verification = config.get("enable_verification", True)

        # ë¶„í•´ëœ ì»´í¬ë„ŒíŠ¸ (feature-flag-guarded)
        from src.infrastructure.feature_flags import FeatureFlags

        flags = FeatureFlags.get_instance()
        if flags.use_decomposed_chatbot():
            from src.agents.external_signal_manager import ExternalSignalManager
            from src.agents.source_provider import SourceProvider
            from src.agents.suggestion_engine import SuggestionEngine

            self.suggestion_engine = SuggestionEngine(knowledge_graph=self.kg, config=config)
            self.source_provider = SourceProvider(config=config, knowledge_graph=self.kg)
            self.signal_manager = ExternalSignalManager(config=config)

    @property
    def verification_pipeline(self) -> Any:
        """ê²€ì¦ íŒŒì´í”„ë¼ì¸ (ì§€ì—° ì´ˆê¸°í™”)"""
        if self._verification_pipeline is None:
            from src.core.verification_pipeline import VerificationPipelineFactory

            self._verification_pipeline = VerificationPipelineFactory.get_instance()
        return self._verification_pipeline

    def set_data_context(self, data: dict[str, Any]) -> None:
        """
        í˜„ì¬ ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ ì„¤ì •

        Args:
            data: ì§€í‘œ/ì¸ì‚¬ì´íŠ¸ ë°ì´í„°
        """
        self._current_data = data

        # ì§€ì‹ ê·¸ë˜í”„ ì—…ë°ì´íŠ¸
        if data:
            self.hybrid_retriever.update_knowledge_graph(metrics_data=data)

    async def chat(
        self, user_message: str, session_id: str | None = None, include_reasoning: bool = True
    ) -> dict[str, Any]:
        """
        ì‚¬ìš©ì ì§ˆë¬¸ì— ì‘ë‹µ

        Args:
            user_message: ì‚¬ìš©ì ë©”ì‹œì§€
            session_id: ì„¸ì…˜ ID
            include_reasoning: ì¶”ë¡  ê³¼ì • í¬í•¨ ì—¬ë¶€

        Returns:
            {
                "response": "...",
                "query_type": "...",
                "inferences": [...],
                "sources": [...],
                "suggestions": [...]
            }
        """
        # ê°ì‚¬ ë¡œê¹… ì‹œì‘
        audit_context = self.logger.chat_request(query=user_message, session_id=session_id)
        start_time = datetime.now()

        if self.tracer:
            self.tracer.start_span("hybrid_chatbot_response", {"query_length": len(user_message)})

        try:
            # 1. ì¿¼ë¦¬ ë¼ìš°íŒ… (ì˜ë„ ë¶„ë¥˜)
            route_result = self.router.route(user_message)
            query_type = route_result.get("query_type")

            self.logger.debug(f"Query type: {query_type}")

            # 2. Fallback ì²˜ë¦¬ (ì˜ë„ ë¶ˆëª…)
            if query_type == QueryType.UNKNOWN:
                fallback_response = route_result.get("fallback_message", "")
                return {
                    "response": fallback_response,
                    "query_type": "unknown",
                    "is_fallback": True,
                    "inferences": [],
                    "sources": [],
                    "suggestions": self.suggestion_engine.get_fallback_suggestions()
                    if hasattr(self, "suggestion_engine")
                    else self._get_fallback_suggestions(),
                }

            # 2.5 ì§ˆë¬¸ ì¬êµ¬ì„± (ëŒ€í™” ë§¥ë½ ê¸°ë°˜)
            rewrite_result = await self._maybe_rewrite_query(user_message)

            # ëª…í™•í™” í•„ìš”ì‹œ ë°”ë¡œ ë°˜í™˜
            if rewrite_result.needs_clarification:
                self.context.add_user_message(user_message)
                self.context.add_assistant_message(rewrite_result.clarification_message)
                return {
                    "response": rewrite_result.clarification_message,
                    "query_type": "clarification",
                    "is_fallback": True,
                    "inferences": [],
                    "sources": [],
                    "suggestions": [
                        "íŠ¹ì • ë¸Œëœë“œë¥¼ ì§€ì •í•´ì£¼ì„¸ìš”",
                        "ì–´ë–¤ ì§€í‘œê°€ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?",
                        "ì œí’ˆëª…ì„ ì•Œë ¤ì£¼ì„¸ìš”",
                    ],
                    "query_info": {
                        "original": user_message,
                        "rewritten": None,
                        "was_rewritten": False,
                        "needs_clarification": True,
                    },
                }

            # ì¬êµ¬ì„±ëœ ì¿¼ë¦¬ ì‚¬ìš© (ê²€ìƒ‰ìš©)
            search_query = rewrite_result.rewritten_query

            if rewrite_result.was_rewritten:
                self.logger.info(f"Query rewritten: '{user_message}' -> '{search_query}'")

            # 3. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ì¶”ë¡  + RAG)
            if self.tracer:
                self.tracer.start_span("hybrid_retrieval")

            hybrid_context = await self.hybrid_retriever.retrieve(
                query=search_query,  # ì¬êµ¬ì„±ëœ ì¿¼ë¦¬ ì‚¬ìš©
                current_metrics=self._current_data,
                include_explanations=include_reasoning,
            )
            self._last_hybrid_context = hybrid_context

            if self.tracer:
                self.tracer.end_span("completed")

            # 3.5. ì™¸ë¶€ ì‹ í˜¸ ìˆ˜ì§‘ (Tavily ë‰´ìŠ¤, RSS, Reddit)
            if hasattr(self, "signal_manager"):
                external_signals = await self.signal_manager.collect(
                    query=search_query, entities=hybrid_context.entities
                )
                self._last_external_signals = external_signals
                failed_signals = self.signal_manager.get_failed_collectors()
            else:
                external_signals = await self._collect_external_signals(
                    query=search_query, entities=hybrid_context.entities
                )
                self._last_external_signals = external_signals
                failed_signals = self._get_failed_signal_collectors()

            # 4. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            if self.tracer:
                self.tracer.start_span("build_context")

            # ì¿¼ë¦¬ ìœ í˜•ì— ë”°ë¼ ë¹Œë” ì„ íƒ
            if query_type in [QueryType.DEFINITION, QueryType.INTERPRETATION]:
                # ê°„ë‹¨í•œ ì§ˆë¬¸ì€ ì»´íŒ©íŠ¸ ë¹Œë”
                context = self.compact_builder.build(
                    hybrid_context=hybrid_context,
                    current_metrics=self._current_data,
                    query=user_message,
                    knowledge_graph=self.kg,
                )
            else:
                # ë¶„ì„ ì§ˆë¬¸ì€ í’€ ë¹Œë” (ì¹´í…Œê³ ë¦¬ ê³„ì¸µ ì¸ì‹ í¬í•¨)
                context = self.context_builder.build(
                    hybrid_context=hybrid_context,
                    current_metrics=self._current_data,
                    query=user_message,
                    knowledge_graph=self.kg,
                )

            if self.tracer:
                self.tracer.end_span("completed")

            # 5. LLM ì‘ë‹µ ìƒì„±
            if self.tracer:
                self.tracer.start_span("llm_response")

            response = await self._generate_response(
                user_message=user_message,
                query_type=query_type,
                context=context,
                inferences=hybrid_context.inferences,
            )

            if self.tracer:
                self.tracer.end_span("completed")

            # 6. ì¶œì²˜ ì •ë³´ ì¶”ì¶œ ë° í¬ë§·íŒ… (ì™¸ë¶€ ì‹ í˜¸ í¬í•¨)
            if hasattr(self, "source_provider"):
                sources = self.source_provider.extract_sources(
                    hybrid_context=hybrid_context,
                    current_data=self._current_data,
                    external_signals=external_signals,
                    model=self.model,
                )
                formatted_sources = self.source_provider.format_sources_for_display(sources)
            else:
                sources = self._extract_sources(hybrid_context, external_signals)
                formatted_sources = self._format_sources_for_response(sources)

            # ì‹¤íŒ¨í•œ ì‹ í˜¸ ìˆ˜ì§‘ê¸° ê²½ê³  ì¶”ê°€
            failed_signal_warning = ""
            if failed_signals:
                failed_signal_warning = (
                    f"\n\n> âš ï¸ **ì™¸ë¶€ ì‹ í˜¸ ìˆ˜ì§‘ ì‹¤íŒ¨**: {', '.join(failed_signals)}"
                )
                failed_signal_warning += "\n> *(ìœ„ ë°ì´í„° ì†ŒìŠ¤ëŠ” í˜„ì¬ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‘ë‹µì€ ë‚˜ë¨¸ì§€ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.)*"

            # 7. ì‘ë‹µì— ì¶œì²˜ ì„¹ì…˜ ë° ê²½ê³  ì¶”ê°€
            full_response = response + failed_signal_warning + formatted_sources

            # 8. ëŒ€í™” ê¸°ë¡ ì €ì¥
            self.context.add_user_message(user_message)
            self.context.add_assistant_message(full_response)

            # 9. í›„ì† ì§ˆë¬¸ ì œì•ˆ (v2 - ì‘ë‹µ ë‚´ìš© ë¶„ì„ í¬í•¨)
            if hasattr(self, "suggestion_engine"):
                suggestions = self.suggestion_engine.generate(
                    query_type=query_type,
                    entities=hybrid_context.entities,
                    inferences=hybrid_context.inferences,
                    response=full_response,
                )
            else:
                suggestions = self._generate_suggestions(
                    query_type=query_type,
                    entities=hybrid_context.entities,
                    inferences=hybrid_context.inferences,
                    response=full_response,
                )

            duration = (datetime.now() - start_time).total_seconds()

            if self.tracer:
                self.tracer.end_span("completed")

            # ê°ì‚¬ ë¡œê¹… ì™„ë£Œ (ìƒì„¸ ë©”íŠ¸ë¦­ í¬í•¨)
            self.logger.chat_response(
                request_context=audit_context,
                response=full_response,
                model=self.model,
                entities_extracted=hybrid_context.entities,
                intent_detected=query_type.value
                if hasattr(query_type, "value")
                else str(query_type),
                kg_facts_count=len(hybrid_context.ontology_facts),
                rag_chunks_count=len(hybrid_context.rag_chunks),
                inferences_count=len(hybrid_context.inferences),
                success=True,
            )

            # ì‘ë‹µ ê²€ì¦ (ì„ íƒì )
            verification_result = None
            if self._enable_verification:
                try:
                    verification_context = {
                        "category": hybrid_context.entities.get("category")
                        if hybrid_context.entities
                        else None,
                        "brand": hybrid_context.entities.get("brand")
                        if hybrid_context.entities
                        else None,
                    }
                    verified = await self.verification_pipeline.verify(
                        full_response, context=verification_context, include_details=True
                    )
                    verification_result = self.verification_pipeline.get_verification_summary(
                        verified
                    )
                    self.logger.debug(
                        f"Verification: {verified.grade.value} ({verified.score:.0%})"
                    )
                except Exception as ve:
                    self.logger.warning(f"Verification failed: {ve}")
                    verification_result = None

            result = {
                "response": full_response,
                "query_type": query_type.value if hasattr(query_type, "value") else str(query_type),
                "is_fallback": False,
                "inferences": [inf.to_dict() for inf in hybrid_context.inferences],
                "sources": sources,
                "suggestions": suggestions,
                "entities": hybrid_context.entities,
                "query_info": {
                    "original": user_message,
                    "rewritten": search_query if rewrite_result.was_rewritten else None,
                    "was_rewritten": rewrite_result.was_rewritten,
                },
                "stats": {
                    "inferences_count": len(hybrid_context.inferences),
                    "rag_chunks_count": len(hybrid_context.rag_chunks),
                    "kg_facts_count": len(hybrid_context.ontology_facts),
                    "response_time_ms": duration * 1000,
                },
            }

            # ê²€ì¦ ê²°ê³¼ ì¶”ê°€
            if verification_result:
                result["verification"] = verification_result

            return result

        except Exception as e:
            if self.tracer:
                self.tracer.end_span("failed", str(e))

            # ê°ì‚¬ ë¡œê¹… (ì—ëŸ¬)
            self.logger.chat_response(
                request_context=audit_context,
                response="",
                model=self.model,
                success=False,
                error=str(e),
            )

            return {
                "response": "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                "query_type": "error",
                "is_fallback": True,
                "error": str(e),
                "inferences": [],
                "sources": [],
                "suggestions": self.suggestion_engine.get_fallback_suggestions()
                if hasattr(self, "suggestion_engine")
                else self._get_fallback_suggestions(),
            }

    async def _generate_response(
        self,
        user_message: str,
        query_type: QueryType,
        context: str,
        inferences: list[InferenceResult],
    ) -> str:
        """LLM ì‘ë‹µ ìƒì„±"""
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì¹´í…Œê³ ë¦¬ ê³„ì¸µ ì¸ì‹ ì¶”ê°€)
        system_prompt = self.context_builder.build_system_prompt(include_guardrails=True)

        # ì¹´í…Œê³ ë¦¬ ê³„ì¸µ ë° ìˆœìœ„ ë¹„êµ ê·œì¹™ ì¶”ê°€
        system_prompt += """

## ì¹´í…Œê³ ë¦¬ ê³„ì¸µ êµ¬ì¡° ì¸ì‹
- ì œí’ˆì€ ì—¬ëŸ¬ ê³„ì¸µì˜ ì¹´í…Œê³ ë¦¬ì— ë™ì‹œì— ì†Œì†ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤
- ì˜ˆ: íŠ¹ì • ë¦½ì¼€ì–´ ì œí’ˆì´ "Lip Care"ì—ì„œ 4ìœ„ì´ë©´ì„œ, ìƒìœ„ ì¹´í…Œê³ ë¦¬ì¸ "Beauty & Personal Care"ì—ì„œëŠ” 73ìœ„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤
- ìˆœìœ„ë¥¼ ì–¸ê¸‰í•  ë•ŒëŠ” ë°˜ë“œì‹œ ì–´ëŠ ì¹´í…Œê³ ë¦¬ì—ì„œì˜ ìˆœìœ„ì¸ì§€ ëª…ì‹œí•˜ì„¸ìš”
- ì¹´í…Œê³ ë¦¬ ê°„ ìˆœìœ„ ì°¨ì´ê°€ ìˆëŠ” ê²½ìš°, ì´ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ í˜„ìƒì…ë‹ˆë‹¤ (í•˜ìœ„ ì¹´í…Œê³ ë¦¬ê°€ ë” ì„¸ë¶„í™”ë˜ì–´ ê²½ìŸ ë²”ìœ„ê°€ ì¢ê¸° ë•Œë¬¸)

## âš ï¸ ìˆœìœ„ ë¹„êµ ê·œì¹™ (ì¤‘ìš”)
- ìˆœìœ„ ë³€ë™ ë¶„ì„ì€ **ë°˜ë“œì‹œ ë™ì¼ ì¹´í…Œê³ ë¦¬ ë‚´ì—ì„œë§Œ** ìœ íš¨í•©ë‹ˆë‹¤
- ì˜ˆì‹œ (ì˜¬ë°”ë¦„): "Lip Care 4ìœ„ â†’ Lip Care 6ìœ„ = 2ë‹¨ê³„ í•˜ë½"
- ì˜ˆì‹œ (ì˜ëª»ë¨): "Lip Care 4ìœ„ â†’ Beauty 67ìœ„ = 63ë‹¨ê³„ í•˜ë½" â† ì„œë¡œ ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬ì´ë¯€ë¡œ ë¹„êµ ë¶ˆê°€
- 30ìœ„ ì´ìƒì˜ ê¸‰ê²©í•œ ìˆœìœ„ ë³€ë™ì´ ê°ì§€ë˜ë©´, ì¹´í…Œê³ ë¦¬ í˜¼ë™ì´ ì•„ë‹Œì§€ ë¨¼ì € í™•ì¸í•˜ì„¸ìš”
- ìˆœìœ„ ë³€ë™ì„ ë³´ê³ í•  ë•ŒëŠ” í•­ìƒ [ì¹´í…Œê³ ë¦¬ëª…]ì„ ëª…ì‹œí•˜ì„¸ìš”

## ë¸Œëœë“œëª… ì •ê·œí™” ê·œì¹™
ë‹¤ìŒ ë¸Œëœë“œëª…ì€ ì˜ë¦° ì´ë¦„ì´ë¯€ë¡œ ì •ì‹ ëª…ì¹­ìœ¼ë¡œ ì‚¬ìš©í•˜ì„¸ìš”:
- "Burt's" â†’ "Burt's Bees"
- "wet" â†’ "wet n wild"
- "Tree" â†’ "Tree Hut"
- "Summer" â†’ "Summer Fridays"
- "Rare" â†’ "Rare Beauty"
- "La" â†’ "La Roche-Posay"
- "Beauty" (ë‹¨ë… ì‚¬ìš© ì‹œ) â†’ "Beauty of Joseon"
- "Tower" â†’ "Tower 28"
- "Drunk" â†’ "Drunk Elephant"
- "Paula's" â†’ "Paula's Choice"
- "The" (ë‹¨ë… ì‚¬ìš© ì‹œ) â†’ "The Ordinary"
- ì£¼ìš” ë¸Œëœë“œ ì™¸ ë¸Œëœë“œëŠ” "ì†Œê·œëª¨/ì‹ í¥ ë¸Œëœë“œ" ë˜ëŠ” "Non-major Brands"ë¡œ í‘œí˜„
- âš ï¸ "Unknown", "ê¸°íƒ€ ë¸Œëœë“œ(Unknown)", "ë¯¸í™•ì¸ ë¸Œëœë“œ" í‘œí˜„ ì ˆëŒ€ ê¸ˆì§€
"""

        # ëŒ€í™” íˆìŠ¤í† ë¦¬
        conversation = self.context.get_conversation_summary()

        # ì¶”ë¡  ê²°ê³¼ ê°•ì¡°
        inference_summary = ""
        if inferences:
            inference_lines = []
            for inf in inferences[:3]:
                inference_lines.append(f"- [{inf.insight_type.value}] {inf.insight}")
            inference_summary = "\n".join(inference_lines)

        # ì¹´í…Œê³ ë¦¬ ê³„ì¸µ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë§ˆì§€ë§‰ í•˜ì´ë¸Œë¦¬ë“œ ì»¨í…ìŠ¤íŠ¸ì—ì„œ)
        category_hierarchy_context = ""
        if self._last_hybrid_context and self._last_hybrid_context.entities:
            category_hierarchy_context = self._build_category_hierarchy_context(
                self._last_hybrid_context.entities
            )

        user_prompt = f"""
{context}

---

## ì¹´í…Œê³ ë¦¬ ê³„ì¸µ ì •ë³´
{category_hierarchy_context if category_hierarchy_context else "ì¹´í…Œê³ ë¦¬ ê³„ì¸µ ì •ë³´ ì—†ìŒ"}

## ì˜¨í†¨ë¡œì§€ ì¶”ë¡  ê²°ê³¼ (ìš°ì„  ì°¸ê³ )
{inference_summary if inference_summary else "ê´€ë ¨ ì¶”ë¡  ê²°ê³¼ ì—†ìŒ"}

## ì´ì „ ëŒ€í™”
{conversation if conversation else "ì—†ìŒ"}

## ì‚¬ìš©ì ì§ˆë¬¸
{user_message}

---

ìš”êµ¬ì‚¬í•­:
1. ì˜¨í†¨ë¡œì§€ ì¶”ë¡  ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€
2. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë¥¼ ì¸ìš©í•˜ì—¬ ë‹µë³€
3. ìˆœìœ„ë¥¼ ì–¸ê¸‰í•  ë•ŒëŠ” ì¹´í…Œê³ ë¦¬ë¥¼ ëª…ì‹œ (ì˜ˆ: "Lip Careì—ì„œ 4ìœ„", "Beauty & Personal Care ì „ì²´ì—ì„œëŠ” 73ìœ„")
4. ë¶ˆí™•ì‹¤í•œ ë¶€ë¶„ì€ ëª…í™•íˆ ë°í˜
5. ë‹¨ì •ì  í‘œí˜„ ëŒ€ì‹  ê°€ëŠ¥ì„± í‘œí˜„ ì‚¬ìš©
6. ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ë‹µë³€
7. ì™¸ë¶€ ë‰´ìŠ¤/ê¸°ì‚¬ë¥¼ ì¸ìš©í•  ë•ŒëŠ” ë°˜ë“œì‹œ [ì¶œì²˜ëª…, ë‚ ì§œ] í˜•ì‹ìœ¼ë¡œ í‘œì‹œ
   ì˜ˆ: "LANEIGEê°€ ê¸€ë˜ìŠ¤ ìŠ¤í‚¨ íŠ¸ë Œë“œë¥¼ ì„ ë„í•˜ê³  ìˆìŠµë‹ˆë‹¤ [Allure, 2026-01-20]"
8. Reddit/YouTube ë“± ì†Œì…œ ë°ì´í„°ë„ [Reddit r/ì„œë¸Œë ˆë”§, ë‚ ì§œ] í˜•ì‹ìœ¼ë¡œ ì¸ìš©
"""

        try:
            response = await acompletion(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                temperature=self.temperature,
                max_tokens=800,
            )

            if response.choices:
                answer = response.choices[0].message.content
            else:
                answer = "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

            # í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡
            if self.metrics and hasattr(response, "usage"):
                self.metrics.record_llm_call(
                    model=self.model,
                    prompt_tokens=response.usage.prompt_tokens,
                    completion_tokens=response.usage.completion_tokens,
                    latency_ms=0,
                    cost=self._estimate_cost(
                        response.usage.prompt_tokens, response.usage.completion_tokens
                    ),
                )

            # ê°€ë“œë ˆì¼ ì ìš©
            answer = self.templates.apply_guardrails(answer)

            # ë¸Œëœë“œëª… ì •ê·œí™” ì ìš©
            answer = self._normalize_response_brands(answer)

            return answer

        except Exception as e:
            self.logger.error(f"LLM call failed: {e}")
            return self._generate_fallback_response(inferences)

    def _generate_fallback_response(self, inferences: list[InferenceResult]) -> str:
        """í´ë°± ì‘ë‹µ ìƒì„±"""
        if inferences:
            lines = ["ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ë“œë¦½ë‹ˆë‹¤:\n"]
            for inf in inferences[:2]:
                lines.append(f"- {inf.insight}")
                if inf.recommendation:
                    lines.append(f"  â†’ ê¶Œì¥: {inf.recommendation}")
            return "\n".join(lines)

        return "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."

    def _generate_suggestions(
        self,
        query_type: QueryType,
        entities: dict[str, list[str]],
        inferences: list[InferenceResult],
        response: str = "",
    ) -> list[str]:
        """
        í›„ì† ì§ˆë¬¸ ì œì•ˆ (v2 - ê°œì„  ë²„ì „)

        ìš°ì„ ìˆœìœ„:
        1. ì‘ë‹µ í‚¤ì›Œë“œ ê¸°ë°˜ (response ë¶„ì„)
        2. ì—”í‹°í‹° ê¸°ë°˜ (KG ê²½ìŸì‚¬ í™œìš©)
        3. ì¶”ë¡  ê²°ê³¼ ê¸°ë°˜
        4. ì¿¼ë¦¬ ìœ í˜• ê¸°ë°˜ (í´ë°±)

        Args:
            query_type: ì§ˆë¬¸ ìœ í˜•
            entities: ì¶”ì¶œëœ ì—”í‹°í‹°
            inferences: ì˜¨í†¨ë¡œì§€ ì¶”ë¡  ê²°ê³¼
            response: AI ì‘ë‹µ ë‚´ìš© (í‚¤ì›Œë“œ ë¶„ì„ìš©)

        Returns:
            3ê°œì˜ í›„ì† ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
        """
        from src.shared.constants import SUGGESTION_MAX_COUNT

        suggestions = []

        # 1ìˆœìœ„: ì‘ë‹µ í‚¤ì›Œë“œ ê¸°ë°˜ ì œì•ˆ
        if response:
            keyword_suggestions = self._extract_response_keywords(response)
            suggestions.extend(keyword_suggestions)

        # 2ìˆœìœ„: ì—”í‹°í‹° ê¸°ë°˜ ì œì•ˆ (KG ê²½ìŸì‚¬ í™œìš©)
        if len(suggestions) < SUGGESTION_MAX_COUNT:
            entity_suggestions = self._generate_entity_suggestions(entities)
            suggestions.extend(entity_suggestions)

        # 3ìˆœìœ„: ì¶”ë¡  ê²°ê³¼ ê¸°ë°˜ ì œì•ˆ
        if len(suggestions) < SUGGESTION_MAX_COUNT and inferences:
            inference_suggestions = self._generate_inference_suggestions(inferences)
            suggestions.extend(inference_suggestions)

        # 4ìˆœìœ„: ì¿¼ë¦¬ ìœ í˜• ê¸°ë°˜ ì œì•ˆ (í´ë°±)
        if len(suggestions) < SUGGESTION_MAX_COUNT:
            type_suggestions = self._generate_type_suggestions(query_type, entities)
            suggestions.extend(type_suggestions)

        # ì¤‘ë³µ ì œê±° ë° ìƒìœ„ 3ê°œ
        unique_suggestions = list(dict.fromkeys(suggestions))
        return unique_suggestions[:SUGGESTION_MAX_COUNT]

    def _extract_response_keywords(self, response: str) -> list[str]:
        """ì‘ë‹µì—ì„œ í›„ì† ì§ˆë¬¸ ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ì¶œ (Phase 3)"""
        import re

        keywords = []

        # íŒ¨í„´ ë§¤ì¹­ - ì‘ë‹µ ë‚´ìš©ì— ë”°ë¼ ê´€ë ¨ í›„ì† ì§ˆë¬¸ ìƒì„±
        patterns = {
            r"ìˆœìœ„.{0,10}(í•˜ë½|ê¸‰ë½|ë–¨ì–´)": "ìˆœìœ„ í•˜ë½ ì›ì¸ ë¶„ì„",
            r"ìˆœìœ„.{0,10}(ìƒìŠ¹|ê¸‰ë“±|ì˜¬ë¼)": "ìƒìŠ¹ ìš”ì¸ ìƒì„¸ ë¶„ì„",
            r"ê²½ìŸì‚¬|ê²½ìŸ ë¸Œëœë“œ|competitor": "ê²½ìŸì‚¬ ìƒì„¸ ë¹„êµ",
            r"ê°€ê²©.{0,10}(ì¸ìƒ|ì¸í•˜|ë³€ë™)": "ê°€ê²© ì „ëµ ë¶„ì„",
            r"ë¦¬ë·°|í‰ì |rating": "ì†Œë¹„ì í”¼ë“œë°± ìƒì„¸ ë¶„ì„",
            r"íŠ¸ë Œë“œ|ìœ í–‰|trend": "íŠ¸ë Œë“œ ìƒì„¸ ë¶„ì„",
            r"ì„±ì¥.{0,5}(ê¸°íšŒ|ê°€ëŠ¥|potential)": "ì„±ì¥ ì „ëµ ì œì•ˆ",
            r"ìœ„í—˜|ë¦¬ìŠ¤í¬|ìœ„í˜‘|risk": "ë¦¬ìŠ¤í¬ ëŒ€ì‘ ì „ëµì€?",
            r"SoS|ì ìœ ìœ¨|share": "ì ìœ ìœ¨ ê°œì„  ì „ëµì€?",
            r"Top.{0,3}(10|5)|ìƒìœ„": "Top 10 ì§„ì… ì „ëµì€?",
        }

        for pattern, suggestion in patterns.items():
            if re.search(pattern, response, re.IGNORECASE):
                keywords.append(suggestion)
                if len(keywords) >= 2:  # ìµœëŒ€ 2ê°œ
                    break

        return keywords

    def _generate_entity_suggestions(self, entities: dict[str, list[str]]) -> list[str]:
        """ì—”í‹°í‹° ê¸°ë°˜ ë™ì  ì œì•ˆ ìƒì„± (Phase 2 - KG ê²½ìŸì‚¬ í™œìš©)"""
        suggestions = []

        brands = entities.get("brands", [])
        categories = entities.get("categories", [])
        indicators = entities.get("indicators", [])

        # ë¸Œëœë“œ ê¸°ë°˜ (KGì—ì„œ ê²½ìŸì‚¬ ì¡°íšŒ)
        if brands:
            brand = brands[0]
            # KGì—ì„œ ê²½ìŸì‚¬ ì¡°íšŒ ì‹œë„
            try:
                competitors = self.kg.get_related_brands(brand, limit=2)
                if competitors:
                    comp = (
                        competitors[0]
                        if isinstance(competitors[0], str)
                        else competitors[0].get("name", "")
                    )
                    if comp:
                        suggestions.append(f"{brand} vs {comp} ë¹„êµ ë¶„ì„")
            except Exception:
                pass  # KG ì—†ìœ¼ë©´ ìŠ¤í‚µ

            suggestions.append(f"{brand} ì œí’ˆë³„ ì„±ê³¼ ë¶„ì„")

            # ë‹¤ì¤‘ ë¸Œëœë“œ ë¹„êµ
            if len(brands) > 1:
                suggestions.append(f"{brands[0]} vs {brands[1]} ë¹„êµ")

        # ì¹´í…Œê³ ë¦¬ ê¸°ë°˜
        if categories:
            cat = categories[0]
            suggestions.append(f"{cat} ì‹œì¥ íŠ¸ë Œë“œ ë¶„ì„")
            suggestions.append(f"{cat} Top 5 ë¸Œëœë“œ í˜„í™©")

        # ì§€í‘œ ê¸°ë°˜
        if indicators:
            ind = indicators[0].upper()
            suggestions.append(f"{ind} ê°œì„  ì „ëµ")
            suggestions.append(f"{ind} ê²½ìŸì‚¬ ë¹„êµ")

        return suggestions

    def _generate_inference_suggestions(self, inferences: list[InferenceResult]) -> list[str]:
        """ì¶”ë¡  ê²°ê³¼ ê¸°ë°˜ ì œì•ˆ"""
        suggestions = []

        for inf in inferences[:2]:
            insight_lower = inf.insight.lower()
            insight_type_val = (
                inf.insight_type.value
                if hasattr(inf.insight_type, "value")
                else str(inf.insight_type)
            )

            if "ê²½ìŸ" in insight_lower or "COMPETITIVE" in insight_type_val:
                suggestions.append("ì£¼ìš” ê²½ìŸì‚¬ ë¶„ì„")
            if "ê°€ê²©" in insight_lower or "PRICE" in insight_type_val:
                suggestions.append("ê°€ê²© ì „ëµ ìƒì„¸ ë¶„ì„")
            if "ì„±ì¥" in insight_lower or "GROWTH" in insight_type_val:
                suggestions.append("ì„±ì¥ ê¸°íšŒ êµ¬ì²´í™”")
            if inf.recommendation:
                # ê¶Œì¥ ì•¡ì…˜ì´ ìˆìœ¼ë©´ ê´€ë ¨ ì§ˆë¬¸
                suggestions.append(f"'{inf.recommendation}' ì‹¤í–‰ ë°©ë²•")

        return suggestions

    def _generate_type_suggestions(
        self, query_type: QueryType, entities: dict[str, list[str]]
    ) -> list[str]:
        """ì¿¼ë¦¬ ìœ í˜• ê¸°ë°˜ í´ë°± ì œì•ˆ"""
        suggestions = []
        indicators = entities.get("indicators", [])

        if query_type == QueryType.DEFINITION:
            if indicators:
                ind = indicators[0].upper()
                suggestions.append(f"{ind}ê°€ ë†’ìœ¼ë©´ ì–´ë–¤ ì˜ë¯¸?")
            suggestions.extend(["ê´€ë ¨ëœ ë‹¤ë¥¸ ì§€í‘œëŠ”?", "ì‹¤ì œ ë°ì´í„°ì— ì ìš©í•´ì£¼ì„¸ìš”"])

        elif query_type == QueryType.INTERPRETATION:
            suggestions.extend(["ì´ ìˆ˜ì¹˜ê°€ ì¢‹ì€ ê±´ê°€ìš”?", "ê°œì„ ì„ ìœ„í•œ ì•¡ì…˜ì€?"])

        elif query_type == QueryType.ANALYSIS:
            suggestions.extend(["ì‹œê³„ì—´ íŠ¸ë Œë“œ ë¶„ì„", "ê²½ìŸì‚¬ì™€ ë¹„êµí•´ì£¼ì„¸ìš”"])

        elif query_type == QueryType.DATA_QUERY:
            suggestions.extend(["ìµœê·¼ 7ì¼ ì¶”ì´ ë¶„ì„", "ê²½ìŸì‚¬ ëŒ€ë¹„ í˜„í™©"])

        elif query_type == QueryType.COMBINATION:
            suggestions.extend(["ë‹¤ë¥¸ ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„", "í˜„ì¬ í•´ë‹¹ ìƒí™© ì¡´ì¬ ì—¬ë¶€"])

        else:
            # ê¸°ë³¸ ì œì•ˆ
            suggestions = ["SoS(ì ìœ ìœ¨) ì„¤ëª…", "LANEIGE í˜„ì¬ ìˆœìœ„", "ì „ëµì  ê¶Œê³ ì‚¬í•­"]

        return suggestions

    def _get_fallback_suggestions(self) -> list[str]:
        """í´ë°± ì œì•ˆ"""
        return ["SoS(ì ìœ ìœ¨)ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”", "ì˜¤ëŠ˜ì˜ ì£¼ìš” ì¸ì‚¬ì´íŠ¸ëŠ”?", "LANEIGE í˜„ì¬ ìˆœìœ„ëŠ”?"]

    async def _generate_llm_suggestions(
        self, user_query: str, response_summary: str, entities: dict[str, list[str]]
    ) -> list[str]:
        """
        LLM ê¸°ë°˜ í›„ì† ì§ˆë¬¸ ìƒì„± (Phase 4)

        ë¹„ìš©: ~$0.0002/í˜¸ì¶œ (GPT-4.1-mini ê¸°ì¤€)

        Args:
            user_query: ì‚¬ìš©ì ì§ˆë¬¸
            response_summary: AI ì‘ë‹µ ìš”ì•½ (300ì ì œí•œ)
            entities: ì¶”ì¶œëœ ì—”í‹°í‹°

        Returns:
            3ê°œì˜ í›„ì† ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ (ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸)
        """
        import json

        from src.shared.constants import SUGGESTION_MAX_TOKENS, SUGGESTION_TEMPERATURE

        prompt = f"""ë‹¹ì‹ ì€ AMORE Pacific ì‹œì¥ ë¶„ì„ ì±—ë´‡ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì™€ì˜ ëŒ€í™”ë¥¼ ì´ì–´ê°€ê¸° ìœ„í•œ í›„ì† ì§ˆë¬¸ 3ê°œë¥¼ ìƒì„±í•˜ì„¸ìš”.

[ì‚¬ìš©ì ì§ˆë¬¸]
{user_query}

[AI ì‘ë‹µ ìš”ì•½]
{response_summary[:300]}

[ì¶”ì¶œëœ ì—”í‹°í‹°]
- ë¸Œëœë“œ: {", ".join(entities.get("brands", [])) or "ì—†ìŒ"}
- ì¹´í…Œê³ ë¦¬: {", ".join(entities.get("categories", [])) or "ì—†ìŒ"}
- ì§€í‘œ: {", ".join(entities.get("indicators", [])) or "ì—†ìŒ"}

[ê·œì¹™]
1. ëŒ€í™” íë¦„ì— ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì§€ëŠ” ì§ˆë¬¸
2. êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì§ˆë¬¸
3. 20ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ
4. JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ

ì‘ë‹µ í˜•ì‹: ["ì§ˆë¬¸1", "ì§ˆë¬¸2", "ì§ˆë¬¸3"]"""

        try:
            response = await acompletion(
                model="gpt-4.1-mini",
                messages=[{"role": "user", "content": prompt}],
                temperature=SUGGESTION_TEMPERATURE,
                max_tokens=SUGGESTION_MAX_TOKENS,
            )

            content = response.choices[0].message.content.strip()
            # JSON íŒŒì‹±
            suggestions = json.loads(content)
            if isinstance(suggestions, list):
                return [str(s) for s in suggestions[:3]]
            return []

        except Exception as e:
            self.logger.warning(f"LLM suggestion generation failed: {e}")
            return []  # í´ë°±ì€ ê¸°ì¡´ ë¡œì§ìœ¼ë¡œ

    def _normalize_brand(self, brand: str) -> str:
        """ë¸Œëœë“œëª… ì •ê·œí™”"""
        if not brand or brand == "Unknown":
            return brand

        brand_lower = brand.lower().strip()

        # ì •ê·œí™” ë§¤í•‘ì—ì„œ ì°¾ê¸°
        if brand_lower in self.BRAND_NORMALIZATION:
            return self.BRAND_NORMALIZATION[brand_lower]

        return brand

    def _normalize_response_brands(self, response: str) -> str:
        """ì‘ë‹µ ë‚´ ë¸Œëœë“œëª… ì •ê·œí™”"""
        import re

        # íŠ¹ìˆ˜ ì¼€ì´ìŠ¤: ì•„í¬ìŠ¤íŠ¸ë¡œí”¼ê°€ í¬í•¨ëœ ë¸Œëœë“œëª…
        special_brands = {
            "Burt's": ("Burt's Bees", r"(?i)\bBurt's(?!\s*Bees)"),
            "Paula's": ("Paula's Choice", r"(?i)\bPaula's(?!\s*Choice)"),
        }

        for _truncated, (full, pattern) in special_brands.items():
            if full.lower() not in response.lower():
                response = re.sub(pattern, full, response)

        # ì¼ë°˜ ë¸Œëœë“œëª… ì •ê·œí™”
        for truncated, full in self.BRAND_NORMALIZATION.items():
            # ì•„í¬ìŠ¤íŠ¸ë¡œí”¼ ë¸Œëœë“œëŠ” ìœ„ì—ì„œ ì²˜ë¦¬í–ˆìœ¼ë¯€ë¡œ ìŠ¤í‚µ
            if "'" in truncated:
                continue

            # ë‹¨ì–´ ê²½ê³„ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •í™•íˆ ë§¤ì¹­ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
            pattern = rf"\b{re.escape(truncated)}\b"
            # ì´ë¯¸ ì „ì²´ ë¸Œëœë“œëª…ì´ í¬í•¨ëœ ê²½ìš°ëŠ” ì œì™¸
            if full.lower() not in response.lower():
                response = re.sub(pattern, full, response, flags=re.IGNORECASE)

        return response

    def _build_category_hierarchy_context(self, entities: dict[str, list[str]]) -> str:
        """
        ì¹´í…Œê³ ë¦¬ ê³„ì¸µ ì»¨í…ìŠ¤íŠ¸ ìƒì„±

        Args:
            entities: ì¶”ì¶œëœ ì—”í‹°í‹° (ì¹´í…Œê³ ë¦¬, ì œí’ˆ ë“±)

        Returns:
            ì¹´í…Œê³ ë¦¬ ê³„ì¸µ ì •ë³´ ë¬¸ìì—´
        """
        if not self.kg:
            return ""

        context_parts = []

        # ì¹´í…Œê³ ë¦¬ ì—”í‹°í‹°ì—ì„œ ê³„ì¸µ ì •ë³´ ì¶”ì¶œ
        if not entities:
            return ""

        categories = entities.get("categories", [])
        for category in categories:
            hierarchy = self.kg.get_category_hierarchy(category)
            if "error" in hierarchy:
                continue

            # í˜„ì¬ ì¹´í…Œê³ ë¦¬ ì •ë³´
            context_parts.append(f"**{hierarchy['name']}** (Level {hierarchy['level']})")

            # ìƒìœ„ ì¹´í…Œê³ ë¦¬ ê²½ë¡œ
            if hierarchy.get("ancestors"):
                path = " > ".join([a["name"] for a in reversed(hierarchy["ancestors"])])
                context_parts.append(f"  - ìƒìœ„ ê²½ë¡œ: {path} > {hierarchy['name']}")

            # í•˜ìœ„ ì¹´í…Œê³ ë¦¬
            if hierarchy.get("descendants"):
                children = ", ".join([d["name"] for d in hierarchy["descendants"][:5]])
                context_parts.append(f"  - í•˜ìœ„ ì¹´í…Œê³ ë¦¬: {children}")

            context_parts.append("")

        # ì œí’ˆì˜ ì¹´í…Œê³ ë¦¬ ì»¨í…ìŠ¤íŠ¸ (ìˆœìœ„ ê´€ë ¨ ì§ˆë¬¸ ì‹œ)
        products = entities.get("products", [])
        for product_asin in products:
            product_ctx = self.kg.get_product_category_context(product_asin)
            if product_ctx.get("categories"):
                context_parts.append(f"**ì œí’ˆ {product_asin}ì˜ ì¹´í…Œê³ ë¦¬ë³„ ìˆœìœ„:**")
                for cat_info in product_ctx["categories"]:
                    hierarchy = cat_info.get("hierarchy", {})
                    cat_name = hierarchy.get("name", cat_info.get("category_id"))
                    rank = cat_info.get("rank", "N/A")
                    context_parts.append(f"  - {cat_name}: {rank}ìœ„")
                context_parts.append("")

        return "\n".join(context_parts) if context_parts else ""

    def _extract_sources(
        self, hybrid_context: HybridContext, external_signals: list[Any] | None = None
    ) -> list[dict[str, Any]]:
        """
        ì¶œì²˜ ì •ë³´ ì¶”ì¶œ (Perplexity/Liner ìŠ¤íƒ€ì¼ ìƒì„¸ ì¶œì²˜ ì œê³µ)

        Args:
            hybrid_context: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸
            external_signals: ì™¸ë¶€ ì‹ í˜¸ ë¦¬ìŠ¤íŠ¸ (Tavily ë‰´ìŠ¤, RSS, Reddit ë“±)

        Returns:
            ì¶œì²˜ ì •ë³´ ë¦¬ìŠ¤íŠ¸ (ìœ í˜•ë³„ ìƒì„¸ ì •ë³´ í¬í•¨)
        """
        sources = []

        # 1. í¬ë¡¤ë§ ë°ì´í„° ì¶œì²˜ - URL ë° ìƒì„¸ ì •ë³´ ì¶”ê°€ (ASIN í¬í•¨ - E2E Audit 2026-01-27)
        if self._current_data:
            metadata = self._current_data.get("metadata", {})
            data_date = metadata.get("data_date", "")
            categories = self._current_data.get("categories", {})

            total_products = (
                sum(len(cat_data.get("rank_records", [])) for cat_data in categories.values())
                if categories
                else 0
            )

            # ì§ˆì˜ì—ì„œ ì–¸ê¸‰ëœ ì œí’ˆì˜ ASIN ì¶”ì¶œ (provenance chain ê°•í™”)
            mentioned_asins = self._extract_mentioned_asins(hybrid_context, categories)

            crawled_source = {
                "type": "crawled_data",
                "icon": "ğŸ“Š",
                "description": "Amazon Best Sellers í¬ë¡¤ë§ ë°ì´í„°",
                "collected_at": data_date,
                "url": "https://www.amazon.com/gp/bestsellers/beauty",
                "details": {
                    "categories": list(categories.keys()) if categories else [],
                    "total_products": total_products,
                    "snapshot_date": data_date,
                },
            }

            # ê´€ë ¨ ì œí’ˆì˜ ASIN ì •ë³´ ì¶”ê°€
            if mentioned_asins:
                crawled_source["mentioned_products"] = mentioned_asins

            sources.append(crawled_source)

        # 2. Knowledge Graph ì¶œì²˜ - ì—”í‹°í‹° ë° ê´€ê³„ ì •ë³´ ì¶”ê°€
        if hybrid_context.ontology_facts:
            sources.append(
                {
                    "type": "knowledge_graph",
                    "icon": "ğŸ”—",
                    "description": "ì§€ì‹ ê·¸ë˜í”„ ê´€ê³„ ë°ì´í„°",
                    "fact_count": len(hybrid_context.ontology_facts),
                    "entities": self._extract_entity_names(hybrid_context.ontology_facts),
                    "relations": self._extract_relation_types(hybrid_context.ontology_facts),
                    "details": {
                        "source": "Amazon US ì‹¤ì‹œê°„ ë°ì´í„° ê¸°ë°˜ ì§€ì‹ ê·¸ë˜í”„",
                        "fact_count": len(hybrid_context.ontology_facts),
                    },
                }
            )

        # 3. ì˜¨í†¨ë¡œì§€ ì¶”ë¡  ì¶œì²˜ - ê·œì¹™ ìƒì„¸ ì •ë³´
        if hybrid_context.inferences:
            for inf in hybrid_context.inferences:
                sources.append(
                    {
                        "type": "ontology_inference",
                        "icon": "ğŸ§ ",
                        "description": f"ì˜¨í†¨ë¡œì§€ ê·œì¹™: {inf.rule_name}",
                        "rule_name": inf.rule_name,
                        "confidence": inf.confidence,
                        "evidence": inf.evidence,
                        "insight_type": inf.insight_type.value
                        if hasattr(inf.insight_type, "value")
                        else str(inf.insight_type),
                        "details": {"insight": inf.insight, "recommendation": inf.recommendation},
                    }
                )

        # 4. RAG ë¬¸ì„œ ì¶œì²˜ - íŒŒì¼ ê²½ë¡œ ë° ê´€ë ¨ì„± ì ìˆ˜
        rag_sources_map = {}
        for chunk in hybrid_context.rag_chunks:
            metadata = chunk.get("metadata", {})
            doc_id = metadata.get("doc_id", "")
            title = metadata.get("title", "")
            file_path = metadata.get("file_path", "")
            score = chunk.get("score", 0)
            section = metadata.get("section", "")

            if doc_id or title:
                doc_key = doc_id or title
                # ê°™ì€ ë¬¸ì„œì˜ ì—¬ëŸ¬ ì²­í¬ ì¤‘ ê°€ì¥ ë†’ì€ ì ìˆ˜ë§Œ ìœ ì§€
                if doc_key not in rag_sources_map or score > rag_sources_map[doc_key].get(
                    "relevance_score", 0
                ):
                    rag_sources_map[doc_key] = {
                        "type": "rag_document",
                        "icon": "ğŸ“„",
                        "description": title or doc_id,
                        "file_path": file_path,
                        "section": section,
                        "relevance_score": score,
                        "details": {"doc_id": doc_id, "title": title},
                    }

        sources.extend(rag_sources_map.values())

        # 5. ì¹´í…Œê³ ë¦¬ ê³„ì¸µ ì¶œì²˜ (ì‹ ê·œ)
        if hybrid_context.entities and hybrid_context.entities.get("categories"):
            for category in hybrid_context.entities["categories"][:3]:  # ìµœëŒ€ 3ê°œ
                hierarchy = self.kg.get_category_hierarchy(category)
                if "error" not in hierarchy:
                    path = []
                    if hierarchy.get("ancestors"):
                        path = [a["name"] for a in reversed(hierarchy["ancestors"])]
                    path.append(hierarchy.get("name", category))

                    sources.append(
                        {
                            "type": "category_hierarchy",
                            "icon": "ğŸ—‚ï¸",
                            "description": "ì¹´í…Œê³ ë¦¬ ê³„ì¸µ êµ¬ì¡°",
                            "path": path,
                            "level": hierarchy.get("level", 0),
                            "url": hierarchy.get("url", ""),
                            "details": {"category": category, "hierarchy_depth": len(path)},
                        }
                    )

        # 6. ì™¸ë¶€ ì‹ í˜¸ ì¶œì²˜ (Tavily ë‰´ìŠ¤, RSS, Reddit ë“±)
        if external_signals:
            for signal in external_signals[:5]:  # ìƒìœ„ 5ê°œë§Œ
                # ExternalSignal ê°ì²´ì—ì„œ ì •ë³´ ì¶”ì¶œ
                signal_source = getattr(signal, "source", "unknown")
                reliability = 0.7  # ê¸°ë³¸ê°’

                # ë©”íƒ€ë°ì´í„°ì—ì„œ ì‹ ë¢°ë„ ì¶”ì¶œ
                if hasattr(signal, "metadata") and signal.metadata:
                    reliability = signal.metadata.get("reliability_score", 0.7)

                # ì†ŒìŠ¤ ìœ í˜•ì— ë”°ë¼ ì•„ì´ì½˜ ê²°ì •
                if "tavily" in signal_source.lower() or "news" in signal_source.lower():
                    icon = "ğŸ“°"
                    source_type = "external_news"
                elif "reddit" in signal_source.lower():
                    icon = "ğŸ’¬"
                    source_type = "social_media"
                elif "rss" in signal_source.lower():
                    icon = "ğŸ“¡"
                    source_type = "rss_feed"
                elif "youtube" in signal_source.lower():
                    icon = "ğŸ“º"
                    source_type = "social_media"
                else:
                    icon = "ğŸŒ"
                    source_type = "external_source"

                sources.append(
                    {
                        "type": source_type,
                        "icon": icon,
                        "description": getattr(signal, "title", "Unknown"),
                        "source": signal_source,
                        "url": getattr(signal, "url", ""),
                        "published_at": getattr(signal, "published_at", ""),
                        "reliability_score": reliability,
                        "relevance_score": getattr(signal, "relevance_score", 0.5),
                        "details": {
                            "content_preview": getattr(signal, "content", "")[:200]
                            if hasattr(signal, "content")
                            else "",
                            "tier": getattr(signal, "tier", "unknown"),
                        },
                    }
                )

        # 7. AI ëª¨ë¸ ì¶œì²˜ (í•­ìƒ í¬í•¨)
        sources.append(
            {
                "type": "ai_model",
                "icon": "ğŸ¤–",
                "description": f"AI ë¶„ì„: {self.model}",
                "model": self.model,
                "disclaimer": "AIê°€ ìƒì„±í•œ ë¶„ì„ì…ë‹ˆë‹¤. ì¤‘ìš”í•œ ì˜ì‚¬ê²°ì • ì‹œ ì¶”ê°€ ê²€ì¦ì„ ê¶Œì¥í•©ë‹ˆë‹¤.",
                "generated_at": datetime.now().isoformat(),
            }
        )

        return sources

    def _extract_entity_names(self, ontology_facts) -> list[str]:
        """
        KG factsì—ì„œ ì—”í‹°í‹° ì´ë¦„ ì¶”ì¶œ

        Args:
            ontology_facts: ì˜¨í†¨ë¡œì§€ ì‚¬ì‹¤ ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” ë”•ì…”ë„ˆë¦¬

        Returns:
            ì—”í‹°í‹° ì´ë¦„ ë¦¬ìŠ¤íŠ¸ (ìµœëŒ€ 5ê°œ)
        """
        entities = set()

        if isinstance(ontology_facts, list):
            for fact in ontology_facts:
                if isinstance(fact, dict):
                    subject = fact.get("subject", "")
                    obj = fact.get("object", "")
                    if subject:
                        entities.add(subject)
                    if obj:
                        entities.add(obj)
        elif isinstance(ontology_facts, dict):
            # ë‹¨ì¼ factì¸ ê²½ìš°
            subject = ontology_facts.get("subject", "")
            obj = ontology_facts.get("object", "")
            if subject:
                entities.add(subject)
            if obj:
                entities.add(obj)

        # Noneì´ë‚˜ ë¹ˆ ë¬¸ìì—´ ì œê±° í›„ ìµœëŒ€ 5ê°œ ë°˜í™˜
        return list(filter(None, entities))[:5]

    def _extract_relation_types(self, ontology_facts) -> list[str]:
        """
        KG factsì—ì„œ ê´€ê³„ ìœ í˜• ì¶”ì¶œ

        Args:
            ontology_facts: ì˜¨í†¨ë¡œì§€ ì‚¬ì‹¤ ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” ë”•ì…”ë„ˆë¦¬

        Returns:
            ê´€ê³„ ìœ í˜• ë¦¬ìŠ¤íŠ¸ (ì¤‘ë³µ ì œê±°)
        """
        relations = set()

        if isinstance(ontology_facts, list):
            for fact in ontology_facts:
                if isinstance(fact, dict):
                    predicate = fact.get("predicate", "")
                    if predicate:
                        relations.add(predicate)
        elif isinstance(ontology_facts, dict):
            # ë‹¨ì¼ factì¸ ê²½ìš°
            predicate = ontology_facts.get("predicate", "")
            if predicate:
                relations.add(predicate)

        # Noneì´ë‚˜ ë¹ˆ ë¬¸ìì—´ ì œê±°
        return list(filter(None, relations))

    def _extract_mentioned_asins(
        self, hybrid_context: HybridContext, categories: dict[str, Any]
    ) -> list[dict[str, Any]]:
        """
        ì§ˆì˜ì—ì„œ ì–¸ê¸‰ëœ ì œí’ˆì˜ ASIN ì •ë³´ ì¶”ì¶œ (E2E Audit - 2026-01-27)

        Args:
            hybrid_context: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸
            categories: í¬ë¡¤ë§ëœ ì¹´í…Œê³ ë¦¬ ë°ì´í„°

        Returns:
            ì œí’ˆ ASIN ì •ë³´ ë¦¬ìŠ¤íŠ¸ [{asin, name, brand, rank, category, url}]
        """
        mentioned_products = []
        seen_asins = set()

        # 1. KG ì—”í‹°í‹°ì—ì„œ ì œí’ˆëª…/ë¸Œëœë“œ ì¶”ì¶œ
        mentioned_brands = set()
        if hybrid_context.entities:
            mentioned_brands = set(hybrid_context.entities.get("brands", []))

        # 2. ì¹´í…Œê³ ë¦¬ ë°ì´í„°ì—ì„œ ê´€ë ¨ ì œí’ˆ ASIN ì¶”ì¶œ
        for category_id, cat_data in categories.items():
            rank_records = cat_data.get("rank_records", [])

            for record in rank_records:
                asin = record.get("asin", "")
                brand = record.get("brand", "")
                product_name = record.get("product_name", record.get("title", ""))
                rank = record.get("rank", 0)

                # ì´ë¯¸ ì²˜ë¦¬ëœ ASIN ìŠ¤í‚µ
                if asin in seen_asins:
                    continue

                # ì–¸ê¸‰ëœ ë¸Œëœë“œì˜ ì œí’ˆë§Œ í¬í•¨ (ìµœëŒ€ 5ê°œ)
                if brand in mentioned_brands:
                    seen_asins.add(asin)
                    mentioned_products.append(
                        {
                            "asin": asin,
                            "name": product_name,
                            "brand": brand,
                            "rank": rank,
                            "category": category_id,
                            "url": f"https://www.amazon.com/dp/{asin}" if asin else "",
                        }
                    )

                    if len(mentioned_products) >= 5:
                        break

            if len(mentioned_products) >= 5:
                break

        # ìˆœìœ„ ê¸°ì¤€ ì •ë ¬
        mentioned_products.sort(key=lambda x: x.get("rank", 999))
        return mentioned_products[:5]

    def _format_sources_for_response(self, sources: list[dict[str, Any]]) -> str:
        """
        ì¶œì²˜ë¥¼ ì‘ë‹µì— í¬í•¨í•  í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (Perplexity ìŠ¤íƒ€ì¼)

        Args:
            sources: ì¶œì²˜ ì •ë³´ ë¦¬ìŠ¤íŠ¸

        Returns:
            ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ì¶œì²˜ ì„¹ì…˜
        """
        if not sources:
            return ""

        lines = ["\n\n---"]

        # ë°ì´í„° ì¶œì²˜ ì‹œì ì„ ëª…í™•íˆ í‘œì‹œ (ì‚¬ìš©ì ìš”ì²­)
        crawled_source = next((s for s in sources if s["type"] == "crawled_data"), None)
        if crawled_source:
            collected_at = crawled_source.get("collected_at", "")
            if collected_at:
                lines.append(f"ğŸ“… **ë°ì´í„° ê¸°ì¤€: Amazon US Best Sellers {collected_at} ìˆ˜ì§‘**")
                lines.append("*(Amazonì€ Best Sellers ìˆœìœ„ë¥¼ ë§¤ ì‹œê°„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤)*")
                lines.append("")

        lines.extend(["**ğŸ“š ì¶œì²˜ ë° ì°¸ê³ ìë£Œ:**", ""])

        for i, source in enumerate(sources, 1):
            icon = source.get("icon", "â€¢")
            desc = source.get("description", "ì•Œ ìˆ˜ ì—†ëŠ” ì¶œì²˜")

            if source["type"] == "crawled_data":
                collected = source.get("collected_at", "")
                url = source.get("url", "")
                details = source.get("details", {})
                total = details.get("total_products", 0)
                mentioned_products = source.get("mentioned_products", [])

                lines.append(f"{i}. {icon} **{desc}**")
                lines.append(f"   - ìˆ˜ì§‘ì¼: {collected}")
                if url:
                    lines.append(f"   - URL: {url}")
                if total > 0:
                    lines.append(f"   - ì´ ì œí’ˆ ìˆ˜: {total}ê°œ")

                # ASIN ê¸°ë°˜ ì œí’ˆ ì¶”ì  ì •ë³´ í‘œì‹œ (E2E Audit - 2026-01-27)
                if mentioned_products:
                    lines.append("   - ğŸ“¦ ê´€ë ¨ ì œí’ˆ (ASIN ê¸°ì¤€):")
                    for prod in mentioned_products[:3]:  # ìµœëŒ€ 3ê°œ í‘œì‹œ
                        asin = prod.get("asin", "")
                        name = prod.get("name", "")
                        rank = prod.get("rank", "")
                        category = prod.get("category", "")
                        lines.append(f"     â€¢ [{asin}] {name} (#{rank} in {category})")

                lines.append("")

            elif source["type"] == "knowledge_graph":
                fact_count = source.get("fact_count", 0)
                entities = source.get("entities", [])
                relations = source.get("relations", [])
                lines.append(f"{i}. {icon} **{desc}** ({fact_count}ê°œ ê´€ê³„)")
                if entities:
                    lines.append(f"   - ì£¼ìš” ì—”í‹°í‹°: {', '.join(entities[:3])}")
                if relations:
                    lines.append(f"   - ê´€ê³„ ìœ í˜•: {', '.join(relations[:3])}")
                lines.append("")

            elif source["type"] == "ontology_inference":
                conf = source.get("confidence", 0) * 100
                rule_name = source.get("rule_name", "ì•Œ ìˆ˜ ì—†ìŒ")
                lines.append(f"{i}. {icon} **{desc}**")
                lines.append(f"   - ì‹ ë¢°ë„: {conf:.0f}%")
                lines.append(f"   - ê·œì¹™: {rule_name}")
                lines.append("")

            elif source["type"] == "rag_document":
                file_path = source.get("file_path", "")
                section = source.get("section", "")
                score = source.get("relevance_score", 0)
                file_name = file_path.split("/")[-1] if file_path else ""
                lines.append(f"{i}. {icon} **{desc}**")
                if file_name:
                    lines.append(f"   - íŒŒì¼: {file_name}")
                if section:
                    lines.append(f"   - ì„¹ì…˜: {section}")
                if score > 0:
                    lines.append(f"   - ê´€ë ¨ë„: {score:.2f}")
                lines.append("")

            elif source["type"] == "category_hierarchy":
                path = source.get("path", [])
                level = source.get("level", 0)
                url = source.get("url", "")
                lines.append(f"{i}. {icon} **{desc}**")
                if path:
                    lines.append(f"   - ê³„ì¸µ: {' > '.join(path)}")
                lines.append(f"   - ë ˆë²¨: {level}")
                if url:
                    lines.append(f"   - URL: {url}")
                lines.append("")

            elif source["type"] in ["external_news", "rss_feed"]:
                # ì™¸ë¶€ ë‰´ìŠ¤ / RSS í”¼ë“œ (Tavily, Allure, WWD ë“±)
                url = source.get("url", "")
                published_at = source.get("published_at", "")
                reliability = source.get("reliability_score", 0.7) * 100
                source_name = source.get("source", "")
                lines.append(f"{i}. {icon} **{desc}** (ì‹ ë¢°ë„: {reliability:.0f}%)")
                if source_name:
                    lines.append(f"   - ì¶œì²˜: {source_name}")
                if published_at:
                    lines.append(f"   - ë‚ ì§œ: {published_at}")
                if url:
                    lines.append(f"   - URL: {url}")
                lines.append("")

            elif source["type"] == "social_media":
                # ì†Œì…œ ë¯¸ë””ì–´ (Reddit, YouTube ë“±)
                url = source.get("url", "")
                published_at = source.get("published_at", "")
                reliability = source.get("reliability_score", 0.5) * 100
                source_name = source.get("source", "")
                relevance = source.get("relevance_score", 0)
                lines.append(f"{i}. {icon} **{desc}** (ì‹ ë¢°ë„: {reliability:.0f}%)")
                if source_name:
                    lines.append(f"   - í”Œë«í¼: {source_name}")
                if published_at:
                    lines.append(f"   - ë‚ ì§œ: {published_at}")
                if relevance > 0:
                    lines.append(f"   - ê´€ë ¨ë„: {relevance:.2f}")
                if url:
                    lines.append(f"   - URL: {url}")
                lines.append("")

            elif source["type"] == "ai_model":
                model = source.get("model", "")
                disclaimer = source.get("disclaimer", "")
                lines.append(f"{i}. {icon} **{desc}**")
                if model:
                    lines.append(f"   - ëª¨ë¸: {model}")
                if disclaimer:
                    lines.append(f"   - ì°¸ê³ : {disclaimer}")
                lines.append("")

        return "\n".join(lines)

    async def _collect_external_signals(
        self, query: str, entities: dict[str, list[str]] | None = None
    ) -> list[Any]:
        """
        ì™¸ë¶€ ì‹ í˜¸ ìˆ˜ì§‘ (Tavily ë‰´ìŠ¤, RSS, Reddit)

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            entities: ì¶”ì¶œëœ ì—”í‹°í‹° (ë¸Œëœë“œ, ì¹´í…Œê³ ë¦¬ ë“±)

        Returns:
            ExternalSignal ë¦¬ìŠ¤íŠ¸
        """
        try:
            # ì™¸ë¶€ ì‹ í˜¸ ìˆ˜ì§‘ê¸° lazy initialization
            if self._external_signal_collector is None:
                try:
                    from src.tools.collectors.external_signal_collector import (
                        ExternalSignalCollector,
                    )

                    self._external_signal_collector = ExternalSignalCollector()
                    await self._external_signal_collector.initialize()
                except ImportError as e:
                    self.logger.warning(f"ExternalSignalCollector not available: {e}")
                    return []
                except Exception as e:
                    self.logger.warning(f"Failed to initialize ExternalSignalCollector: {e}")
                    return []

            # ì—”í‹°í‹°ì—ì„œ ë¸Œëœë“œ/í† í”½ ì¶”ì¶œ
            brands = []
            topics = []

            if entities:
                brands = entities.get("brands", [])
                categories = entities.get("categories", [])
                # ì¹´í…Œê³ ë¦¬ë¥¼ í† í”½ìœ¼ë¡œ ë³€í™˜
                topics = [cat.replace("_", " ") for cat in categories]

            # ê¸°ë³¸ê°’ ì„¤ì •
            if not brands:
                brands = ["LANEIGE", "K-Beauty"]
            if not topics:
                topics = ["skincare trends", "beauty news"]

            # Tavily ë‰´ìŠ¤ ê²€ìƒ‰ (ë¹„ë™ê¸°) - ê²€ìƒ‰ ê¸°ê°„ í™•ì¥
            all_signals = []

            try:
                tavily_signals = await self._external_signal_collector.fetch_tavily_news(
                    brands=brands[:3],  # ìµœëŒ€ 3ê°œ ë¸Œëœë“œ
                    topics=topics[:2],  # ìµœëŒ€ 2ê°œ í† í”½
                    days=14,  # 2ì£¼ë¡œ í™•ì¥ (ë” ë§ì€ ë‰´ìŠ¤ ìˆ˜ì§‘)
                    max_results=8,  # ìµœëŒ€ 8ê°œë¡œ ì¦ê°€
                )
                all_signals.extend(tavily_signals)
                self.logger.info(f"Collected {len(tavily_signals)} Tavily news signals")
            except Exception as e:
                self.logger.warning(f"Tavily news fetch failed: {e}")

            # RSS í”¼ë“œ ìˆ˜ì§‘ (ì„ íƒì )
            try:
                keywords = brands + topics
                rss_signals = await self._external_signal_collector.fetch_all_rss_feeds(
                    keywords=keywords[:5]
                )
                # ìƒìœ„ 3ê°œë§Œ ì¶”ê°€
                all_signals.extend(rss_signals[:3])
                self.logger.debug(f"Collected {len(rss_signals)} RSS signals")
            except Exception as e:
                self.logger.debug(f"RSS fetch skipped: {e}")

            # ì‹ ë¢°ë„ * ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ 8ê°œ ë°˜í™˜
            all_signals.sort(
                key=lambda s: (
                    getattr(s, "metadata", {}).get("reliability_score", 0.7)
                    * getattr(s, "relevance_score", 0.5)
                ),
                reverse=True,
            )

            return all_signals[:8]

        except Exception as e:
            self.logger.error(f"External signal collection failed: {e}")
            return []

    def _estimate_cost(self, prompt_tokens: int, completion_tokens: int) -> float:
        """ë¹„ìš© ì¶”ì •"""
        input_cost = (prompt_tokens / 1_000_000) * 0.40
        output_cost = (completion_tokens / 1_000_000) * 1.60
        return round(input_cost + output_cost, 6)

    def get_conversation_history(self, limit: int = 10) -> list[dict]:
        """ëŒ€í™” ê¸°ë¡ ì¡°íšŒ"""
        return self.context.get_conversation_history(limit)

    def clear_conversation(self) -> None:
        """ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"""
        self.context.reset()
        self.query_rewriter.clear_cache()

    def _get_failed_signal_collectors(self) -> list[str]:
        """
        ì‚¬ìš© ë¶ˆê°€ëŠ¥í•œ ì™¸ë¶€ ì‹ í˜¸ ìˆ˜ì§‘ê¸° ëª©ë¡ ë°˜í™˜

        Returns:
            ì‹¤íŒ¨í•œ ìˆ˜ì§‘ê¸° ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        """
        failed = []

        # ExternalSignalCollector ì²´í¬
        if self._external_signal_collector is None:
            try:
                import importlib.util

                if importlib.util.find_spec("src.tools.external_signal_collector") is None:
                    failed.append("External Signals (Tavily/RSS/Reddit)")
            except ImportError:
                failed.append("External Signals (Tavily/RSS/Reddit)")

        return failed

    async def _maybe_rewrite_query(self, query: str) -> RewriteResult:
        """
        í•„ìš”ì‹œ ì§ˆë¬¸ ì¬êµ¬ì„± (ëŒ€í™” ë§¥ë½ ê¸°ë°˜)

        í›„ì† ì§ˆë¬¸ì—ì„œ ì§€ì‹œì–´(ê·¸ê²ƒ, ê·¸ ì œí’ˆ, í•´ë‹¹ ë“±)ë¥¼ ì´ì „ ëŒ€í™” ë§¥ë½ì„ ì°¸ì¡°í•˜ì—¬
        êµ¬ì²´ì ì¸ ëŒ€ìƒìœ¼ë¡œ ì¹˜í™˜í•©ë‹ˆë‹¤.

        ìµœì í™”:
        1. ëŒ€í™” íˆìŠ¤í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ
        2. ì§€ì‹œì–´ê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ (LLM í˜¸ì¶œ ì ˆì•½)

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸

        Returns:
            RewriteResult ê°ì²´
        """
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ
        history = self.context.get_conversation_history(limit=3)
        if not history:
            return create_rewrite_result_no_change(query)

        # ì§€ì‹œì–´ê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ (LLM í˜¸ì¶œ ì ˆì•½)
        if not self.query_rewriter.needs_rewrite(query):
            return create_rewrite_result_no_change(query)

        # LLMìœ¼ë¡œ ì¬êµ¬ì„±
        return await self.query_rewriter.rewrite(query, history)

    def get_last_hybrid_context(self) -> HybridContext | None:
        """ë§ˆì§€ë§‰ í•˜ì´ë¸Œë¦¬ë“œ ì»¨í…ìŠ¤íŠ¸"""
        return self._last_hybrid_context

    def get_knowledge_graph(self) -> KnowledgeGraph:
        """ì§€ì‹ ê·¸ë˜í”„ ë°˜í™˜"""
        return self.kg

    def get_reasoner(self) -> OntologyReasoner:
        """ì¶”ë¡ ê¸° ë°˜í™˜"""
        return self.reasoner

    async def explain_last_response(self) -> str:
        """ë§ˆì§€ë§‰ ì‘ë‹µì˜ ì¶”ë¡  ê³¼ì • ì„¤ëª…"""
        if not self._last_hybrid_context or not self._last_hybrid_context.inferences:
            return "ì„¤ëª…í•  ì¶”ë¡  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."

        return self.reasoner.explain_all(self._last_hybrid_context.inferences)


class HybridChatbotSession:
    """í•˜ì´ë¸Œë¦¬ë“œ ì±—ë´‡ ì„¸ì…˜ ê´€ë¦¬ (ë©€í‹° ìœ ì € ì§€ì›)"""

    def __init__(
        self,
        knowledge_graph: KnowledgeGraph | None = None,
        reasoner: OntologyReasoner | None = None,
    ):
        """
        Args:
            knowledge_graph: ê³µìœ  ì§€ì‹ ê·¸ë˜í”„
            reasoner: ê³µìœ  ì¶”ë¡ ê¸°
        """
        self._sessions: dict[str, HybridChatbotAgent] = {}
        self._shared_kg = knowledge_graph
        self._shared_reasoner = reasoner

    def get_or_create(self, session_id: str, **kwargs) -> HybridChatbotAgent:
        """ì„¸ì…˜ë³„ ì±—ë´‡ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
        if session_id not in self._sessions:
            self._sessions[session_id] = HybridChatbotAgent(
                knowledge_graph=self._shared_kg, reasoner=self._shared_reasoner, **kwargs
            )
        return self._sessions[session_id]

    def close_session(self, session_id: str) -> None:
        """ì„¸ì…˜ ì¢…ë£Œ"""
        if session_id in self._sessions:
            del self._sessions[session_id]

    def list_sessions(self) -> list[str]:
        """í™œì„± ì„¸ì…˜ ëª©ë¡"""
        return list(self._sessions.keys())
