"""
Hybrid Chatbot Agent
Ontology-RAG í•˜ì´ë¸Œë¦¬ë“œ ì±—ë´‡ ì—ì´ì „íŠ¸

Flow:
1. ì¿¼ë¦¬ì—ì„œ ì—”í‹°í‹°/ì˜ë„ ì¶”ì¶œ
2. Knowledge Graphì—ì„œ ê´€ë ¨ ì‚¬ì‹¤ ì¡°íšŒ
3. Ontology Reasonerë¡œ ì¶”ë¡ 
4. RAGë¡œ ê°€ì´ë“œë¼ì¸ ê²€ìƒ‰
5. í†µí•© ì»¨í…ìŠ¤íŠ¸ë¡œ LLM ì‘ë‹µ ìƒì„±
"""

from datetime import datetime
from typing import Dict, Any, List, Optional

from litellm import acompletion

from src.ontology.knowledge_graph import KnowledgeGraph
from src.ontology.reasoner import OntologyReasoner
from src.ontology.business_rules import register_all_rules
from src.domain.entities.relations import InferenceResult

from src.rag.hybrid_retriever import HybridRetriever, HybridContext, EntityExtractor
from src.rag.context_builder import ContextBuilder, CompactContextBuilder
from src.rag.router import RAGRouter, QueryType
from src.rag.retriever import DocumentRetriever
from src.rag.templates import ResponseTemplates

from src.memory.context import ContextManager

from src.monitoring.logger import AgentLogger
from src.monitoring.tracer import ExecutionTracer
from src.monitoring.metrics import QualityMetrics


class HybridChatbotAgent:
    """
    Ontology-RAG í•˜ì´ë¸Œë¦¬ë“œ ì±—ë´‡ ì—ì´ì „íŠ¸

    ê¸°ì¡´ ChatbotAgentì™€ì˜ ì°¨ì´ì :
    - ì˜¨í†¨ë¡œì§€ ì¶”ë¡  ê²°ê³¼ ê¸°ë°˜ ì‘ë‹µ
    - ì§€ì‹ ê·¸ë˜í”„ì—ì„œ ê´€ë ¨ ì‚¬ì‹¤ ì¡°íšŒ
    - ì¶”ë¡  ê³¼ì • ì„¤ëª… ì œê³µ

    ì‚¬ìš© ì˜ˆ:
        agent = HybridChatbotAgent()
        result = await agent.chat("LANEIGE Lip Care ê²½ìŸë ¥ ë¶„ì„í•´ì¤˜")
    """

    def __init__(
        self,
        model: str = "gpt-4.1-mini",
        docs_dir: str = ".",
        knowledge_graph: Optional[KnowledgeGraph] = None,
        reasoner: Optional[OntologyReasoner] = None,
        logger: Optional[AgentLogger] = None,
        tracer: Optional[ExecutionTracer] = None,
        metrics: Optional[QualityMetrics] = None,
        context_manager: Optional[ContextManager] = None
    ):
        """
        Args:
            model: LLM ëª¨ë¸ëª…
            docs_dir: RAG ë¬¸ì„œ ë””ë ‰í† ë¦¬
            knowledge_graph: ì§€ì‹ ê·¸ë˜í”„ (ê³µìœ  ê°€ëŠ¥)
            reasoner: ì¶”ë¡ ê¸° (ê³µìœ  ê°€ëŠ¥)
            logger: ë¡œê±°
            tracer: ì¶”ì ê¸°
            metrics: ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸°
            context_manager: ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ì
        """
        self.model = model

        # ì˜¨í†¨ë¡œì§€ ì»´í¬ë„ŒíŠ¸
        self.kg = knowledge_graph or KnowledgeGraph()
        self.reasoner = reasoner or OntologyReasoner(self.kg)

        # ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ë“±ë¡
        if not self.reasoner.rules:
            register_all_rules(self.reasoner)

        # RAG ì»´í¬ë„ŒíŠ¸
        self.doc_retriever = DocumentRetriever(docs_dir)
        self.router = RAGRouter()

        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸°
        self.hybrid_retriever = HybridRetriever(
            knowledge_graph=self.kg,
            reasoner=self.reasoner,
            doc_retriever=self.doc_retriever,
            auto_init_rules=False
        )

        # ì»¨í…ìŠ¤íŠ¸ ë¹Œë”
        self.context_builder = ContextBuilder(max_tokens=3000)
        self.compact_builder = CompactContextBuilder(max_tokens=1500)

        # í…œí”Œë¦¿
        self.templates = ResponseTemplates()

        # ë©”ëª¨ë¦¬
        self.context = context_manager or ContextManager()

        # ëª¨ë‹ˆí„°ë§
        self.logger = logger or AgentLogger("hybrid_chatbot")
        self.tracer = tracer
        self.metrics = metrics

        # í˜„ì¬ ë°ì´í„° ì»¨í…ìŠ¤íŠ¸
        self._current_data: Dict[str, Any] = {}

        # ë§ˆì§€ë§‰ í•˜ì´ë¸Œë¦¬ë“œ ì»¨í…ìŠ¤íŠ¸
        self._last_hybrid_context: Optional[HybridContext] = None

    def set_data_context(self, data: Dict[str, Any]) -> None:
        """
        í˜„ì¬ ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ ì„¤ì •

        Args:
            data: ì§€í‘œ/ì¸ì‚¬ì´íŠ¸ ë°ì´í„°
        """
        self._current_data = data

        # ì§€ì‹ ê·¸ë˜í”„ ì—…ë°ì´íŠ¸
        if data:
            self.hybrid_retriever.update_knowledge_graph(
                metrics_data=data
            )

    async def chat(
        self,
        user_message: str,
        session_id: Optional[str] = None,
        include_reasoning: bool = True
    ) -> Dict[str, Any]:
        """
        ì‚¬ìš©ì ì§ˆë¬¸ì— ì‘ë‹µ

        Args:
            user_message: ì‚¬ìš©ì ë©”ì‹œì§€
            session_id: ì„¸ì…˜ ID
            include_reasoning: ì¶”ë¡  ê³¼ì • í¬í•¨ ì—¬ë¶€

        Returns:
            {
                "response": "...",
                "query_type": "...",
                "inferences": [...],
                "sources": [...],
                "suggestions": [...]
            }
        """
        self.logger.info(f"User query: {user_message[:50]}...")
        start_time = datetime.now()

        if self.tracer:
            self.tracer.start_span("hybrid_chatbot_response", {
                "query_length": len(user_message)
            })

        try:
            # 1. ì¿¼ë¦¬ ë¼ìš°íŒ… (ì˜ë„ ë¶„ë¥˜)
            route_result = self.router.route(user_message)
            query_type = route_result.get("query_type")

            self.logger.debug(f"Query type: {query_type}")

            # 2. Fallback ì²˜ë¦¬ (ì˜ë„ ë¶ˆëª…)
            if query_type == QueryType.UNKNOWN:
                fallback_response = route_result.get("fallback_message", "")
                return {
                    "response": fallback_response,
                    "query_type": "unknown",
                    "is_fallback": True,
                    "inferences": [],
                    "sources": [],
                    "suggestions": self._get_fallback_suggestions()
                }

            # 3. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ì¶”ë¡  + RAG)
            if self.tracer:
                self.tracer.start_span("hybrid_retrieval")

            hybrid_context = await self.hybrid_retriever.retrieve(
                query=user_message,
                current_metrics=self._current_data,
                include_explanations=include_reasoning
            )
            self._last_hybrid_context = hybrid_context

            if self.tracer:
                self.tracer.end_span("completed")

            # 4. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            if self.tracer:
                self.tracer.start_span("build_context")

            # ì¿¼ë¦¬ ìœ í˜•ì— ë”°ë¼ ë¹Œë” ì„ íƒ
            if query_type in [QueryType.DEFINITION, QueryType.INTERPRETATION]:
                # ê°„ë‹¨í•œ ì§ˆë¬¸ì€ ì»´íŒ©íŠ¸ ë¹Œë”
                context = self.compact_builder.build(
                    hybrid_context=hybrid_context,
                    current_metrics=self._current_data,
                    query=user_message,
                    knowledge_graph=self.kg
                )
            else:
                # ë¶„ì„ ì§ˆë¬¸ì€ í’€ ë¹Œë” (ì¹´í…Œê³ ë¦¬ ê³„ì¸µ ì¸ì‹ í¬í•¨)
                context = self.context_builder.build(
                    hybrid_context=hybrid_context,
                    current_metrics=self._current_data,
                    query=user_message,
                    knowledge_graph=self.kg
                )

            if self.tracer:
                self.tracer.end_span("completed")

            # 5. LLM ì‘ë‹µ ìƒì„±
            if self.tracer:
                self.tracer.start_span("llm_response")

            response = await self._generate_response(
                user_message=user_message,
                query_type=query_type,
                context=context,
                inferences=hybrid_context.inferences
            )

            if self.tracer:
                self.tracer.end_span("completed")

            # 6. ì¶œì²˜ ì •ë³´ ì¶”ì¶œ ë° í¬ë§·íŒ…
            sources = self._extract_sources(hybrid_context)
            formatted_sources = self._format_sources_for_response(sources)

            # 7. ì‘ë‹µì— ì¶œì²˜ ì„¹ì…˜ ì¶”ê°€
            full_response = response + formatted_sources

            # 8. ëŒ€í™” ê¸°ë¡ ì €ì¥
            self.context.add_user_message(user_message)
            self.context.add_assistant_message(full_response)

            # 9. í›„ì† ì§ˆë¬¸ ì œì•ˆ
            suggestions = self._generate_suggestions(
                query_type=query_type,
                entities=hybrid_context.entities,
                inferences=hybrid_context.inferences
            )

            duration = (datetime.now() - start_time).total_seconds()

            if self.tracer:
                self.tracer.end_span("completed")

            self.logger.info(
                f"Response generated in {duration:.2f}s",
                {"query_type": query_type.value if hasattr(query_type, 'value') else str(query_type)}
            )

            return {
                "response": full_response,
                "query_type": query_type.value if hasattr(query_type, 'value') else str(query_type),
                "is_fallback": False,
                "inferences": [inf.to_dict() for inf in hybrid_context.inferences],
                "sources": sources,
                "suggestions": suggestions,
                "entities": hybrid_context.entities,
                "stats": {
                    "inferences_count": len(hybrid_context.inferences),
                    "rag_chunks_count": len(hybrid_context.rag_chunks),
                    "response_time_ms": duration * 1000
                }
            }

        except Exception as e:
            if self.tracer:
                self.tracer.end_span("failed", str(e))

            self.logger.error(f"Chat error: {e}")

            return {
                "response": "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                "query_type": "error",
                "is_fallback": True,
                "error": str(e),
                "inferences": [],
                "sources": [],
                "suggestions": self._get_fallback_suggestions()
            }

    async def _generate_response(
        self,
        user_message: str,
        query_type: QueryType,
        context: str,
        inferences: List[InferenceResult]
    ) -> str:
        """LLM ì‘ë‹µ ìƒì„±"""
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì¹´í…Œê³ ë¦¬ ê³„ì¸µ ì¸ì‹ ì¶”ê°€)
        system_prompt = self.context_builder.build_system_prompt(
            include_guardrails=True
        )

        # ì¹´í…Œê³ ë¦¬ ê³„ì¸µ ì„¤ëª… ì¶”ê°€
        system_prompt += """

## ì¹´í…Œê³ ë¦¬ ê³„ì¸µ êµ¬ì¡° ì¸ì‹
- ì œí’ˆì€ ì—¬ëŸ¬ ê³„ì¸µì˜ ì¹´í…Œê³ ë¦¬ì— ë™ì‹œì— ì†Œì†ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤
- ì˜ˆ: íŠ¹ì • ë¦½ì¼€ì–´ ì œí’ˆì´ "Lip Care"ì—ì„œ 4ìœ„ì´ë©´ì„œ, ìƒìœ„ ì¹´í…Œê³ ë¦¬ì¸ "Beauty & Personal Care"ì—ì„œëŠ” 73ìœ„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤
- ìˆœìœ„ë¥¼ ì–¸ê¸‰í•  ë•ŒëŠ” ë°˜ë“œì‹œ ì–´ëŠ ì¹´í…Œê³ ë¦¬ì—ì„œì˜ ìˆœìœ„ì¸ì§€ ëª…ì‹œí•˜ì„¸ìš”
- ì¹´í…Œê³ ë¦¬ ê°„ ìˆœìœ„ ì°¨ì´ê°€ ìˆëŠ” ê²½ìš°, ì´ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ í˜„ìƒì…ë‹ˆë‹¤ (í•˜ìœ„ ì¹´í…Œê³ ë¦¬ê°€ ë” ì„¸ë¶„í™”ë˜ì–´ ê²½ìŸ ë²”ìœ„ê°€ ì¢ê¸° ë•Œë¬¸)
"""

        # ëŒ€í™” íˆìŠ¤í† ë¦¬
        conversation = self.context.get_conversation_summary()

        # ì¶”ë¡  ê²°ê³¼ ê°•ì¡°
        inference_summary = ""
        if inferences:
            inference_lines = []
            for inf in inferences[:3]:
                inference_lines.append(f"- [{inf.insight_type.value}] {inf.insight}")
            inference_summary = "\n".join(inference_lines)

        # ì¹´í…Œê³ ë¦¬ ê³„ì¸µ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë§ˆì§€ë§‰ í•˜ì´ë¸Œë¦¬ë“œ ì»¨í…ìŠ¤íŠ¸ì—ì„œ)
        category_hierarchy_context = ""
        if self._last_hybrid_context and self._last_hybrid_context.entities:
            category_hierarchy_context = self._build_category_hierarchy_context(
                self._last_hybrid_context.entities
            )

        user_prompt = f"""
{context}

---

## ì¹´í…Œê³ ë¦¬ ê³„ì¸µ ì •ë³´
{category_hierarchy_context if category_hierarchy_context else "ì¹´í…Œê³ ë¦¬ ê³„ì¸µ ì •ë³´ ì—†ìŒ"}

## ì˜¨í†¨ë¡œì§€ ì¶”ë¡  ê²°ê³¼ (ìš°ì„  ì°¸ê³ )
{inference_summary if inference_summary else "ê´€ë ¨ ì¶”ë¡  ê²°ê³¼ ì—†ìŒ"}

## ì´ì „ ëŒ€í™”
{conversation if conversation else "ì—†ìŒ"}

## ì‚¬ìš©ì ì§ˆë¬¸
{user_message}

---

ìš”êµ¬ì‚¬í•­:
1. ì˜¨í†¨ë¡œì§€ ì¶”ë¡  ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€
2. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë¥¼ ì¸ìš©í•˜ì—¬ ë‹µë³€
3. ìˆœìœ„ë¥¼ ì–¸ê¸‰í•  ë•ŒëŠ” ì¹´í…Œê³ ë¦¬ë¥¼ ëª…ì‹œ (ì˜ˆ: "Lip Careì—ì„œ 4ìœ„", "Beauty & Personal Care ì „ì²´ì—ì„œëŠ” 73ìœ„")
4. ë¶ˆí™•ì‹¤í•œ ë¶€ë¶„ì€ ëª…í™•íˆ ë°í˜
5. ë‹¨ì •ì  í‘œí˜„ ëŒ€ì‹  ê°€ëŠ¥ì„± í‘œí˜„ ì‚¬ìš©
6. ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ë‹µë³€
"""

        try:
            response = await acompletion(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.3,
                max_tokens=800
            )

            if response.choices:
                answer = response.choices[0].message.content
            else:
                answer = "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

            # í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡
            if self.metrics and hasattr(response, 'usage'):
                self.metrics.record_llm_call(
                    model=self.model,
                    prompt_tokens=response.usage.prompt_tokens,
                    completion_tokens=response.usage.completion_tokens,
                    latency_ms=0,
                    cost=self._estimate_cost(
                        response.usage.prompt_tokens,
                        response.usage.completion_tokens
                    )
                )

            # ê°€ë“œë ˆì¼ ì ìš©
            answer = self.templates.apply_guardrails(answer)

            return answer

        except Exception as e:
            self.logger.error(f"LLM call failed: {e}")
            return self._generate_fallback_response(inferences)

    def _generate_fallback_response(
        self,
        inferences: List[InferenceResult]
    ) -> str:
        """í´ë°± ì‘ë‹µ ìƒì„±"""
        if inferences:
            lines = ["ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ë“œë¦½ë‹ˆë‹¤:\n"]
            for inf in inferences[:2]:
                lines.append(f"- {inf.insight}")
                if inf.recommendation:
                    lines.append(f"  â†’ ê¶Œì¥: {inf.recommendation}")
            return "\n".join(lines)

        return "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."

    def _generate_suggestions(
        self,
        query_type: QueryType,
        entities: Dict[str, List[str]],
        inferences: List[InferenceResult]
    ) -> List[str]:
        """í›„ì† ì§ˆë¬¸ ì œì•ˆ"""
        suggestions = []

        # ì¶”ë¡  ê²°ê³¼ ê¸°ë°˜ ì œì•ˆ
        if inferences:
            for inf in inferences[:2]:
                if "ê²½ìŸ" in inf.insight or "COMPETITIVE" in inf.insight_type.value:
                    suggestions.append("ì£¼ìš” ê²½ìŸì‚¬ ë¶„ì„ì„ í•´ì£¼ì„¸ìš”")
                if "ê°€ê²©" in inf.insight or "PRICE" in inf.insight_type.value:
                    suggestions.append("ê°€ê²© ì „ëµì— ëŒ€í•´ ë” ì•Œë ¤ì£¼ì„¸ìš”")
                if "ì„±ì¥" in inf.insight or "GROWTH" in inf.insight_type.value:
                    suggestions.append("ì„±ì¥ ê¸°íšŒë¥¼ ë” êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”")

        # ì¿¼ë¦¬ ìœ í˜•ë³„ ì œì•ˆ
        if query_type == QueryType.DEFINITION:
            suggestions.extend([
                "ì´ ì§€í‘œì˜ í•´ì„ ê¸°ì¤€ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
                "ê´€ë ¨ëœ ë‹¤ë¥¸ ì§€í‘œëŠ” ë¬´ì—‡ì´ ìˆë‚˜ìš”?"
            ])
        elif query_type == QueryType.INTERPRETATION:
            suggestions.extend([
                "ì´ ìˆ˜ì¹˜ê°€ ì¢‹ì€ ê±´ê°€ìš”?",
                "ê°œì„ ì„ ìœ„í•œ ì•¡ì…˜ì´ ìˆë‚˜ìš”?"
            ])
        elif query_type == QueryType.ANALYSIS:
            suggestions.extend([
                "ì‹œê³„ì—´ íŠ¸ë Œë“œë¥¼ ì•Œë ¤ì£¼ì„¸ìš”",
                "ê²½ìŸì‚¬ì™€ ë¹„êµí•´ì£¼ì„¸ìš”"
            ])

        # ì—”í‹°í‹° ê¸°ë°˜ ì œì•ˆ
        if entities.get("brands"):
            brand = entities["brands"][0]
            suggestions.append(f"{brand}ì˜ ìµœê·¼ ìˆœìœ„ ë³€ë™ì€?")

        if entities.get("categories"):
            category = entities["categories"][0]
            suggestions.append(f"{category} ì¹´í…Œê³ ë¦¬ Top 5ëŠ”?")

        # ì¤‘ë³µ ì œê±° ë° ìƒìœ„ 3ê°œ
        unique_suggestions = list(dict.fromkeys(suggestions))
        return unique_suggestions[:3]

    def _get_fallback_suggestions(self) -> List[str]:
        """í´ë°± ì œì•ˆ"""
        return [
            "SoS(ì ìœ ìœ¨)ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”",
            "ì˜¤ëŠ˜ì˜ ì£¼ìš” ì¸ì‚¬ì´íŠ¸ëŠ”?",
            "LANEIGE í˜„ì¬ ìˆœìœ„ëŠ”?"
        ]

    def _build_category_hierarchy_context(
        self,
        entities: Dict[str, List[str]]
    ) -> str:
        """
        ì¹´í…Œê³ ë¦¬ ê³„ì¸µ ì»¨í…ìŠ¤íŠ¸ ìƒì„±

        Args:
            entities: ì¶”ì¶œëœ ì—”í‹°í‹° (ì¹´í…Œê³ ë¦¬, ì œí’ˆ ë“±)

        Returns:
            ì¹´í…Œê³ ë¦¬ ê³„ì¸µ ì •ë³´ ë¬¸ìì—´
        """
        if not self.kg:
            return ""

        context_parts = []

        # ì¹´í…Œê³ ë¦¬ ì—”í‹°í‹°ì—ì„œ ê³„ì¸µ ì •ë³´ ì¶”ì¶œ
        if not entities:
            return ""

        categories = entities.get("categories", [])
        for category in categories:
            hierarchy = self.kg.get_category_hierarchy(category)
            if "error" in hierarchy:
                continue

            # í˜„ì¬ ì¹´í…Œê³ ë¦¬ ì •ë³´
            context_parts.append(f"**{hierarchy['name']}** (Level {hierarchy['level']})")

            # ìƒìœ„ ì¹´í…Œê³ ë¦¬ ê²½ë¡œ
            if hierarchy.get('ancestors'):
                path = " > ".join([a['name'] for a in reversed(hierarchy['ancestors'])])
                context_parts.append(f"  - ìƒìœ„ ê²½ë¡œ: {path} > {hierarchy['name']}")

            # í•˜ìœ„ ì¹´í…Œê³ ë¦¬
            if hierarchy.get('descendants'):
                children = ", ".join([d['name'] for d in hierarchy['descendants'][:5]])
                context_parts.append(f"  - í•˜ìœ„ ì¹´í…Œê³ ë¦¬: {children}")

            context_parts.append("")

        # ì œí’ˆì˜ ì¹´í…Œê³ ë¦¬ ì»¨í…ìŠ¤íŠ¸ (ìˆœìœ„ ê´€ë ¨ ì§ˆë¬¸ ì‹œ)
        products = entities.get("products", [])
        for product_asin in products:
            product_ctx = self.kg.get_product_category_context(product_asin)
            if product_ctx.get("categories"):
                context_parts.append(f"**ì œí’ˆ {product_asin}ì˜ ì¹´í…Œê³ ë¦¬ë³„ ìˆœìœ„:**")
                for cat_info in product_ctx["categories"]:
                    hierarchy = cat_info.get("hierarchy", {})
                    cat_name = hierarchy.get("name", cat_info.get("category_id"))
                    rank = cat_info.get("rank", "N/A")
                    context_parts.append(f"  - {cat_name}: {rank}ìœ„")
                context_parts.append("")

        return "\n".join(context_parts) if context_parts else ""

    def _extract_sources(self, hybrid_context: HybridContext) -> List[Dict[str, Any]]:
        """
        ì¶œì²˜ ì •ë³´ ì¶”ì¶œ (Perplexity/Liner ìŠ¤íƒ€ì¼ ìƒì„¸ ì¶œì²˜ ì œê³µ)

        Args:
            hybrid_context: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸

        Returns:
            ì¶œì²˜ ì •ë³´ ë¦¬ìŠ¤íŠ¸ (ìœ í˜•ë³„ ìƒì„¸ ì •ë³´ í¬í•¨)
        """
        sources = []

        # 1. í¬ë¡¤ë§ ë°ì´í„° ì¶œì²˜ - URL ë° ìƒì„¸ ì •ë³´ ì¶”ê°€
        if self._current_data:
            metadata = self._current_data.get("metadata", {})
            data_date = metadata.get("data_date", "")
            categories = self._current_data.get("categories", {})

            total_products = sum(
                len(cat_data.get("rank_records", []))
                for cat_data in categories.values()
            ) if categories else 0

            sources.append({
                "type": "crawled_data",
                "icon": "ğŸ“Š",
                "description": "Amazon Best Sellers í¬ë¡¤ë§ ë°ì´í„°",
                "collected_at": data_date,
                "url": "https://www.amazon.com/gp/bestsellers/beauty",
                "details": {
                    "categories": list(categories.keys()) if categories else [],
                    "total_products": total_products,
                    "snapshot_date": data_date
                }
            })

        # 2. Knowledge Graph ì¶œì²˜ - ì—”í‹°í‹° ë° ê´€ê³„ ì •ë³´ ì¶”ê°€
        if hybrid_context.ontology_facts:
            sources.append({
                "type": "knowledge_graph",
                "icon": "ğŸ”—",
                "description": "ì§€ì‹ ê·¸ë˜í”„ ê´€ê³„ ë°ì´í„°",
                "fact_count": len(hybrid_context.ontology_facts),
                "entities": self._extract_entity_names(hybrid_context.ontology_facts),
                "relations": self._extract_relation_types(hybrid_context.ontology_facts),
                "details": {
                    "source": "Amazon US ì‹¤ì‹œê°„ ë°ì´í„° ê¸°ë°˜ ì§€ì‹ ê·¸ë˜í”„",
                    "fact_count": len(hybrid_context.ontology_facts)
                }
            })

        # 3. ì˜¨í†¨ë¡œì§€ ì¶”ë¡  ì¶œì²˜ - ê·œì¹™ ìƒì„¸ ì •ë³´
        if hybrid_context.inferences:
            for inf in hybrid_context.inferences:
                sources.append({
                    "type": "ontology_inference",
                    "icon": "ğŸ§ ",
                    "description": f"ì˜¨í†¨ë¡œì§€ ê·œì¹™: {inf.rule_name}",
                    "rule_name": inf.rule_name,
                    "confidence": inf.confidence,
                    "evidence": inf.evidence,
                    "insight_type": inf.insight_type.value if hasattr(inf.insight_type, 'value') else str(inf.insight_type),
                    "details": {
                        "insight": inf.insight,
                        "recommendation": inf.recommendation
                    }
                })

        # 4. RAG ë¬¸ì„œ ì¶œì²˜ - íŒŒì¼ ê²½ë¡œ ë° ê´€ë ¨ì„± ì ìˆ˜
        rag_sources_map = {}
        for chunk in hybrid_context.rag_chunks:
            metadata = chunk.get("metadata", {})
            doc_id = metadata.get("doc_id", "")
            title = metadata.get("title", "")
            file_path = metadata.get("file_path", "")
            score = chunk.get("score", 0)
            section = metadata.get("section", "")

            if doc_id or title:
                doc_key = doc_id or title
                # ê°™ì€ ë¬¸ì„œì˜ ì—¬ëŸ¬ ì²­í¬ ì¤‘ ê°€ì¥ ë†’ì€ ì ìˆ˜ë§Œ ìœ ì§€
                if doc_key not in rag_sources_map or score > rag_sources_map[doc_key].get("relevance_score", 0):
                    rag_sources_map[doc_key] = {
                        "type": "rag_document",
                        "icon": "ğŸ“„",
                        "description": title or doc_id,
                        "file_path": file_path,
                        "section": section,
                        "relevance_score": score,
                        "details": {
                            "doc_id": doc_id,
                            "title": title
                        }
                    }

        sources.extend(rag_sources_map.values())

        # 5. ì¹´í…Œê³ ë¦¬ ê³„ì¸µ ì¶œì²˜ (ì‹ ê·œ)
        if hybrid_context.entities and hybrid_context.entities.get("categories"):
            for category in hybrid_context.entities["categories"][:3]:  # ìµœëŒ€ 3ê°œ
                hierarchy = self.kg.get_category_hierarchy(category)
                if "error" not in hierarchy:
                    path = []
                    if hierarchy.get("ancestors"):
                        path = [a["name"] for a in reversed(hierarchy["ancestors"])]
                    path.append(hierarchy.get("name", category))

                    sources.append({
                        "type": "category_hierarchy",
                        "icon": "ğŸ—‚ï¸",
                        "description": "ì¹´í…Œê³ ë¦¬ ê³„ì¸µ êµ¬ì¡°",
                        "path": path,
                        "level": hierarchy.get("level", 0),
                        "url": hierarchy.get("url", ""),
                        "details": {
                            "category": category,
                            "hierarchy_depth": len(path)
                        }
                    })

        # 6. AI ëª¨ë¸ ì¶œì²˜ (í•­ìƒ í¬í•¨)
        sources.append({
            "type": "ai_model",
            "icon": "ğŸ¤–",
            "description": f"AI ë¶„ì„: {self.model}",
            "model": self.model,
            "disclaimer": "AIê°€ ìƒì„±í•œ ë¶„ì„ì…ë‹ˆë‹¤. ì¤‘ìš”í•œ ì˜ì‚¬ê²°ì • ì‹œ ì¶”ê°€ ê²€ì¦ì„ ê¶Œì¥í•©ë‹ˆë‹¤.",
            "generated_at": datetime.now().isoformat()
        })

        return sources

    def _extract_entity_names(self, ontology_facts) -> List[str]:
        """
        KG factsì—ì„œ ì—”í‹°í‹° ì´ë¦„ ì¶”ì¶œ

        Args:
            ontology_facts: ì˜¨í†¨ë¡œì§€ ì‚¬ì‹¤ ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” ë”•ì…”ë„ˆë¦¬

        Returns:
            ì—”í‹°í‹° ì´ë¦„ ë¦¬ìŠ¤íŠ¸ (ìµœëŒ€ 5ê°œ)
        """
        entities = set()

        if isinstance(ontology_facts, list):
            for fact in ontology_facts:
                if isinstance(fact, dict):
                    subject = fact.get("subject", "")
                    obj = fact.get("object", "")
                    if subject:
                        entities.add(subject)
                    if obj:
                        entities.add(obj)
        elif isinstance(ontology_facts, dict):
            # ë‹¨ì¼ factì¸ ê²½ìš°
            subject = ontology_facts.get("subject", "")
            obj = ontology_facts.get("object", "")
            if subject:
                entities.add(subject)
            if obj:
                entities.add(obj)

        # Noneì´ë‚˜ ë¹ˆ ë¬¸ìì—´ ì œê±° í›„ ìµœëŒ€ 5ê°œ ë°˜í™˜
        return list(filter(None, entities))[:5]

    def _extract_relation_types(self, ontology_facts) -> List[str]:
        """
        KG factsì—ì„œ ê´€ê³„ ìœ í˜• ì¶”ì¶œ

        Args:
            ontology_facts: ì˜¨í†¨ë¡œì§€ ì‚¬ì‹¤ ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” ë”•ì…”ë„ˆë¦¬

        Returns:
            ê´€ê³„ ìœ í˜• ë¦¬ìŠ¤íŠ¸ (ì¤‘ë³µ ì œê±°)
        """
        relations = set()

        if isinstance(ontology_facts, list):
            for fact in ontology_facts:
                if isinstance(fact, dict):
                    predicate = fact.get("predicate", "")
                    if predicate:
                        relations.add(predicate)
        elif isinstance(ontology_facts, dict):
            # ë‹¨ì¼ factì¸ ê²½ìš°
            predicate = ontology_facts.get("predicate", "")
            if predicate:
                relations.add(predicate)

        # Noneì´ë‚˜ ë¹ˆ ë¬¸ìì—´ ì œê±°
        return list(filter(None, relations))

    def _format_sources_for_response(self, sources: List[Dict[str, Any]]) -> str:
        """
        ì¶œì²˜ë¥¼ ì‘ë‹µì— í¬í•¨í•  í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (Perplexity ìŠ¤íƒ€ì¼)

        Args:
            sources: ì¶œì²˜ ì •ë³´ ë¦¬ìŠ¤íŠ¸

        Returns:
            ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ì¶œì²˜ ì„¹ì…˜
        """
        if not sources:
            return ""

        lines = ["\n\n---"]

        # ë°ì´í„° ì¶œì²˜ ì‹œì ì„ ëª…í™•íˆ í‘œì‹œ (ì‚¬ìš©ì ìš”ì²­)
        crawled_source = next((s for s in sources if s["type"] == "crawled_data"), None)
        if crawled_source:
            collected_at = crawled_source.get("collected_at", "")
            if collected_at:
                lines.append(f"ğŸ“… **ë°ì´í„° ê¸°ì¤€: Amazon US Best Sellers {collected_at} ìˆ˜ì§‘**")
                lines.append("*(Amazonì€ Best Sellers ìˆœìœ„ë¥¼ ë§¤ ì‹œê°„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤)*")
                lines.append("")

        lines.extend(["**ğŸ“š ì¶œì²˜ ë° ì°¸ê³ ìë£Œ:**", ""])

        for i, source in enumerate(sources, 1):
            icon = source.get("icon", "â€¢")
            desc = source.get("description", "ì•Œ ìˆ˜ ì—†ëŠ” ì¶œì²˜")

            if source["type"] == "crawled_data":
                collected = source.get("collected_at", "")
                url = source.get("url", "")
                details = source.get("details", {})
                total = details.get("total_products", 0)
                lines.append(f"{i}. {icon} **{desc}**")
                lines.append(f"   - ìˆ˜ì§‘ì¼: {collected}")
                if url:
                    lines.append(f"   - URL: {url}")
                if total > 0:
                    lines.append(f"   - ì´ ì œí’ˆ ìˆ˜: {total}ê°œ")
                lines.append("")

            elif source["type"] == "knowledge_graph":
                fact_count = source.get("fact_count", 0)
                entities = source.get("entities", [])
                relations = source.get("relations", [])
                lines.append(f"{i}. {icon} **{desc}** ({fact_count}ê°œ ê´€ê³„)")
                if entities:
                    lines.append(f"   - ì£¼ìš” ì—”í‹°í‹°: {', '.join(entities[:3])}")
                if relations:
                    lines.append(f"   - ê´€ê³„ ìœ í˜•: {', '.join(relations[:3])}")
                lines.append("")

            elif source["type"] == "ontology_inference":
                conf = source.get("confidence", 0) * 100
                rule_name = source.get("rule_name", "ì•Œ ìˆ˜ ì—†ìŒ")
                lines.append(f"{i}. {icon} **{desc}**")
                lines.append(f"   - ì‹ ë¢°ë„: {conf:.0f}%")
                lines.append(f"   - ê·œì¹™: {rule_name}")
                lines.append("")

            elif source["type"] == "rag_document":
                file_path = source.get("file_path", "")
                section = source.get("section", "")
                score = source.get("relevance_score", 0)
                file_name = file_path.split("/")[-1] if file_path else ""
                lines.append(f"{i}. {icon} **{desc}**")
                if file_name:
                    lines.append(f"   - íŒŒì¼: {file_name}")
                if section:
                    lines.append(f"   - ì„¹ì…˜: {section}")
                if score > 0:
                    lines.append(f"   - ê´€ë ¨ë„: {score:.2f}")
                lines.append("")

            elif source["type"] == "category_hierarchy":
                path = source.get("path", [])
                level = source.get("level", 0)
                url = source.get("url", "")
                lines.append(f"{i}. {icon} **{desc}**")
                if path:
                    lines.append(f"   - ê³„ì¸µ: {' > '.join(path)}")
                lines.append(f"   - ë ˆë²¨: {level}")
                if url:
                    lines.append(f"   - URL: {url}")
                lines.append("")

            elif source["type"] == "ai_model":
                model = source.get("model", "")
                disclaimer = source.get("disclaimer", "")
                lines.append(f"{i}. {icon} **{desc}**")
                if model:
                    lines.append(f"   - ëª¨ë¸: {model}")
                if disclaimer:
                    lines.append(f"   - ì°¸ê³ : {disclaimer}")
                lines.append("")

        return "\n".join(lines)

    def _estimate_cost(self, prompt_tokens: int, completion_tokens: int) -> float:
        """ë¹„ìš© ì¶”ì •"""
        input_cost = (prompt_tokens / 1_000_000) * 0.40
        output_cost = (completion_tokens / 1_000_000) * 1.60
        return round(input_cost + output_cost, 6)

    def get_conversation_history(self, limit: int = 10) -> List[Dict]:
        """ëŒ€í™” ê¸°ë¡ ì¡°íšŒ"""
        return self.context.get_conversation_history(limit)

    def clear_conversation(self) -> None:
        """ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"""
        self.context.reset()

    def get_last_hybrid_context(self) -> Optional[HybridContext]:
        """ë§ˆì§€ë§‰ í•˜ì´ë¸Œë¦¬ë“œ ì»¨í…ìŠ¤íŠ¸"""
        return self._last_hybrid_context

    def get_knowledge_graph(self) -> KnowledgeGraph:
        """ì§€ì‹ ê·¸ë˜í”„ ë°˜í™˜"""
        return self.kg

    def get_reasoner(self) -> OntologyReasoner:
        """ì¶”ë¡ ê¸° ë°˜í™˜"""
        return self.reasoner

    async def explain_last_response(self) -> str:
        """ë§ˆì§€ë§‰ ì‘ë‹µì˜ ì¶”ë¡  ê³¼ì • ì„¤ëª…"""
        if not self._last_hybrid_context or not self._last_hybrid_context.inferences:
            return "ì„¤ëª…í•  ì¶”ë¡  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."

        return self.reasoner.explain_all(self._last_hybrid_context.inferences)


class HybridChatbotSession:
    """í•˜ì´ë¸Œë¦¬ë“œ ì±—ë´‡ ì„¸ì…˜ ê´€ë¦¬ (ë©€í‹° ìœ ì € ì§€ì›)"""

    def __init__(
        self,
        knowledge_graph: Optional[KnowledgeGraph] = None,
        reasoner: Optional[OntologyReasoner] = None
    ):
        """
        Args:
            knowledge_graph: ê³µìœ  ì§€ì‹ ê·¸ë˜í”„
            reasoner: ê³µìœ  ì¶”ë¡ ê¸°
        """
        self._sessions: Dict[str, HybridChatbotAgent] = {}
        self._shared_kg = knowledge_graph
        self._shared_reasoner = reasoner

    def get_or_create(
        self,
        session_id: str,
        **kwargs
    ) -> HybridChatbotAgent:
        """ì„¸ì…˜ë³„ ì±—ë´‡ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
        if session_id not in self._sessions:
            self._sessions[session_id] = HybridChatbotAgent(
                knowledge_graph=self._shared_kg,
                reasoner=self._shared_reasoner,
                **kwargs
            )
        return self._sessions[session_id]

    def close_session(self, session_id: str) -> None:
        """ì„¸ì…˜ ì¢…ë£Œ"""
        if session_id in self._sessions:
            del self._sessions[session_id]

    def list_sessions(self) -> List[str]:
        """í™œì„± ì„¸ì…˜ ëª©ë¡"""
        return list(self._sessions.keys())
