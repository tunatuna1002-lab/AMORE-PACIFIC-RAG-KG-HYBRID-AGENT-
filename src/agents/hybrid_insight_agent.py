"""
Hybrid Insight Agent
Ontology-RAG í•˜ì´ë¸Œë¦¬ë“œ ì¸ì‚¬ì´íŠ¸ ìƒì„± ì—ì´ì „íŠ¸

Flow:
1. í˜„ì¬ ë°ì´í„°ë¡œ Knowledge Graph ì—…ë°ì´íŠ¸
2. Ontology Reasonerë¡œ ê·œì¹™ ê¸°ë°˜ ì¶”ë¡ 
3. RAGë¡œ ê´€ë ¨ ê°€ì´ë“œë¼ì¸ ê²€ìƒ‰
4. ì¶”ë¡  ê²°ê³¼ + RAG ì»¨í…ìŠ¤íŠ¸ë¡œ LLM ì¸ì‚¬ì´íŠ¸ ìƒì„±
"""

import json
from datetime import datetime
from typing import Dict, Any, List, Optional

from litellm import acompletion

from src.ontology.knowledge_graph import KnowledgeGraph
from src.ontology.reasoner import OntologyReasoner
from src.ontology.business_rules import register_all_rules
from src.domain.entities.relations import (
    InferenceResult,
    InsightType,
    RelationType,
    Relation,
)

from src.rag.hybrid_retriever import HybridRetriever, HybridContext
from src.rag.context_builder import ContextBuilder
from src.rag.retriever import DocumentRetriever
from src.rag.templates import ResponseTemplates

from src.monitoring.logger import AgentLogger
from src.monitoring.tracer import ExecutionTracer
from src.monitoring.metrics import QualityMetrics
from src.tools.external_signal_collector import ExternalSignalCollector
from src.tools.market_intelligence import MarketIntelligenceEngine, DataLayer
from src.tools.source_manager import SourceManager, InsightSourceBuilder

# New collectors (Phase 1 & 2)
try:
    from src.tools.google_trends_collector import GoogleTrendsCollector

    GOOGLE_TRENDS_AVAILABLE = True
except ImportError as e:
    from src.monitoring.logger import get_logger

    _logger = get_logger("hybrid_insight")
    _logger.warning(
        f"GoogleTrendsCollector not available - Google Trends signals will be skipped: {e}"
    )
    GOOGLE_TRENDS_AVAILABLE = False

try:
    from src.tools.youtube_collector import YouTubeCollector

    YOUTUBE_AVAILABLE = True
except ImportError as e:
    from src.monitoring.logger import get_logger

    _logger = get_logger("hybrid_insight")
    _logger.warning(
        f"YouTubeCollector not available - YouTube signals will be skipped: {e}"
    )
    YOUTUBE_AVAILABLE = False


class HybridInsightAgent:
    """
    Ontology-RAG í•˜ì´ë¸Œë¦¬ë“œ ì¸ì‚¬ì´íŠ¸ ìƒì„± ì—ì´ì „íŠ¸

    ê¸°ì¡´ InsightAgentì™€ì˜ ì°¨ì´ì :
    - ì˜¨í†¨ë¡œì§€ ì¶”ë¡  ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¸ì‚¬ì´íŠ¸ ìƒì„±
    - ê·œì¹™ ê¸°ë°˜ ì¶”ë¡ ìœ¼ë¡œ ì¼ê´€ì„± ë³´ì¥
    - ì¶”ë¡  ê³¼ì • ì„¤ëª… ê°€ëŠ¥ (Explainability)

    ì‚¬ìš© ì˜ˆ:
        agent = HybridInsightAgent(model="gpt-4.1-mini")
        result = await agent.execute(metrics_data)
    """

    def __init__(
        self,
        model: str = "gpt-4.1-mini",
        docs_dir: str = ".",
        knowledge_graph: Optional[KnowledgeGraph] = None,
        reasoner: Optional[OntologyReasoner] = None,
        logger: Optional[AgentLogger] = None,
        tracer: Optional[ExecutionTracer] = None,
        metrics: Optional[QualityMetrics] = None,
    ):
        """
        Args:
            model: LLM ëª¨ë¸ëª…
            docs_dir: RAG ë¬¸ì„œ ë””ë ‰í† ë¦¬
            knowledge_graph: ì§€ì‹ ê·¸ë˜í”„ (ê³µìœ  ê°€ëŠ¥)
            reasoner: ì¶”ë¡ ê¸° (ê³µìœ  ê°€ëŠ¥)
            logger: ë¡œê±°
            tracer: ì¶”ì ê¸°
            metrics: ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸°
        """
        import os

        self.model = model
        # Temperature: ì¸ì‚¬ì´íŠ¸ ì „ìš© í™˜ê²½ë³€ìˆ˜ > ì¼ë°˜ í™˜ê²½ë³€ìˆ˜ > ê¸°ë³¸ê°’(0.6)
        # ì¸ì‚¬ì´íŠ¸ëŠ” ì°½ì˜ì  ë¶„ì„/ì „ëµ ì œì•ˆì„ ìœ„í•´ ì•½ê°„ ë†’ì€ temperature ì‚¬ìš© (E2E Audit - 2026-01-27)
        from src.shared.constants import INSIGHT_TEMPERATURE

        self.temperature = float(
            os.getenv(
                "LLM_INSIGHT_TEMPERATURE",
                os.getenv("LLM_TEMPERATURE", str(INSIGHT_TEMPERATURE)),
            )
        )

        # ì˜¨í†¨ë¡œì§€ ì»´í¬ë„ŒíŠ¸
        self.kg = knowledge_graph or KnowledgeGraph()
        self.reasoner = reasoner or OntologyReasoner(self.kg)

        # ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ë“±ë¡
        if not self.reasoner.rules:
            register_all_rules(self.reasoner)

        # RAG ì»´í¬ë„ŒíŠ¸
        self.doc_retriever = DocumentRetriever(docs_dir)

        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸°
        self.hybrid_retriever = HybridRetriever(
            knowledge_graph=self.kg,
            reasoner=self.reasoner,
            doc_retriever=self.doc_retriever,
            auto_init_rules=False,  # ì´ë¯¸ ë“±ë¡ë¨
        )

        # ì»¨í…ìŠ¤íŠ¸ ë¹Œë”
        self.context_builder = ContextBuilder(max_tokens=4000)

        # í…œí”Œë¦¿
        self.templates = ResponseTemplates()

        # ëª¨ë‹ˆí„°ë§
        self.logger = logger or AgentLogger("hybrid_insight")
        self.tracer = tracer
        self.metrics = metrics

        # ê²°ê³¼ ìºì‹œ
        self._results: Dict[str, Any] = {}
        self._last_hybrid_context: Optional[HybridContext] = None

        # External Signal Collector
        self._signal_collector: Optional[ExternalSignalCollector] = None

        # Market Intelligence Engine
        self._market_intelligence: Optional[MarketIntelligenceEngine] = None
        self._insight_source_builder: Optional[InsightSourceBuilder] = None

        # New collectors (Phase 1 & 2)
        self._google_trends: Optional[GoogleTrendsCollector] = None
        self._youtube_collector: Optional[YouTubeCollector] = None

    async def execute(
        self,
        metrics_data: Dict[str, Any],
        crawl_data: Optional[Dict[str, Any]] = None,
        crawl_summary: Optional[Dict] = None,
    ) -> Dict[str, Any]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ì¸ì‚¬ì´íŠ¸ ìƒì„±

        Args:
            metrics_data: ì§€í‘œ ì—ì´ì „íŠ¸ ê²°ê³¼
            crawl_data: í¬ë¡¤ë§ ì›ë³¸ ë°ì´í„° (KG ì—…ë°ì´íŠ¸ìš©)
            crawl_summary: í¬ë¡¤ë§ ìš”ì•½

        Returns:
            {
                "status": "completed",
                "daily_insight": "...",
                "action_items": [...],
                "highlights": [...],
                "inferences": [...],
                "explanations": [...]
            }
        """
        self.logger.agent_start("HybridInsightAgent", "í•˜ì´ë¸Œë¦¬ë“œ ì¸ì‚¬ì´íŠ¸ ìƒì„±")
        start_time = datetime.now()

        if self.metrics:
            self.metrics.record_agent_start("hybrid_insight")

        if self.tracer:
            self.tracer.start_span("hybrid_insight_agent")

        try:
            # ë°ì´í„° ì¶œì²˜ ì •ë³´ ì¶”ì¶œ
            data_source = self._extract_data_source_info(metrics_data, crawl_data)

            results = {
                "status": "completed",
                "generated_at": datetime.now().isoformat(),
                "data_source": data_source,  # ë°ì´í„° ì¶œì²˜ ì •ë³´ ì¶”ê°€
                "daily_insight": "",
                "action_items": [],
                "highlights": [],
                "warnings": [],
                "inferences": [],
                "explanations": [],
                "hybrid_stats": {},
            }

            # 1. Knowledge Graph ì—…ë°ì´íŠ¸
            if self.tracer:
                self.tracer.start_span("update_knowledge_graph")

            kg_stats = self._update_knowledge_graph(crawl_data, metrics_data)
            results["hybrid_stats"]["kg_update"] = kg_stats

            if self.tracer:
                self.tracer.end_span("completed")

            # 2. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ì¶”ë¡  + RAG)
            if self.tracer:
                self.tracer.start_span("hybrid_retrieval")

            hybrid_context = await self._run_hybrid_retrieval(metrics_data)
            self._last_hybrid_context = hybrid_context
            results["inferences"] = [inf.to_dict() for inf in hybrid_context.inferences]

            if self.tracer:
                self.tracer.end_span("completed")

            # 3. RAG â†’ KG ì§€ì‹ ì¶”ì¶œ
            rag_kg_stats = self._ingest_rag_knowledge(hybrid_context.rag_chunks)
            results["hybrid_stats"]["rag_to_kg"] = rag_kg_stats

            # 4. ì¶”ë¡  ì„¤ëª… ìƒì„±
            if self.tracer:
                self.tracer.start_span("generate_explanations")

            explanations = self._generate_explanations(hybrid_context.inferences)
            results["explanations"] = explanations

            if self.tracer:
                self.tracer.end_span("completed")

            # 5. External Signal ìˆ˜ì§‘ (LLM í˜¸ì¶œ ì „ì— ìˆ˜í–‰)
            if self.tracer:
                self.tracer.start_span("collect_external_signals")

            external_signals = await self._collect_external_signals()
            results["external_signals"] = external_signals
            signal_kg_stats = self._ingest_external_signals(external_signals)
            results["hybrid_stats"]["signal_to_kg"] = signal_kg_stats

            # ì‹¤íŒ¨í•œ ì‹ í˜¸ ìˆ˜ì§‘ê¸° ì¶”ì 
            results["failed_signals"] = self._get_failed_signal_collectors()

            if self.tracer:
                self.tracer.end_span("completed")

            # 5.5. Market Intelligence ìˆ˜ì§‘ (Layer 2-4)
            if self.tracer:
                self.tracer.start_span("collect_market_intelligence")

            market_intelligence = await self._collect_market_intelligence()
            results["market_intelligence"] = market_intelligence

            if self.tracer:
                self.tracer.end_span("completed")

            # 6. ì¼ì¼ ì¸ì‚¬ì´íŠ¸ ìƒì„± (LLM + External Signal + Market Intelligence í¬í•¨)
            if self.tracer:
                self.tracer.start_span("generate_daily_insight")

            daily_insight = await self._generate_daily_insight(
                hybrid_context,
                metrics_data,
                crawl_summary,
                external_signals,
                market_intelligence,
                results.get("failed_signals", []),
            )
            results["daily_insight"] = daily_insight

            if self.tracer:
                self.tracer.end_span("completed")

            # 7. ì•¡ì…˜ ì•„ì´í…œ ì¶”ì¶œ
            if self.tracer:
                self.tracer.start_span("extract_actions")

            action_items = self._extract_action_items(
                hybrid_context.inferences, metrics_data
            )
            results["action_items"] = action_items

            if self.tracer:
                self.tracer.end_span("completed")

            # 8. í•˜ì´ë¼ì´íŠ¸ ì¶”ì¶œ
            results["highlights"] = self._extract_highlights(
                hybrid_context.inferences, metrics_data
            )

            # 9. ê²½ê³  ìˆ˜ì§‘
            alerts = metrics_data.get("alerts", [])
            results["warnings"] = [
                a for a in alerts if a.get("severity") in ["warning", "critical"]
            ]

            if self.tracer:
                self.tracer.end_span("completed")

            # 10. í†µê³„
            results["hybrid_stats"].update(
                {
                    "inferences_count": len(hybrid_context.inferences),
                    "rag_chunks_count": len(hybrid_context.rag_chunks),
                    "ontology_facts_count": len(hybrid_context.ontology_facts),
                    "external_signals_count": len(external_signals.get("signals", [])),
                    "market_intelligence_sources": len(
                        market_intelligence.get("sources", [])
                    ),
                }
            )

            self._results = results
            duration = (datetime.now() - start_time).total_seconds()

            if self.tracer:
                self.tracer.end_span("completed")

            if self.metrics:
                self.metrics.record_agent_complete(
                    "hybrid_insight",
                    {
                        "action_items": len(results["action_items"]),
                        "inferences": len(results["inferences"]),
                    },
                )

            self.logger.agent_complete(
                "HybridInsightAgent",
                duration,
                f"{len(results['inferences'])} inferences, "
                f"{len(results['action_items'])} actions",
            )

            return results

        except Exception as e:
            duration = (datetime.now() - start_time).total_seconds()

            if self.tracer:
                self.tracer.end_span("failed", str(e))

            if self.metrics:
                self.metrics.record_agent_error("hybrid_insight", str(e))

            self.logger.agent_error("HybridInsightAgent", str(e), duration)
            raise

    def _update_knowledge_graph(
        self, crawl_data: Optional[Dict], metrics_data: Dict
    ) -> Dict[str, int]:
        """Knowledge Graph ì—…ë°ì´íŠ¸"""
        stats = {"crawl_relations": 0, "metrics_relations": 0}

        if crawl_data:
            stats["crawl_relations"] = self.kg.load_from_crawl_data(crawl_data)
            self.logger.debug(
                f"KG updated from crawl: {stats['crawl_relations']} relations"
            )

        if metrics_data:
            stats["metrics_relations"] = self.kg.load_from_metrics_data(metrics_data)
            self.logger.debug(
                f"KG updated from metrics: {stats['metrics_relations']} relations"
            )

        return stats

    async def _run_hybrid_retrieval(self, metrics_data: Dict) -> HybridContext:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰"""
        # ì¼ì¼ ì¸ì‚¬ì´íŠ¸ìš© ì¿¼ë¦¬
        query = "LANEIGE ì˜¤ëŠ˜ì˜ Amazon ë² ìŠ¤íŠ¸ì…€ëŸ¬ ì„±ê³¼ ë¶„ì„"

        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
        context = await self.hybrid_retriever.retrieve(
            query=query, current_metrics=metrics_data, include_explanations=True
        )

        self.logger.info(
            f"Hybrid retrieval: {len(context.inferences)} inferences, "
            f"{len(context.rag_chunks)} RAG chunks"
        )

        return context

    def _generate_explanations(
        self, inferences: List[InferenceResult]
    ) -> List[Dict[str, Any]]:
        """ì¶”ë¡  ì„¤ëª… ìƒì„±"""
        explanations = []

        for inf in inferences:
            explanation = {
                "rule": inf.rule_name,
                "type": inf.insight_type.value,
                "insight": inf.insight,
                "explanation": self.reasoner.explain_inference(inf),
                "confidence": inf.confidence,
            }
            explanations.append(explanation)

        return explanations

    async def _generate_daily_insight(
        self,
        hybrid_context: HybridContext,
        metrics_data: Dict,
        crawl_summary: Optional[Dict],
        external_signals: Optional[Dict] = None,
        market_intelligence: Optional[Dict] = None,
        failed_signals: Optional[List[str]] = None,
    ) -> str:
        """ì¼ì¼ ì¸ì‚¬ì´íŠ¸ ìƒì„± (LLM)"""
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = self.context_builder.build(
            hybrid_context=hybrid_context,
            current_metrics=metrics_data,
            query="ì˜¤ëŠ˜ì˜ LANEIGE Amazon ë² ìŠ¤íŠ¸ì…€ëŸ¬ ì¸ì‚¬ì´íŠ¸",
            knowledge_graph=self.kg,
        )

        # External Signal ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        external_context = ""
        if external_signals and external_signals.get("report_section"):
            external_context = f"""

## ì™¸ë¶€ íŠ¸ë Œë“œ ì‹ í˜¸

{external_signals["report_section"]}

_â€» ìœ„ ì™¸ë¶€ ì‹ í˜¸ëŠ” ì „ë¬¸ ë§¤ì²´(Allure, Byrdie ë“±), Reddit, TikTok ë“±ì—ì„œ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤._
"""

        # Market Intelligence ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        market_context = ""
        if market_intelligence and market_intelligence.get("insight_section"):
            market_context = f"""

## ì‹œì¥ ì¸í…”ë¦¬ì „ìŠ¤ (4-Layer ë¶„ì„)

{market_intelligence["insight_section"]}

"""

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        system_prompt = self.context_builder.build_system_prompt(
            include_guardrails=True
        )

        # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ (4-Layer Why ë¶„ì„ í…œí”Œë¦¿)
        reference_section = self._build_reference_section(
            hybrid_context, external_signals, market_intelligence
        )
        user_prompt = f"""
{context}
{external_context}
{market_context}
{reference_section}
---

## ìš”ì²­ì‚¬í•­

ìœ„ ë¶„ì„ ê²°ê³¼ì™€ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì˜¤ëŠ˜ì˜ LANEIGE Amazon US ì¼ì¼ ì¸ì‚¬ì´íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

### ì¶œë ¥ í˜•ì‹ (ë°˜ë“œì‹œ ì´ êµ¬ì¡°ë¥¼ ë”°ë¥´ì„¸ìš”):

```markdown
# LANEIGE Amazon US ì¼ì¼ ì¸ì‚¬ì´íŠ¸

## ğŸ“Œ ì˜¤ëŠ˜ì˜ í•µì‹¬
[ê°€ì¥ ì¤‘ìš”í•œ ë³€í™” 1-2ê°€ì§€ + ê·¸ ì›ì¸ì„ ì—°ê²°í•˜ì—¬ ì„¤ëª…]
ì˜ˆ: "Lip Sleeping Mask ìˆœìœ„ ìƒìŠ¹ì€ Q3 Americas ë§¤ì¶œ +6.9% ì„±ì¥[2]ê³¼ ìµœê·¼ TikTok ë°”ì´ëŸ´[3]ì˜ ë³µí•© íš¨ê³¼ë¡œ íŒë‹¨ë©ë‹ˆë‹¤."

## ğŸ” ì›ì¸ ë¶„ì„ (Why?)

### Layer 4: ê±°ì‹œê²½ì œ/ë¬´ì—­
â€¢ [ê´€ì„¸ì²­ ìˆ˜ì¶œì… ë°ì´í„° ê¸°ë°˜ ë¶„ì„] [1]
â€¢ [í™˜ìœ¨/ê´€ì„¸ ì˜í–¥ ë¶„ì„]

### Layer 3: ì‚°ì—…/ê¸°ì—… ë™í–¥
â€¢ [ì•„ëª¨ë ˆí¼ì‹œí”½ IR ì‹¤ì  ê¸°ë°˜ ë¶„ì„] [2]
â€¢ [ë¸Œëœë“œ ì „ëµ/ìº í˜ì¸ ì˜í–¥]

### Layer 2: ì†Œë¹„ì íŠ¸ë Œë“œ
â€¢ [Reddit/SNS íŠ¸ë Œë“œ ë¶„ì„] [3]
â€¢ [ë·°í‹° ë§¤ì²´ ë³´ë„ ë‚´ìš©]

### Layer 1: Amazon ì„±ê³¼
â€¢ [ìˆœìœ„ ë³€ë™, SoS, ê°€ê²© ë“± í•µì‹¬ ì§€í‘œ]
â€¢ [ê²½ìŸì‚¬ ë™í–¥]

## âš ï¸ ì£¼ì˜ ì‚¬í•­
â€¢ [ë¦¬ìŠ¤í¬ ë˜ëŠ” ëª¨ë‹ˆí„°ë§ í•„ìš” ì‚¬í•­]

## ğŸ’¡ ê¶Œì¥ ì•¡ì…˜
1. [ì¦‰ì‹œ ì‹¤í–‰] êµ¬ì²´ì  ì•¡ì…˜ 1
2. [ëª¨ë‹ˆí„°ë§] êµ¬ì²´ì  ì•¡ì…˜ 2
3. [ê²€í†  í•„ìš”] êµ¬ì²´ì  ì•¡ì…˜ 3

## ğŸ“š ì°¸ê³ ìë£Œ
[ì œê³µëœ ì°¸ê³ ìë£Œ ëª©ë¡ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©]
```

### ì‘ì„± ì›ì¹™:
1. **ì¸ê³¼ê´€ê³„ ì¤‘ì‹¬**: "Aê°€ ë°œìƒí–ˆë‹¤" â†’ "AëŠ” B ë•Œë¬¸ì— ë°œìƒí•œ ê²ƒìœ¼ë¡œ íŒë‹¨ëœë‹¤"
2. **ì¶œì²˜ í•„ìˆ˜ ì¸ìš©**: ëª¨ë“  ì‚¬ì‹¤ ì£¼ì¥ì— [1], [2] í˜•íƒœë¡œ ì¶œì²˜ ì¸ìš©
3. **ê³„ì¸µì  ë¶„ì„**: Layer 4(ê±°ì‹œ) â†’ Layer 1(Amazon)ìœ¼ë¡œ ì›ì¸-ê²°ê³¼ ì—°ê²°
4. **ì •ëŸ‰ì  í‘œí˜„**: "ì¦ê°€" ëŒ€ì‹  "+12%", "ë§ìŒ" ëŒ€ì‹  "2,400 ì—…ë³´íŠ¸"
5. **ê°€ì„¤ì  í‘œí˜„**: í™•ì‹¤í•˜ì§€ ì•Šì€ ë‚´ìš©ì€ "~ë¡œ íŒë‹¨ë©ë‹ˆë‹¤", "~ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤" ì‚¬ìš©
"""

        try:
            response = await acompletion(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                temperature=self.temperature,
                max_tokens=1200,
            )

            insight = response.choices[0].message.content

            # í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡
            if self.metrics and hasattr(response, "usage"):
                self.metrics.record_llm_call(
                    model=self.model,
                    prompt_tokens=response.usage.prompt_tokens,
                    completion_tokens=response.usage.completion_tokens,
                    latency_ms=0,
                    cost=self._estimate_cost(
                        response.usage.prompt_tokens, response.usage.completion_tokens
                    ),
                )

            # ê°€ë“œë ˆì¼ ì ìš©
            insight = self.templates.apply_guardrails(insight)

            # ì‹¤íŒ¨í•œ ì‹ í˜¸ ìˆ˜ì§‘ê¸° ê²½ê³  ì¶”ê°€
            if failed_signals:
                warning_section = "\n\n---\n"
                warning_section += f"âš ï¸ **ì™¸ë¶€ íŠ¸ë Œë“œ ì •ë³´ ì¼ë¶€ ë¯¸ë°˜ì˜**\n"
                warning_section += f"- **ìˆ˜ì§‘ ì‹¤íŒ¨**: {', '.join(failed_signals)}\n"
                warning_section += (
                    "- **ì˜í–¥**: ë³¸ ë¦¬í¬íŠ¸ëŠ” í¬ë¡¤ë§/KG ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ ì‘ì„±ë¨\n"
                )
                warning_section += (
                    "- **ê¶Œì¥**: 1-2ì‹œê°„ í›„ ì¬ì‹œë„ ë˜ëŠ” ìˆ˜ë™ìœ¼ë¡œ íŠ¸ë Œë“œ í™•ì¸"
                )
                insight += warning_section

            return insight

        except Exception as e:
            self.logger.error(f"LLM call failed: {e}")
            return self._generate_fallback_insight(hybrid_context, metrics_data)

    def _generate_fallback_insight(
        self, hybrid_context: HybridContext, metrics_data: Dict
    ) -> str:
        """í´ë°± ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        summary = metrics_data.get("summary", {})
        inferences = hybrid_context.inferences

        insight_parts = [
            f"## ì˜¤ëŠ˜ì˜ LANEIGE Amazon ë² ìŠ¤íŠ¸ì…€ëŸ¬ ë¶„ì„\n",
            f"- ì¶”ì  ì¤‘ì¸ ì œí’ˆ: {summary.get('laneige_products_tracked', 0)}ê°œ",
            f"- ì•Œë¦¼: {summary.get('alert_count', 0)}ê±´",
        ]

        # ì¶”ë¡  ê²°ê³¼ ì¶”ê°€
        if inferences:
            insight_parts.append("\n### ì£¼ìš” ë¶„ì„ ê²°ê³¼")
            for inf in inferences[:3]:
                insight_parts.append(f"- {inf.insight}")

        insight_parts.append(
            "\n\n_â€» ìƒì„¸ ì¸ì‚¬ì´íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ ê¸°ë³¸ ìš”ì•½ì„ ì œê³µí•©ë‹ˆë‹¤._"
        )

        reference_section = self._build_reference_section(hybrid_context, {}, None)
        if reference_section:
            insight_parts.append("\n" + reference_section)

        return "\n".join(insight_parts)

    def _extract_action_items(
        self, inferences: List[InferenceResult], metrics_data: Dict
    ) -> List[Dict]:
        """ì•¡ì…˜ ì•„ì´í…œ ì¶”ì¶œ"""
        actions = []

        # ì¶”ë¡  ê²°ê³¼ì—ì„œ ì•¡ì…˜ ì¶”ì¶œ
        for inf in inferences:
            if inf.recommendation:
                priority = self._get_priority_from_insight(inf)
                actions.append(
                    {
                        "priority": priority,
                        "type": inf.insight_type.value,
                        "action": inf.recommendation,
                        "source": "ontology_inference",
                        "rule": inf.rule_name,
                        "confidence": inf.confidence,
                    }
                )

        # ì•Œë¦¼ ê¸°ë°˜ ì•¡ì…˜ ì¶”ê°€
        for alert in metrics_data.get("alerts", []):
            if alert.get("severity") == "critical":
                actions.append(
                    {
                        "priority": "high",
                        "type": alert.get("type"),
                        "action": f"[ê¸´ê¸‰] {alert.get('message')} - ì¦‰ì‹œ í™•ì¸ í•„ìš”",
                        "source": "alert",
                        "product": alert.get("title"),
                        "asin": alert.get("asin"),
                    }
                )
            elif alert.get("severity") == "warning":
                actions.append(
                    {
                        "priority": "medium",
                        "type": alert.get("type"),
                        "action": f"[ì£¼ì˜] {alert.get('message')} - ëª¨ë‹ˆí„°ë§ ê°•í™”",
                        "source": "alert",
                        "product": alert.get("title"),
                        "asin": alert.get("asin"),
                    }
                )

        # ìš°ì„ ìˆœìœ„ ì •ë ¬
        priority_order = {"high": 0, "medium": 1, "low": 2}
        actions.sort(key=lambda x: priority_order.get(x.get("priority"), 3))

        return actions[:10]

    def _build_reference_section(
        self,
        hybrid_context: HybridContext,
        external_signals: Optional[Dict[str, Any]],
        market_intelligence: Optional[Dict[str, Any]] = None,
    ) -> str:
        """
        ì°¸ê³ ìë£Œ ì„¹ì…˜ ìƒì„± (ìˆ«ì ì¸ìš©ìš©)

        ì¶œì²˜ ìš°ì„ ìˆœìœ„:
        1. Market Intelligence (Layer 4 â†’ Layer 3 â†’ Layer 2)
        2. External Signals
        3. RAG Documents
        4. Knowledge Graph
        """
        entries = []
        idx = 1

        # 1. Market Intelligence ì¶œì²˜ (Layer 4: ê±°ì‹œê²½ì œ/ë¬´ì—­)
        if market_intelligence:
            sources = market_intelligence.get("sources", [])
            for source in sources[:6]:  # ìµœëŒ€ 6ê°œ
                publisher = source.get("publisher", "")
                title = source.get("title", "")
                date = source.get("date", "")
                source_type = source.get("source_type", "")
                reliability = source.get("reliability_score", 0)

                # ì¶œì²˜ ìœ í˜•ë³„ í¬ë§·
                if source_type == "government":
                    entries.append(f"[{idx}] {publisher}, {title}, {date}")
                elif source_type == "ir":
                    url = source.get("url", "")
                    entries.append(f'[{idx}] {publisher}, "{title}", {date}')
                elif source_type == "news":
                    entries.append(f'[{idx}] {publisher}, "{title}", {date}')
                else:
                    entries.append(f"[{idx}] {publisher}: {title} ({date})")
                idx += 1

        # 2. External Signal ì¶œì²˜ (Reddit, TikTok ë“±)
        for signal in (external_signals or {}).get("signals", [])[:3]:
            source = signal.get("source", "").replace("_", " ").title()
            title = signal.get("title", "")[:50]
            collected_at = signal.get("collected_at", "") or signal.get(
                "published_at", ""
            )
            if collected_at:
                collected_at = collected_at[:10]  # YYYY-MM-DDë§Œ
            entries.append(f'[{idx}] {source}, "{title}...", {collected_at}')
            idx += 1

        # 3. RAG ë¬¸ì„œ ì¶œì²˜
        for chunk in (hybrid_context.rag_chunks or [])[:2]:
            metadata = chunk.get("metadata", {})
            title = metadata.get("title") or metadata.get("doc_id", "ê°€ì´ë“œ ë¬¸ì„œ")
            source_filename = metadata.get("source_filename", "")
            if source_filename:
                entries.append(f"[{idx}] ë‚´ë¶€ ê°€ì´ë“œ: {title} ({source_filename})")
            else:
                entries.append(f"[{idx}] ë‚´ë¶€ ê°€ì´ë“œ: {title}")
            idx += 1

        # 4. KG ê·¼ê±° (ìš”ì•½)
        if hybrid_context.ontology_facts:
            fact_types = sorted(
                {
                    fact.get("type")
                    for fact in hybrid_context.ontology_facts
                    if fact.get("type")
                }
            )
            if fact_types:
                entries.append(
                    f"[{idx}] KnowledgeGraph: ì˜¨í†¨ë¡œì§€ ì¶”ë¡  ({', '.join(fact_types)})"
                )

        if not entries:
            return ""

        return "## ì°¸ê³ ìë£Œ\n" + "\n".join(entries)

    def _get_priority_from_insight(self, inference: InferenceResult) -> str:
        """ì¸ì‚¬ì´íŠ¸ ìœ í˜•ì—ì„œ ìš°ì„ ìˆœìœ„ ê²°ì •"""
        high_priority = {
            InsightType.RISK_ALERT,
            InsightType.COMPETITIVE_THREAT,
            InsightType.RANK_SHOCK,
        }
        medium_priority = {
            InsightType.PRICE_QUALITY_GAP,
            InsightType.COMPETITIVE_ADVANTAGE,
            InsightType.GROWTH_OPPORTUNITY,
        }

        if inference.insight_type in high_priority:
            return "high"
        elif inference.insight_type in medium_priority:
            return "medium"
        else:
            return "low"

    def _extract_highlights(
        self, inferences: List[InferenceResult], metrics_data: Dict
    ) -> List[Dict]:
        """í•˜ì´ë¼ì´íŠ¸ ì¶”ì¶œ"""
        highlights = []

        # ê¸ì •ì  ì¶”ë¡  ê²°ê³¼
        positive_types = {
            InsightType.MARKET_DOMINANCE,
            InsightType.GROWTH_MOMENTUM,
            InsightType.STABILITY,
            InsightType.COMPETITIVE_ADVANTAGE,
        }

        for inf in inferences:
            if inf.insight_type in positive_types:
                highlights.append(
                    {
                        "type": inf.insight_type.value,
                        "title": inf.insight_type.value.replace("_", " ").title(),
                        "detail": inf.insight,
                        "source": "ontology",
                    }
                )

        # ì œí’ˆ ë©”íŠ¸ë¦­ì—ì„œ í•˜ì´ë¼ì´íŠ¸
        product_metrics = metrics_data.get("product_metrics", [])

        # Top 10 ì§„ì…
        for p in product_metrics:
            if p.get("current_rank", 100) <= 10:
                highlights.append(
                    {
                        "type": "top_rank",
                        "title": f"Top 10: {p.get('product_title', '')[:30]}...",
                        "detail": f"{p.get('category_id')} ì¹´í…Œê³ ë¦¬ {p.get('current_rank')}ìœ„",
                        "asin": p.get("asin"),
                    }
                )

        # ìˆœìœ„ ìƒìŠ¹
        improving = [
            p
            for p in product_metrics
            if p.get("rank_change_1d") and p.get("rank_change_1d") < -3
        ]
        for p in improving[:3]:
            highlights.append(
                {
                    "type": "rank_up",
                    "title": f"ìˆœìœ„ ìƒìŠ¹: {p.get('product_title', '')[:30]}...",
                    "detail": f"{abs(p.get('rank_change_1d'))}ë‹¨ê³„ ìƒìŠ¹ â†’ í˜„ì¬ {p.get('current_rank')}ìœ„",
                    "asin": p.get("asin"),
                }
            )

        return highlights[:10]

    def _estimate_cost(self, prompt_tokens: int, completion_tokens: int) -> float:
        """ë¹„ìš© ì¶”ì •"""
        input_cost = (prompt_tokens / 1_000_000) * 0.40
        output_cost = (completion_tokens / 1_000_000) * 1.60
        return round(input_cost + output_cost, 6)

    def get_results(self) -> Dict[str, Any]:
        """ë§ˆì§€ë§‰ ì‹¤í–‰ ê²°ê³¼"""
        return self._results

    def get_last_hybrid_context(self) -> Optional[HybridContext]:
        """ë§ˆì§€ë§‰ í•˜ì´ë¸Œë¦¬ë“œ ì»¨í…ìŠ¤íŠ¸"""
        return self._last_hybrid_context

    def get_knowledge_graph(self) -> KnowledgeGraph:
        """ì§€ì‹ ê·¸ë˜í”„ ë°˜í™˜"""
        return self.kg

    def get_reasoner(self) -> OntologyReasoner:
        """ì¶”ë¡ ê¸° ë°˜í™˜"""
        return self.reasoner

    async def _collect_external_signals(self) -> Dict[str, Any]:
        """
        External Signal ìˆ˜ì§‘

        Returns:
            {
                "signals": [...],
                "report_section": "â–  ì „ë¬¸ ë§¤ì²´ ê·¼ê±°: ...",
                "stats": {"by_tier": {...}, "by_source": {...}}
            }
        """
        result = {"signals": [], "report_section": "", "stats": {}}

        try:
            if not self._signal_collector:
                self._signal_collector = ExternalSignalCollector()
                await self._signal_collector.initialize()

            # ê¸°ì¡´ ìˆ˜ì§‘ëœ ì‹ í˜¸ í™•ì¸
            if self._signal_collector.signals:
                result["signals"] = [
                    s.to_dict() for s in self._signal_collector.signals[-20:]
                ]
                result["report_section"] = (
                    self._signal_collector.generate_report_section(days=7)
                )
                result["stats"] = self._signal_collector.get_stats()

            self.logger.debug(
                f"External signals: {len(result['signals'])} signals loaded"
            )

        except Exception as e:
            self.logger.warning(f"External signal collection failed: {e}")

        return result

    def _get_failed_signal_collectors(self) -> List[str]:
        """
        ì‚¬ìš© ë¶ˆê°€ëŠ¥í•œ ì™¸ë¶€ ì‹ í˜¸ ìˆ˜ì§‘ê¸° ëª©ë¡ ë°˜í™˜

        Returns:
            ì‹¤íŒ¨í•œ ìˆ˜ì§‘ê¸° ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        """
        failed = []

        if not GOOGLE_TRENDS_AVAILABLE:
            failed.append("Google Trends")

        if not YOUTUBE_AVAILABLE:
            failed.append("YouTube")

        # ExternalSignalCollector ì²´í¬
        try:
            from src.tools.external_signal_collector import ExternalSignalCollector
        except ImportError:
            failed.append("External Signals (Tavily/RSS/Reddit)")

        # Market Intelligence ì²´í¬
        try:
            from src.tools.market_intelligence import MarketIntelligenceEngine
        except ImportError:
            failed.append("Market Intelligence")

        return failed

    async def _collect_market_intelligence(self) -> Dict[str, Any]:
        """
        Market Intelligence ë°ì´í„° ìˆ˜ì§‘ (Layer 2-4)

        Returns:
            {
                "layer_4": {...},  # ê±°ì‹œê²½ì œ/ë¬´ì—­
                "layer_3": {...},  # ì‚°ì—…/ê¸°ì—…
                "layer_2": {...},  # ì†Œë¹„ì íŠ¸ë Œë“œ
                "sources": [...],  # ì¶œì²˜ ëª©ë¡
                "insight_section": "..."  # ìƒì„±ëœ ì¸ì‚¬ì´íŠ¸ ì„¹ì…˜
            }
        """
        result = {
            "layer_4": {},
            "layer_3": {},
            "layer_2": {},
            "sources": [],
            "insight_section": "",
        }

        try:
            if not self._market_intelligence:
                self._market_intelligence = MarketIntelligenceEngine()
                await self._market_intelligence.initialize()

            # ëª¨ë“  ë ˆì´ì–´ ë³‘ë ¬ ìˆ˜ì§‘
            await self._market_intelligence.collect_all_layers()

            # ë ˆì´ì–´ë³„ ë°ì´í„° ì¶”ì¶œ
            layer_data = self._market_intelligence.layer_data

            if DataLayer.LAYER_4_MACRO in layer_data:
                result["layer_4"] = layer_data[DataLayer.LAYER_4_MACRO].data
                result["sources"].extend(layer_data[DataLayer.LAYER_4_MACRO].sources)

            if DataLayer.LAYER_3_INDUSTRY in layer_data:
                result["layer_3"] = layer_data[DataLayer.LAYER_3_INDUSTRY].data
                result["sources"].extend(layer_data[DataLayer.LAYER_3_INDUSTRY].sources)

            if DataLayer.LAYER_2_CONSUMER in layer_data:
                result["layer_2"] = layer_data[DataLayer.LAYER_2_CONSUMER].data
                result["sources"].extend(layer_data[DataLayer.LAYER_2_CONSUMER].sources)

            # ì¸ì‚¬ì´íŠ¸ ì„¹ì…˜ ìƒì„±
            result["insight_section"] = (
                self._market_intelligence.generate_layered_insight()
            )

            # Google Trends ìˆ˜ì§‘ (Phase 1)
            google_trends = await self._collect_google_trends()
            if google_trends.get("trends"):
                result["google_trends"] = google_trends["trends"]
                if google_trends.get("insight_section"):
                    result["insight_section"] += (
                        "\n\n" + google_trends["insight_section"]
                    )

            # YouTube ë¦¬ë·° ìˆ˜ì§‘ (Phase 2)
            youtube_reviews = await self._collect_youtube_reviews()
            if youtube_reviews.get("videos"):
                result["youtube_reviews"] = youtube_reviews["videos"]
                result["youtube_signals"] = youtube_reviews["signals"]
                if youtube_reviews.get("insight_section"):
                    result["insight_section"] += (
                        "\n\n" + youtube_reviews["insight_section"]
                    )

            self.logger.info(
                f"Market Intelligence collected: {len(result['sources'])} sources"
            )

        except Exception as e:
            self.logger.warning(f"Market Intelligence collection failed: {e}")

        return result

    async def _collect_google_trends(self) -> Dict[str, Any]:
        """
        Google Trends ë°ì´í„° ìˆ˜ì§‘

        Returns:
            {
                "trends": [...],
                "insight_section": "### Google Trends ê²€ìƒ‰ ê´€ì‹¬ë„\n...",
                "collected_at": str
            }
        """
        result = {"trends": [], "insight_section": "", "collected_at": ""}

        if not GOOGLE_TRENDS_AVAILABLE:
            self.logger.debug("Google Trends collector not available")
            return result

        try:
            if not self._google_trends:
                self._google_trends = GoogleTrendsCollector(
                    geo="US", timeframe="today 3-m"
                )

            # ë·°í‹° íŠ¸ë Œë“œ ìˆ˜ì§‘
            trends = await self._google_trends.fetch_beauty_trends()

            if trends:
                result["trends"] = [t.to_dict() for t in trends]
                result["insight_section"] = (
                    self._google_trends.generate_insight_section(trends)
                )
                result["collected_at"] = trends[0].collected_at if trends else ""

                # ë°ì´í„° ì €ì¥
                await self._google_trends.save_trends(trends)

            self.logger.info(f"Google Trends collected: {len(trends)} keywords")

        except Exception as e:
            self.logger.warning(f"Google Trends collection failed: {e}")

        return result

    async def _collect_youtube_reviews(self) -> Dict[str, Any]:
        """
        YouTube ë¦¬ë·° ë°ì´í„° ìˆ˜ì§‘

        Returns:
            {
                "videos": [...],
                "signals": [...],  # ExternalSignal í˜•ì‹
                "insight_section": "### YouTube ë¦¬ë·° íŠ¸ë Œë“œ\n...",
                "collected_at": str
            }
        """
        result = {
            "videos": [],
            "signals": [],
            "insight_section": "",
            "collected_at": "",
        }

        if not YOUTUBE_AVAILABLE:
            self.logger.debug("YouTube collector not available")
            return result

        try:
            if not self._youtube_collector:
                self._youtube_collector = YouTubeCollector()

            # LANEIGE ë¦¬ë·° ìˆ˜ì§‘
            videos = await self._youtube_collector.fetch_laneige_reviews(max_results=30)

            if videos:
                result["videos"] = [v.to_dict() for v in videos]
                result["signals"] = self._youtube_collector.to_external_signals(videos)
                result["insight_section"] = (
                    self._youtube_collector.generate_insight_section(videos)
                )
                result["collected_at"] = videos[0].collected_at if videos else ""

                # ë°ì´í„° ì €ì¥
                await self._youtube_collector.save_videos(videos)

            self.logger.info(f"YouTube reviews collected: {len(videos)} videos")

        except Exception as e:
            self.logger.warning(f"YouTube collection failed: {e}")

        return result

    def _extract_data_source_info(
        self, metrics_data: Optional[Dict], crawl_data: Optional[Dict]
    ) -> Dict[str, Any]:
        """
        ë°ì´í„° ì¶œì²˜ ì •ë³´ ì¶”ì¶œ

        Args:
            metrics_data: ì§€í‘œ ë°ì´í„°
            crawl_data: í¬ë¡¤ë§ ë°ì´í„°

        Returns:
            ë°ì´í„° ì¶œì²˜ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        source_info = {
            "platform": "Amazon US Best Sellers",
            "collected_at": None,
            "snapshot_date": None,
            "categories": [],
            "total_products": 0,
            "disclaimer": "Amazonì€ Best Sellers ìˆœìœ„ë¥¼ ë§¤ ì‹œê°„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. í‘œì‹œëœ ë°ì´í„°ëŠ” ìˆ˜ì§‘ ì‹œì ì˜ ìŠ¤ëƒ…ìƒ·ì…ë‹ˆë‹¤.",
        }

        # í¬ë¡¤ë§ ë°ì´í„°ì—ì„œ ìˆ˜ì§‘ ì‹œì  ì¶”ì¶œ
        if crawl_data:
            collected_at = crawl_data.get("collected_at")
            if collected_at:
                source_info["collected_at"] = collected_at

            # í¬ë¡¤ë§ ìš”ì•½ì—ì„œ ì •ë³´ ì¶”ì¶œ
            if "summary" in crawl_data:
                summary = crawl_data["summary"]
                source_info["total_products"] = summary.get("total_products", 0)
                source_info["categories"] = summary.get("categories", [])

        # ì§€í‘œ ë°ì´í„°ì—ì„œ ë‚ ì§œ ì •ë³´ ì¶”ì¶œ
        if metrics_data:
            metadata = metrics_data.get("metadata", {})
            if metadata:
                data_date = metadata.get("data_date")
                if data_date:
                    source_info["snapshot_date"] = data_date
                if not source_info["collected_at"]:
                    source_info["collected_at"] = metadata.get("generated_at")

            # ì¹´í…Œê³ ë¦¬ ì •ë³´
            categories = metrics_data.get("categories", {})
            if categories and not source_info["categories"]:
                source_info["categories"] = list(categories.keys())

            # ì œí’ˆ ìˆ˜
            if not source_info["total_products"]:
                total = (
                    sum(
                        len(cat_data.get("rank_records", []))
                        for cat_data in categories.values()
                    )
                    if categories
                    else 0
                )
                source_info["total_products"] = total

        return source_info

    def _ingest_rag_knowledge(self, rag_chunks: List[Dict[str, Any]]) -> Dict[str, int]:
        """RAG ì²­í¬ì—ì„œ ì§€ì‹ ì¶”ì¶œ í›„ KGì— ì ì¬"""
        stats = {"trend_relations": 0, "action_relations": 0}

        for chunk in rag_chunks or []:
            metadata = chunk.get("metadata", {})
            doc_type = metadata.get("doc_type", "")
            doc_id = metadata.get("doc_id", "")
            chunk_id = metadata.get("chunk_id") or chunk.get("id")
            target_brand = metadata.get("target_brand")
            brands_covered = metadata.get("brands_covered", [])

            subject = (
                self._normalize_brand_name(target_brand) if target_brand else "MARKET"
            )
            if not target_brand and brands_covered:
                subject = self._normalize_brand_name(brands_covered[0])

            # íŠ¸ë Œë“œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì¸í…”ë¦¬ì „ìŠ¤ ë¬¸ì„œ ìš°ì„ )
            if doc_type in {"intelligence", "knowledge_base"}:
                trend_keywords = metadata.get("keywords", [])
                for keyword in trend_keywords:
                    if len(keyword) < 3:
                        continue
                    relation = Relation(
                        subject=subject,
                        predicate=RelationType.HAS_TREND,
                        object=keyword,
                        properties={
                            "source": "rag",
                            "doc_id": doc_id,
                            "chunk_id": chunk_id,
                            "doc_type": doc_type,
                            "source_filename": metadata.get("source_filename", ""),
                        },
                    )
                    if self.kg.add_relation(relation):
                        stats["trend_relations"] += 1

            # ì•¡ì…˜ ì•„ì´í…œ ì¶”ì¶œ (í”Œë ˆì´ë¶/ëŒ€ì‘ ê°€ì´ë“œ)
            if doc_type in {"playbook", "response_guide"}:
                action_lines = self._extract_action_lines(chunk.get("content", ""))
                for action in action_lines:
                    relation = Relation(
                        subject=subject,
                        predicate=RelationType.REQUIRES_ACTION,
                        object=action,
                        properties={
                            "source": "rag",
                            "doc_id": doc_id,
                            "chunk_id": chunk_id,
                            "doc_type": doc_type,
                            "source_filename": metadata.get("source_filename", ""),
                        },
                    )
                    if self.kg.add_relation(relation):
                        stats["action_relations"] += 1

        return stats

    def _ingest_external_signals(
        self, external_signals: Dict[str, Any]
    ) -> Dict[str, int]:
        """External Signalì„ KGì— ì ì¬"""
        stats = {"trend_relations": 0}
        signals = external_signals.get("signals", []) if external_signals else []

        for signal in signals:
            keywords = signal.get("keywords", [])
            if not keywords:
                continue

            subject = self._infer_signal_subject(keywords)
            for keyword in keywords:
                relation = Relation(
                    subject=subject,
                    predicate=RelationType.HAS_TREND,
                    object=keyword,
                    properties={
                        "source": "external_signal",
                        "signal_id": signal.get("signal_id"),
                        "source_name": signal.get("source"),
                        "url": signal.get("url"),
                        "published_at": signal.get("published_at"),
                        "collected_at": signal.get("collected_at"),
                    },
                )
                if self.kg.add_relation(relation):
                    stats["trend_relations"] += 1

        return stats

    def _extract_action_lines(self, content: str) -> List[str]:
        """ë¬¸ì„œ ë³¸ë¬¸ì—ì„œ ì•¡ì…˜ í•­ëª© ì¶”ì¶œ"""
        actions = []
        for line in content.splitlines():
            stripped = line.strip()
            if not stripped:
                continue
            if stripped.startswith("- ") or stripped.startswith("* "):
                actions.append(stripped[2:].strip())
            elif stripped[:2].isdigit() and stripped[1:3] == ". ":
                actions.append(stripped[3:].strip())
        return [a for a in actions if 5 <= len(a) <= 140]

    def _infer_signal_subject(self, keywords: List[str]) -> str:
        """External Signal í‚¤ì›Œë“œì—ì„œ ëŒ€ìƒ ì—”í‹°í‹° ì¶”ì •"""
        brand_keywords = {
            "laneige": "LANEIGE",
            "cosrx": "COSRX",
            "tirtir": "TIRTIR",
            "rare beauty": "RARE BEAUTY",
            "innisfree": "INNISFREE",
            "etude": "ETUDE",
            "sulwhasoo": "SULWHASOO",
            "hera": "HERA",
        }
        for keyword in keywords:
            normalized = keyword.lower()
            if normalized in brand_keywords:
                return brand_keywords[normalized]
        return "MARKET"

    def _normalize_brand_name(self, brand: Optional[str]) -> str:
        if not brand:
            return "MARKET"
        return brand.upper() if brand.isalpha() else brand
