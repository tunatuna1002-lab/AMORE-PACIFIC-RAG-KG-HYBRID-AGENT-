"""
Source Provider
ì¶œì²˜ ì¶”ì¶œ ë° í¬ë§·íŒ… ëª¨ë“ˆ

Perplexity/Liner ìŠ¤íƒ€ì¼ ìƒì„¸ ì¶œì²˜ ì œê³µ:
- 7ê°œ ì¶œì²˜ ìœ í˜• ì§€ì›
- ASIN ê¸°ë°˜ ì œí’ˆ ì¶”ì 
- ë§ˆí¬ë‹¤ìš´ í¬ë§·íŒ…
"""

from datetime import datetime
from typing import Any

from src.rag.hybrid_retriever import HybridContext


class SourceProvider:
    """Extracts and formats source citations for chatbot responses.

    Supports 7 source types and Perplexity-style markdown formatting.
    """

    def __init__(self, config: dict[str, Any] | None = None, knowledge_graph=None):
        """
        Args:
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
            knowledge_graph: KnowledgeGraph ì¸ìŠ¤í„´ìŠ¤ (optional, for category hierarchy)
        """
        self.config = config or {}
        self.knowledge_graph = knowledge_graph

    def extract_sources(
        self,
        hybrid_context: HybridContext,
        current_data: dict[str, Any] | None = None,
        external_signals: list[Any] | None = None,
        model: str = "gpt-4.1-mini",
    ) -> list[dict[str, Any]]:
        """Extract sources from RAG context and response.

        Args:
            hybrid_context: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸
            current_data: í˜„ìž¬ ë°ì´í„° ì»¨í…ìŠ¤íŠ¸
            external_signals: ì™¸ë¶€ ì‹ í˜¸ ë¦¬ìŠ¤íŠ¸ (Tavily ë‰´ìŠ¤, RSS, Reddit ë“±)
            model: LLM ëª¨ë¸ëª…

        Returns:
            ì¶œì²˜ ì •ë³´ ë¦¬ìŠ¤íŠ¸ (ìœ í˜•ë³„ ìƒì„¸ ì •ë³´ í¬í•¨)
        """
        sources = []

        # 1. í¬ë¡¤ë§ ë°ì´í„° ì¶œì²˜ - URL ë° ìƒì„¸ ì •ë³´ ì¶”ê°€ (ASIN í¬í•¨)
        if current_data:
            metadata = current_data.get("metadata", {})
            data_date = metadata.get("data_date", "")
            categories = current_data.get("categories", {})

            total_products = (
                sum(len(cat_data.get("rank_records", [])) for cat_data in categories.values())
                if categories
                else 0
            )

            # ì§ˆì˜ì—ì„œ ì–¸ê¸‰ëœ ì œí’ˆì˜ ASIN ì¶”ì¶œ
            mentioned_asins = self._extract_mentioned_asins(hybrid_context, categories)

            crawled_source = {
                "type": "crawled_data",
                "icon": "ðŸ“Š",
                "description": "Amazon Best Sellers í¬ë¡¤ë§ ë°ì´í„°",
                "collected_at": data_date,
                "url": "https://www.amazon.com/gp/bestsellers/beauty",
                "details": {
                    "categories": list(categories.keys()) if categories else [],
                    "total_products": total_products,
                    "snapshot_date": data_date,
                },
            }

            # ê´€ë ¨ ì œí’ˆì˜ ASIN ì •ë³´ ì¶”ê°€
            if mentioned_asins:
                crawled_source["mentioned_products"] = mentioned_asins

            sources.append(crawled_source)

        # 2. Knowledge Graph ì¶œì²˜ - ì—”í‹°í‹° ë° ê´€ê³„ ì •ë³´ ì¶”ê°€
        if hybrid_context.ontology_facts:
            sources.append(
                {
                    "type": "knowledge_graph",
                    "icon": "ðŸ”—",
                    "description": "ì§€ì‹ ê·¸ëž˜í”„ ê´€ê³„ ë°ì´í„°",
                    "fact_count": len(hybrid_context.ontology_facts),
                    "entities": self._extract_entity_names(hybrid_context.ontology_facts),
                    "relations": self._extract_relation_types(hybrid_context.ontology_facts),
                    "details": {
                        "source": "Amazon US ì‹¤ì‹œê°„ ë°ì´í„° ê¸°ë°˜ ì§€ì‹ ê·¸ëž˜í”„",
                        "fact_count": len(hybrid_context.ontology_facts),
                    },
                }
            )

        # 3. ì˜¨í†¨ë¡œì§€ ì¶”ë¡  ì¶œì²˜ - ê·œì¹™ ìƒì„¸ ì •ë³´
        if hybrid_context.inferences:
            for inf in hybrid_context.inferences:
                sources.append(
                    {
                        "type": "ontology_inference",
                        "icon": "ðŸ§ ",
                        "description": f"ì˜¨í†¨ë¡œì§€ ê·œì¹™: {inf.rule_name}",
                        "rule_name": inf.rule_name,
                        "confidence": inf.confidence,
                        "evidence": inf.evidence,
                        "insight_type": inf.insight_type.value
                        if hasattr(inf.insight_type, "value")
                        else str(inf.insight_type),
                        "details": {"insight": inf.insight, "recommendation": inf.recommendation},
                    }
                )

        # 4. RAG ë¬¸ì„œ ì¶œì²˜ - íŒŒì¼ ê²½ë¡œ ë° ê´€ë ¨ì„± ì ìˆ˜
        rag_sources_map = {}
        for chunk in hybrid_context.rag_chunks:
            metadata = chunk.get("metadata", {})
            doc_id = metadata.get("doc_id", "")
            title = metadata.get("title", "")
            file_path = metadata.get("file_path", "")
            score = chunk.get("score", 0)
            section = metadata.get("section", "")

            if doc_id or title:
                doc_key = doc_id or title
                # ê°™ì€ ë¬¸ì„œì˜ ì—¬ëŸ¬ ì²­í¬ ì¤‘ ê°€ìž¥ ë†’ì€ ì ìˆ˜ë§Œ ìœ ì§€
                if doc_key not in rag_sources_map or score > rag_sources_map[doc_key].get(
                    "relevance_score", 0
                ):
                    rag_sources_map[doc_key] = {
                        "type": "rag_document",
                        "icon": "ðŸ“„",
                        "description": title or doc_id,
                        "file_path": file_path,
                        "section": section,
                        "relevance_score": score,
                        "details": {"doc_id": doc_id, "title": title},
                    }

        sources.extend(rag_sources_map.values())

        # 5. ì¹´í…Œê³ ë¦¬ ê³„ì¸µ ì¶œì²˜
        if (
            hybrid_context.entities
            and hybrid_context.entities.get("categories")
            and self.knowledge_graph
        ):
            for category in hybrid_context.entities["categories"][:3]:  # ìµœëŒ€ 3ê°œ
                hierarchy = self.knowledge_graph.get_category_hierarchy(category)
                if "error" not in hierarchy:
                    path = []
                    if hierarchy.get("ancestors"):
                        path = [a["name"] for a in reversed(hierarchy["ancestors"])]
                    path.append(hierarchy.get("name", category))

                    sources.append(
                        {
                            "type": "category_hierarchy",
                            "icon": "ðŸ—‚ï¸",
                            "description": "ì¹´í…Œê³ ë¦¬ ê³„ì¸µ êµ¬ì¡°",
                            "path": path,
                            "level": hierarchy.get("level", 0),
                            "url": hierarchy.get("url", ""),
                            "details": {"category": category, "hierarchy_depth": len(path)},
                        }
                    )

        # 6. ì™¸ë¶€ ì‹ í˜¸ ì¶œì²˜ (Tavily ë‰´ìŠ¤, RSS, Reddit ë“±)
        if external_signals:
            for signal in external_signals[:5]:  # ìƒìœ„ 5ê°œë§Œ
                signal_source = getattr(signal, "source", "unknown")
                reliability = 0.7  # ê¸°ë³¸ê°’

                # ë©”íƒ€ë°ì´í„°ì—ì„œ ì‹ ë¢°ë„ ì¶”ì¶œ
                if hasattr(signal, "metadata") and signal.metadata:
                    reliability = signal.metadata.get("reliability_score", 0.7)

                # ì†ŒìŠ¤ ìœ í˜•ì— ë”°ë¼ ì•„ì´ì½˜ ê²°ì •
                if "tavily" in signal_source.lower() or "news" in signal_source.lower():
                    icon = "ðŸ“°"
                    source_type = "external_news"
                elif "reddit" in signal_source.lower():
                    icon = "ðŸ’¬"
                    source_type = "social_media"
                elif "rss" in signal_source.lower():
                    icon = "ðŸ“¡"
                    source_type = "rss_feed"
                elif "youtube" in signal_source.lower():
                    icon = "ðŸ“º"
                    source_type = "social_media"
                else:
                    icon = "ðŸŒ"
                    source_type = "external_source"

                sources.append(
                    {
                        "type": source_type,
                        "icon": icon,
                        "description": getattr(signal, "title", "Unknown"),
                        "source": signal_source,
                        "url": getattr(signal, "url", ""),
                        "published_at": getattr(signal, "published_at", ""),
                        "reliability_score": reliability,
                        "relevance_score": getattr(signal, "relevance_score", 0.5),
                        "details": {
                            "content_preview": getattr(signal, "content", "")[:200]
                            if hasattr(signal, "content")
                            else "",
                            "tier": getattr(signal, "tier", "unknown"),
                        },
                    }
                )

        # 7. AI ëª¨ë¸ ì¶œì²˜ (í•­ìƒ í¬í•¨)
        sources.append(
            {
                "type": "ai_model",
                "icon": "ðŸ¤–",
                "description": f"AI ë¶„ì„: {model}",
                "model": model,
                "disclaimer": "AIê°€ ìƒì„±í•œ ë¶„ì„ìž…ë‹ˆë‹¤. ì¤‘ìš”í•œ ì˜ì‚¬ê²°ì • ì‹œ ì¶”ê°€ ê²€ì¦ì„ ê¶Œìž¥í•©ë‹ˆë‹¤.",
                "generated_at": datetime.now().isoformat(),
            }
        )

        return sources

    def format_sources_for_display(self, sources: list[dict[str, Any]]) -> str:
        """Format sources as Perplexity-style markdown with numbered citations.

        Args:
            sources: ì¶œì²˜ ì •ë³´ ë¦¬ìŠ¤íŠ¸

        Returns:
            ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ì¶œì²˜ ì„¹ì…˜
        """
        if not sources:
            return ""

        lines = ["\n\n---"]

        # ë°ì´í„° ì¶œì²˜ ì‹œì ì„ ëª…í™•ížˆ í‘œì‹œ
        crawled_source = next((s for s in sources if s["type"] == "crawled_data"), None)
        if crawled_source:
            collected_at = crawled_source.get("collected_at", "")
            if collected_at:
                lines.append(f"ðŸ“… **ë°ì´í„° ê¸°ì¤€: Amazon US Best Sellers {collected_at} ìˆ˜ì§‘**")
                lines.append("*(Amazonì€ Best Sellers ìˆœìœ„ë¥¼ ë§¤ ì‹œê°„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤)*")
                lines.append("")

        lines.extend(["**ðŸ“š ì¶œì²˜ ë° ì°¸ê³ ìžë£Œ:**", ""])

        for i, source in enumerate(sources, 1):
            icon = source.get("icon", "â€¢")
            desc = source.get("description", "ì•Œ ìˆ˜ ì—†ëŠ” ì¶œì²˜")

            if source["type"] == "crawled_data":
                collected = source.get("collected_at", "")
                url = source.get("url", "")
                details = source.get("details", {})
                total = details.get("total_products", 0)
                mentioned_products = source.get("mentioned_products", [])

                lines.append(f"{i}. {icon} **{desc}**")
                lines.append(f"   - ìˆ˜ì§‘ì¼: {collected}")
                if url:
                    lines.append(f"   - URL: {url}")
                if total > 0:
                    lines.append(f"   - ì´ ì œí’ˆ ìˆ˜: {total}ê°œ")

                # ASIN ê¸°ë°˜ ì œí’ˆ ì¶”ì  ì •ë³´ í‘œì‹œ
                if mentioned_products:
                    lines.append("   - ðŸ“¦ ê´€ë ¨ ì œí’ˆ (ASIN ê¸°ì¤€):")
                    for prod in mentioned_products[:3]:  # ìµœëŒ€ 3ê°œ í‘œì‹œ
                        asin = prod.get("asin", "")
                        name = prod.get("name", "")
                        rank = prod.get("rank", "")
                        category = prod.get("category", "")
                        lines.append(f"     â€¢ [{asin}] {name} (#{rank} in {category})")

                lines.append("")

            elif source["type"] == "knowledge_graph":
                fact_count = source.get("fact_count", 0)
                entities = source.get("entities", [])
                relations = source.get("relations", [])
                lines.append(f"{i}. {icon} **{desc}** ({fact_count}ê°œ ê´€ê³„)")
                if entities:
                    lines.append(f"   - ì£¼ìš” ì—”í‹°í‹°: {', '.join(entities[:3])}")
                if relations:
                    lines.append(f"   - ê´€ê³„ ìœ í˜•: {', '.join(relations[:3])}")
                lines.append("")

            elif source["type"] == "ontology_inference":
                conf = source.get("confidence", 0) * 100
                rule_name = source.get("rule_name", "ì•Œ ìˆ˜ ì—†ìŒ")
                lines.append(f"{i}. {icon} **{desc}**")
                lines.append(f"   - ì‹ ë¢°ë„: {conf:.0f}%")
                lines.append(f"   - ê·œì¹™: {rule_name}")
                lines.append("")

            elif source["type"] == "rag_document":
                file_path = source.get("file_path", "")
                section = source.get("section", "")
                score = source.get("relevance_score", 0)
                file_name = file_path.split("/")[-1] if file_path else ""
                lines.append(f"{i}. {icon} **{desc}**")
                if file_name:
                    lines.append(f"   - íŒŒì¼: {file_name}")
                if section:
                    lines.append(f"   - ì„¹ì…˜: {section}")
                if score > 0:
                    lines.append(f"   - ê´€ë ¨ë„: {score:.2f}")
                lines.append("")

            elif source["type"] == "category_hierarchy":
                path = source.get("path", [])
                level = source.get("level", 0)
                url = source.get("url", "")
                lines.append(f"{i}. {icon} **{desc}**")
                if path:
                    lines.append(f"   - ê³„ì¸µ: {' > '.join(path)}")
                lines.append(f"   - ë ˆë²¨: {level}")
                if url:
                    lines.append(f"   - URL: {url}")
                lines.append("")

            elif source["type"] in ["external_news", "rss_feed"]:
                # ì™¸ë¶€ ë‰´ìŠ¤ / RSS í”¼ë“œ (Tavily, Allure, WWD ë“±)
                url = source.get("url", "")
                published_at = source.get("published_at", "")
                reliability = source.get("reliability_score", 0.7) * 100
                source_name = source.get("source", "")
                lines.append(f"{i}. {icon} **{desc}** (ì‹ ë¢°ë„: {reliability:.0f}%)")
                if source_name:
                    lines.append(f"   - ì¶œì²˜: {source_name}")
                if published_at:
                    lines.append(f"   - ë‚ ì§œ: {published_at}")
                if url:
                    lines.append(f"   - URL: {url}")
                lines.append("")

            elif source["type"] == "social_media":
                # ì†Œì…œ ë¯¸ë””ì–´ (Reddit, YouTube ë“±)
                url = source.get("url", "")
                published_at = source.get("published_at", "")
                reliability = source.get("reliability_score", 0.5) * 100
                source_name = source.get("source", "")
                relevance = source.get("relevance_score", 0)
                lines.append(f"{i}. {icon} **{desc}** (ì‹ ë¢°ë„: {reliability:.0f}%)")
                if source_name:
                    lines.append(f"   - í”Œëž«í¼: {source_name}")
                if published_at:
                    lines.append(f"   - ë‚ ì§œ: {published_at}")
                if relevance > 0:
                    lines.append(f"   - ê´€ë ¨ë„: {relevance:.2f}")
                if url:
                    lines.append(f"   - URL: {url}")
                lines.append("")

            elif source["type"] == "ai_model":
                model = source.get("model", "")
                disclaimer = source.get("disclaimer", "")
                lines.append(f"{i}. {icon} **{desc}**")
                if model:
                    lines.append(f"   - ëª¨ë¸: {model}")
                if disclaimer:
                    lines.append(f"   - ì°¸ê³ : {disclaimer}")
                lines.append("")

        return "\n".join(lines)

    def _extract_entity_names(self, ontology_facts) -> list[str]:
        """KG factsì—ì„œ ì—”í‹°í‹° ì´ë¦„ ì¶”ì¶œ"""
        entities = set()

        if isinstance(ontology_facts, list):
            for fact in ontology_facts:
                if isinstance(fact, dict):
                    subject = fact.get("subject", "")
                    obj = fact.get("object", "")
                    if subject:
                        entities.add(subject)
                    if obj:
                        entities.add(obj)
        elif isinstance(ontology_facts, dict):
            # ë‹¨ì¼ factì¸ ê²½ìš°
            subject = ontology_facts.get("subject", "")
            obj = ontology_facts.get("object", "")
            if subject:
                entities.add(subject)
            if obj:
                entities.add(obj)

        # Noneì´ë‚˜ ë¹ˆ ë¬¸ìžì—´ ì œê±° í›„ ìµœëŒ€ 5ê°œ ë°˜í™˜
        return list(filter(None, entities))[:5]

    def _extract_relation_types(self, ontology_facts) -> list[str]:
        """KG factsì—ì„œ ê´€ê³„ ìœ í˜• ì¶”ì¶œ"""
        relations = set()

        if isinstance(ontology_facts, list):
            for fact in ontology_facts:
                if isinstance(fact, dict):
                    predicate = fact.get("predicate", "")
                    if predicate:
                        relations.add(predicate)
        elif isinstance(ontology_facts, dict):
            # ë‹¨ì¼ factì¸ ê²½ìš°
            predicate = ontology_facts.get("predicate", "")
            if predicate:
                relations.add(predicate)

        # Noneì´ë‚˜ ë¹ˆ ë¬¸ìžì—´ ì œê±°
        return list(filter(None, relations))

    def _extract_mentioned_asins(
        self, hybrid_context: HybridContext, categories: dict[str, Any]
    ) -> list[dict[str, Any]]:
        """ì§ˆì˜ì—ì„œ ì–¸ê¸‰ëœ ì œí’ˆì˜ ASIN ì •ë³´ ì¶”ì¶œ"""
        mentioned_products = []
        seen_asins = set()

        # 1. KG ì—”í‹°í‹°ì—ì„œ ì œí’ˆëª…/ë¸Œëžœë“œ ì¶”ì¶œ
        mentioned_brands = set()
        if hybrid_context.entities:
            mentioned_brands = set(hybrid_context.entities.get("brands", []))

        # 2. ì¹´í…Œê³ ë¦¬ ë°ì´í„°ì—ì„œ ê´€ë ¨ ì œí’ˆ ASIN ì¶”ì¶œ
        for category_id, cat_data in categories.items():
            rank_records = cat_data.get("rank_records", [])

            for record in rank_records:
                asin = record.get("asin", "")
                brand = record.get("brand", "")
                product_name = record.get("product_name", record.get("title", ""))
                rank = record.get("rank", 0)

                # ì´ë¯¸ ì²˜ë¦¬ëœ ASIN ìŠ¤í‚µ
                if asin in seen_asins:
                    continue

                # ì–¸ê¸‰ëœ ë¸Œëžœë“œì˜ ì œí’ˆë§Œ í¬í•¨ (ìµœëŒ€ 5ê°œ)
                if brand in mentioned_brands:
                    seen_asins.add(asin)
                    mentioned_products.append(
                        {
                            "asin": asin,
                            "name": product_name,
                            "brand": brand,
                            "rank": rank,
                            "category": category_id,
                            "url": f"https://www.amazon.com/dp/{asin}" if asin else "",
                        }
                    )

                    if len(mentioned_products) >= 5:
                        break

            if len(mentioned_products) >= 5:
                break

        # ìˆœìœ„ ê¸°ì¤€ ì •ë ¬
        mentioned_products.sort(key=lambda x: x.get("rank", 999))
        return mentioned_products[:5]
