"""
Chat Routes
===========
ì±—ë´‡ ê´€ë ¨ API ì—”ë“œí¬ì¸íŠ¸ (v1, v2, v3, v4)
"""

import time
import logging

logger = logging.getLogger(__name__)
from typing import Dict, Any, List, Optional

from fastapi import APIRouter, HTTPException, Request
from pydantic import BaseModel
from litellm import acompletion

# Dependencies
from src.api.dependencies import (
    limiter,
    conversation_memory,
    get_conversation_history,
    add_to_memory,
    log_chat_interaction,
    load_dashboard_data
)

# RAG & Router
from src.rag.router import RAGRouter, QueryType
from src.rag.retriever import DocumentRetriever

# Core Services
from src.core.simple_chat import get_chat_service
from src.core.unified_orchestrator import get_unified_orchestrator
from src.core.brain import get_initialized_brain
from src.core.crawl_manager import get_crawl_manager

# ============= Router Setup =============

router = APIRouter()

# RAG System
rag_router = RAGRouter()
DOCS_PATH = "./docs/guides"
doc_retriever = DocumentRetriever(DOCS_PATH)


# ============= Pydantic Models =============

class ChatRequest(BaseModel):
    """ì±—ë´‡ ìš”ì²­ (v1)"""
    message: str
    session_id: Optional[str] = "default"
    context: Optional[Dict] = None


class ChatResponse(BaseModel):
    """ì±—ë´‡ ì‘ë‹µ (v1)"""
    response: str
    query_type: str
    confidence: float
    sources: List[str]
    suggestions: List[str]
    entities: Dict[str, Any]


class SimpleChatRequest(BaseModel):
    """Simple Chat ìš”ì²­ (v3)"""
    message: str
    session_id: Optional[str] = "default"


class SimpleChatResponse(BaseModel):
    """Simple Chat ì‘ë‹µ (v3)"""
    text: str
    suggestions: List[str]
    tools_used: List[str]
    sources: List[Dict[str, Any]] = []
    data_date: str
    processing_time_ms: float


class OrchestratorChatRequest(BaseModel):
    """LLM Orchestrator ì±—ë´‡ ìš”ì²­ (v2)"""
    message: str
    session_id: Optional[str] = "default"
    skip_cache: bool = False


class OrchestratorChatResponse(BaseModel):
    """LLM Orchestrator ì±—ë´‡ ì‘ë‹µ (v2)"""
    text: str
    query_type: str
    confidence_level: str
    confidence_score: float
    sources: List[str]
    entities: Dict[str, Any]
    tools_called: List[str]
    suggestions: List[str]
    is_fallback: bool
    is_clarification: bool
    processing_time_ms: float


class BrainChatRequest(BaseModel):
    """Brain ì±—ë´‡ ìš”ì²­ (v4)"""
    message: str
    session_id: Optional[str] = "default"
    skip_cache: bool = False


class BrainChatResponse(BaseModel):
    """Brain ì±—ë´‡ ì‘ë‹µ (v4)"""
    text: str
    confidence: float
    sources: List[str]
    reasoning: Optional[str] = None
    tools_used: List[str]
    processing_time_ms: float
    from_cache: bool
    brain_mode: str


# ============= Helper Functions =============

def build_data_context(data: Dict, query_type: QueryType, entities: Dict) -> str:
    """
    ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (Ontology ê¸°ë°˜)

    ì§ˆë¬¸ ìœ í˜•ê³¼ ì¶”ì¶œëœ ì—”í‹°í‹°ì— ë”°ë¼ í•„ìš”í•œ ë°ì´í„°ë§Œ ì„ íƒ
    """
    if not data:
        return "í˜„ì¬ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

    context_parts = []

    # ë©”íƒ€ë°ì´í„° (í•­ìƒ í¬í•¨)
    metadata = data.get("metadata", {})
    context_parts.append(f"""[ë°ì´í„° í˜„í™©]
- ê¸°ì¤€ì¼: {metadata.get('data_date', 'N/A')}
- ì´ ì œí’ˆ ìˆ˜: {metadata.get('total_products', 0)}ê°œ
- LANEIGE ì œí’ˆ ìˆ˜: {metadata.get('laneige_products', 0)}ê°œ""")

    # ì§ˆë¬¸ ìœ í˜•ë³„ ë°ì´í„° ì„ íƒ
    brand_kpis = data.get("brand", {}).get("kpis", {})

    # ì‹œì¥/ë¸Œëœë“œ ì§€í‘œ (DEFINITION, INTERPRETATION, ANALYSIS)
    if query_type in [QueryType.DEFINITION, QueryType.INTERPRETATION, QueryType.ANALYSIS, QueryType.COMBINATION]:
        if brand_kpis:
            context_parts.append(f"""
[LANEIGE ë¸Œëœë“œ KPI] (Ontology: BrandMetrics)
- SoS (Share of Shelf): {brand_kpis.get('sos', 0)}% {brand_kpis.get('sos_delta', '')}
- Top 10 ì œí’ˆ ìˆ˜: {brand_kpis.get('top10_count', 0)}ê°œ
- í‰ê·  ìˆœìœ„: {brand_kpis.get('avg_rank', 0)}ìœ„
- HHI (ì‹œì¥ ì§‘ì¤‘ë„): {brand_kpis.get('hhi', 0)}""")

    # ê²½ìŸì‚¬ ì •ë³´ (ANALYSIS, DATA_QUERYì—ì„œ ê²½ìŸì‚¬ ì–¸ê¸‰ ì‹œ)
    competitors = data.get("brand", {}).get("competitors", [])
    brands_mentioned = entities.get("brands", [])

    if query_type == QueryType.ANALYSIS or any(b for b in brands_mentioned if b.lower() != "laneige"):
        if competitors:
            top_comps = competitors[:5]
            comp_lines = [f"  - {c['brand']}: SoS {c['sos']}%, í‰ê·  ìˆœìœ„ {c['avg_rank']}ìœ„, ì œí’ˆ {c['product_count']}ê°œ" for c in top_comps]
            context_parts.append("[ê²½ìŸì‚¬ í˜„í™©]\n" + "\n".join(comp_lines))

    # ì œí’ˆ ì •ë³´ (DATA_QUERY, íŠ¹ì • ì œí’ˆ ì–¸ê¸‰ ì‹œ)
    products = data.get("products", {})
    products_mentioned = entities.get("products", [])

    if query_type == QueryType.DATA_QUERY or products_mentioned:
        if products:
            prod_lines = []
            for asin, p in list(products.items())[:5]:
                prod_lines.append(f"""  - {p['name'][:40]}
    ìˆœìœ„: #{p['rank']} ({p['rank_delta']}), í‰ì : {p['rating']}, ë³€ë™ì„±: {p.get('volatility_status', 'N/A')}""")
            context_parts.append("[LANEIGE ì œí’ˆ í˜„í™©] (Ontology: ProductMetrics)\n" + "\n".join(prod_lines))

    # ì¹´í…Œê³ ë¦¬ ì •ë³´
    categories = data.get("categories", {})
    categories_mentioned = entities.get("categories", [])

    if categories_mentioned or query_type in [QueryType.ANALYSIS, QueryType.INTERPRETATION]:
        if categories:
            cat_lines = []
            for cat_id, cat in categories.items():
                cat_lines.append(f"  - {cat['name']}: SoS {cat['sos']}%, ìµœê³  ìˆœìœ„ #{cat['best_rank']}, CPI {cat.get('cpi', 100)}")
            context_parts.append("[ì¹´í…Œê³ ë¦¬ í˜„í™©] (Ontology: MarketMetrics)\n" + "\n".join(cat_lines))

    # ì•¡ì…˜ ì•„ì´í…œ (ì „ëµ ì§ˆë¬¸)
    if query_type == QueryType.ANALYSIS:
        action_items = data.get("home", {}).get("action_items", [])
        if action_items:
            action_lines = [f"  - [{a['priority']}] {a['product_name']}: {a['signal']} â†’ {a['action_tag']}" for a in action_items[:4]]
            context_parts.append("[í˜„ì¬ ì•¡ì…˜ ì•„ì´í…œ]\n" + "\n".join(action_lines))

    return "\n\n".join(context_parts)


async def get_rag_context(query: str, query_type: QueryType) -> tuple[str, List[str]]:
    """
    RAG ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰

    Returns:
        (ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´, ì¶œì²˜ ëª©ë¡)
    """
    # DocumentRetriever ì´ˆê¸°í™” (ì²˜ìŒ í˜¸ì¶œ ì‹œ)
    if not doc_retriever._initialized:
        await doc_retriever.initialize()

    # ì§ˆë¬¸ ìœ í˜•ì— ë§ëŠ” ë¬¸ì„œ ê²€ìƒ‰
    target_doc = rag_router.get_target_document(query_type)

    # ê²€ìƒ‰ ì‹¤í–‰
    results = await doc_retriever.search(query, top_k=3, doc_filter=target_doc)

    if not results:
        return "", []

    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context_parts = []
    sources = []

    for result in results:
        metadata = result.get("metadata", {})
        content = result.get("content", "")
        title = metadata.get("title", "Unknown")
        doc_id = metadata.get("doc_id", "")

        context_parts.append(f"[{title}]\n{content}")

        # ì¶œì²˜ ì¶”ê°€
        doc_name_map = {
            "strategic_indicators": "Strategic Indicators Definition",
            "metric_interpretation": "Metric Interpretation Guide",
            "indicator_combination": "Indicator Combination Playbook",
            "home_insight_rules": "Home Page Insight Rules"
        }
        if doc_id in doc_name_map and doc_name_map[doc_id] not in sources:
            sources.append(doc_name_map[doc_id])

    return "\n\n---\n\n".join(context_parts), sources


def get_dynamic_suggestions(query_type: QueryType, entities: Dict, response: str) -> List[str]:
    """
    ë™ì  í›„ì† ì§ˆë¬¸ ì œì•ˆ

    ì‘ë‹µ ë‚´ìš©ê³¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë§ì¶¤í˜• ì œì•ˆ ìƒì„±
    """
    suggestions = []

    # ì—”í‹°í‹° ê¸°ë°˜ ì œì•ˆ
    brands = entities.get("brands", [])
    indicators = entities.get("indicators", [])
    categories = entities.get("categories", [])

    if query_type == QueryType.DEFINITION:
        # ì •ì˜ ì§ˆë¬¸ â†’ í•´ì„/í™œìš© ì§ˆë¬¸ ì œì•ˆ
        if indicators:
            ind = indicators[0].upper()
            suggestions.append(f"{ind}ê°€ ë†’ìœ¼ë©´ ì–´ë–¤ ì˜ë¯¸ì¸ê°€ìš”?")
            suggestions.append(f"{ind} ê°œì„ ì„ ìœ„í•œ ì „ëµì€?")
        suggestions.append("ë‹¤ë¥¸ ì§€í‘œì™€ í•¨ê»˜ í•´ì„í•˜ë©´ ì–´ë–¤ê°€ìš”?")

    elif query_type == QueryType.INTERPRETATION:
        # í•´ì„ ì§ˆë¬¸ â†’ ì•¡ì…˜/ì¡°í•© ì œì•ˆ
        suggestions.append("í˜„ì¬ LANEIGE ìˆ˜ì¹˜ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”")
        suggestions.append("ê²½ìŸì‚¬ì™€ ë¹„êµí•˜ë©´ ì–´ë–¤ê°€ìš”?")
        suggestions.append("ê°œì„ ì„ ìœ„í•œ ì•¡ì…˜ ì•„ì´í…œì€?")

    elif query_type == QueryType.DATA_QUERY:
        # ë°ì´í„° ì¡°íšŒ â†’ ì‹¬í™” ë¶„ì„ ì œì•ˆ
        suggestions.append("ì´ ìˆ˜ì¹˜ê°€ ì¢‹ì€ ê±´ê°€ìš”?")
        suggestions.append("ìµœê·¼ 7ì¼ ì¶”ì´ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”")
        suggestions.append("ê²½ìŸì‚¬ ëŒ€ë¹„ ì–´ë–¤ê°€ìš”?")

    elif query_type == QueryType.ANALYSIS:
        # ë¶„ì„ â†’ ì „ëµ/ì•¡ì…˜ ì œì•ˆ
        suggestions.append("ê°€ì¥ ì‹œê¸‰í•œ ì•¡ì…˜ì€ ë¬´ì—‡ì¸ê°€ìš”?")
        suggestions.append("Top 10 ì§„ì…ì„ ìœ„í•œ ì „ëµì€?")
        suggestions.append("ë¦¬ìŠ¤í¬ ìš”ì¸ì´ ìˆë‚˜ìš”?")

    elif query_type == QueryType.COMBINATION:
        # ì¡°í•© ì§ˆë¬¸ â†’ ë‹¤ë¥¸ ì¡°í•© ì œì•ˆ
        suggestions.append("ë‹¤ë¥¸ ì‹œë‚˜ë¦¬ì˜¤ë„ ë¶„ì„í•´ì£¼ì„¸ìš”")
        suggestions.append("í˜„ì¬ ë°ì´í„°ì—ì„œ í•´ë‹¹ ìƒí™©ì´ ìˆë‚˜ìš”?")

    else:
        # ê¸°ë³¸ ì œì•ˆ
        suggestions = [
            "SoS(ì ìœ ìœ¨)ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”",
            "í˜„ì¬ LANEIGE ìˆœìœ„ëŠ”?",
            "ì „ëµì  ê¶Œê³ ì‚¬í•­ì´ ìˆë‚˜ìš”?"
        ]

    return suggestions[:3]


# ============= Chat API v1 (RAG + Ontology) =============

@router.post("/chat", response_model=ChatResponse)
@limiter.limit("30/minute")
async def chat(request: ChatRequest, req: Request):
    """
    ChatGPT + RAG + Ontology í†µí•© ì±—ë´‡ API

    1. ì§ˆë¬¸ ë¶„ì„ (RAGRouter)
    2. ì—”í‹°í‹° ì¶”ì¶œ (Ontology ê¸°ë°˜)
    3. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ (RAG)
    4. ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    5. ëŒ€í™” ê¸°ë¡ ì°¸ì¡°
    6. LLM ì‘ë‹µ ìƒì„±
    7. Audit Trail ë¡œê¹…
    """
    start_time = time.time()

    message = request.message.strip()
    session_id = request.session_id or "default"

    if not message:
        raise HTTPException(status_code=400, detail="Message is required")

    # 1. ì§ˆë¬¸ ë¶„ë¥˜ (RAGRouter ì‚¬ìš©)
    route_result = rag_router.route(message)
    query_type = route_result["query_type"]
    confidence = route_result["confidence"]

    # 2. ì—”í‹°í‹° ì¶”ì¶œ (Ontology ê¸°ë°˜)
    entities = rag_router.extract_entities(message)

    # 3. ëª…í™•í™” í•„ìš” ì—¬ë¶€ í™•ì¸
    clarification = rag_router.needs_clarification(route_result, entities)
    if clarification and confidence < 0.5:
        # ëª…í™•í™” ìš”ì²­
        add_to_memory(session_id, "user", message)
        add_to_memory(session_id, "assistant", clarification)

        return ChatResponse(
            response=clarification,
            query_type=query_type.value if hasattr(query_type, 'value') else str(query_type),
            confidence=confidence,
            sources=[],
            suggestions=["ì˜ˆ, ì „ì²´ ë¸Œëœë“œ ë¶„ì„í•´ì£¼ì„¸ìš”", "LANEIGEë§Œ ë¶„ì„í•´ì£¼ì„¸ìš”", "Lip Care ì¹´í…Œê³ ë¦¬ë§Œ"],
            entities=entities
        )

    # 4. RAG ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
    rag_context, sources = await get_rag_context(message, query_type)

    # 5. ë°ì´í„° ë¡œë“œ ë° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    data = load_dashboard_data()
    data_context = build_data_context(data, query_type, entities)

    # 6. ëŒ€í™” ê¸°ë¡ ì¡°íšŒ
    conversation_history = get_conversation_history(session_id)

    # 7. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    system_prompt = """ë‹¹ì‹ ì€ AMORE Pacificì˜ LANEIGE ë¸Œëœë“œ Amazon ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì—­í• :
- Amazon US ë² ìŠ¤íŠ¸ì…€ëŸ¬ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì¸ì‚¬ì´íŠ¸ ì œê³µ
- LANEIGE ë¸Œëœë“œì˜ ì‹œì¥ í¬ì§€ì…˜ ë¶„ì„
- ê²½ìŸì‚¬ ëŒ€ë¹„ ì „ëµì  ê¶Œê³  ì œê³µ
- ì§€í‘œ ì •ì˜ ë° í•´ì„ ê°€ì´ë“œ ì œê³µ

Ontology ì—”í‹°í‹° ì´í•´:
- Brand: ë¸Œëœë“œ ì •ë³´ (LANEIGE, ê²½ìŸì‚¬ ë“±)
- Product: ì œí’ˆ ì •ë³´ (ASIN, ìˆœìœ„, í‰ì , ê°€ê²© ë“±)
- Category: ì¹´í…Œê³ ë¦¬ (Lip Care, Skin Care ë“±)
- BrandMetrics: SoS, í‰ê· ìˆœìœ„, ì œí’ˆìˆ˜ ë“±
- ProductMetrics: ìˆœìœ„ë³€ë™ì„±, ì—°ì†ì²´ë¥˜ì¼, í‰ì ì¶”ì„¸ ë“±
- MarketMetrics: HHI(ì‹œì¥ì§‘ì¤‘ë„), êµì²´ìœ¨ ë“±

ì‘ë‹µ ê°€ì´ë“œë¼ì¸:
1. ë°ì´í„°ì— ê¸°ë°˜í•œ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ ì¸ìš©
2. RAG ë¬¸ì„œì˜ ì •ì˜/í•´ì„ ê¸°ì¤€ í™œìš©
3. ì´ì „ ëŒ€í™” ë§¥ë½ ê³ ë ¤
4. ê°„ê²°í•˜ê³  ì•¡ì…˜ ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸ ì œê³µ
5. ë¶ˆí™•ì‹¤í•œ ê²½ìš° ëª…í™•íˆ ë°í ê²ƒ
6. ë‹¨ì •ì  í‘œí˜„ í”¼í•˜ê¸°
7. í•œêµ­ì–´ë¡œ ì‘ë‹µ

ì§ˆë¬¸ ìœ í˜•ë³„ ì‘ë‹µ ìŠ¤íƒ€ì¼:
- ì •ì˜(DEFINITION): ì§€í‘œì˜ ì •ì˜, ì‚°ì¶œì‹, ì˜ë¯¸ë¥¼ ì„¤ëª…
- í•´ì„(INTERPRETATION): ìˆ˜ì¹˜ì˜ ì˜ë¯¸, ì¢‹ê³  ë‚˜ì¨ì˜ ê¸°ì¤€ ì„¤ëª…
- ì¡°í•©(COMBINATION): ì—¬ëŸ¬ ì§€í‘œë¥¼ í•¨ê»˜ í•´ì„, ì‹œë‚˜ë¦¬ì˜¤ë³„ ì•¡ì…˜ ì œì•ˆ
- ë°ì´í„°ì¡°íšŒ(DATA_QUERY): í˜„ì¬ ìˆ˜ì¹˜ì™€ ë³€ë™ í˜„í™© ì•ˆë‚´
- ë¶„ì„(ANALYSIS): ì¢…í•© ë¶„ì„ê³¼ ì „ëµì  ê¶Œê³  ì œê³µ
"""

    # 8. ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    user_prompt = f"""## ì‚¬ìš©ì ì§ˆë¬¸
{message}

## ì§ˆë¬¸ ìœ í˜•
{query_type.value if hasattr(query_type, 'value') else str(query_type)} (ì‹ ë¢°ë„: {confidence:.1%})

## ì¶”ì¶œëœ ì—”í‹°í‹°
- ë¸Œëœë“œ: {', '.join(entities.get('brands', [])) or 'ì—†ìŒ'}
- ì¹´í…Œê³ ë¦¬: {', '.join(entities.get('categories', [])) or 'ì—†ìŒ'}
- ì§€í‘œ: {', '.join(entities.get('indicators', [])) or 'ì—†ìŒ'}
- ê¸°ê°„: {entities.get('time_range') or 'ì—†ìŒ'}

## RAG ì°¸ì¡° ë¬¸ì„œ
{rag_context if rag_context else 'ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ'}

## í˜„ì¬ ë°ì´í„°
{data_context}

## ì´ì „ ëŒ€í™”
{conversation_history if conversation_history else 'ì´ì „ ëŒ€í™” ì—†ìŒ'}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
- ì§ˆë¬¸ ìœ í˜•ì— ë§ëŠ” ì‘ë‹µ ìŠ¤íƒ€ì¼ì„ ì‚¬ìš©í•˜ì„¸ìš”.
- RAG ë¬¸ì„œì— ê´€ë ¨ ì •ì˜/í•´ì„ì´ ìˆìœ¼ë©´ ì¸ìš©í•˜ì„¸ìš”.
- ì´ì „ ëŒ€í™” ë§¥ë½ì´ ìˆìœ¼ë©´ ê³ ë ¤í•˜ì„¸ìš”.
"""

    try:
        response = await acompletion(
            model="gpt-4.1-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.3,
            max_tokens=1000
        )

        answer = response.choices[0].message.content

        # 9. ëŒ€í™” ë©”ëª¨ë¦¬ì— ì €ì¥
        add_to_memory(session_id, "user", message)
        add_to_memory(session_id, "assistant", answer)

        # 10. ë™ì  í›„ì† ì§ˆë¬¸ ì œì•ˆ
        suggestions = get_dynamic_suggestions(query_type, entities, answer)

        # 11. Audit Trail ë¡œê¹…
        response_time_ms = (time.time() - start_time) * 1000
        log_chat_interaction(
            session_id=session_id,
            user_query=message,
            ai_response=answer,
            query_type=query_type.value if hasattr(query_type, 'value') else str(query_type),
            confidence=confidence,
            entities=entities,
            sources=sources,
            response_time_ms=response_time_ms
        )

        return ChatResponse(
            response=answer,
            query_type=query_type.value if hasattr(query_type, 'value') else str(query_type),
            confidence=confidence,
            sources=sources,
            suggestions=suggestions,
            entities=entities
        )

    except Exception as e:
        logger.error(f"LLM Error: {e}")

        # Fallback ì‘ë‹µ
        fallback = route_result.get("fallback_message") or rag_router.get_fallback_response("unknown")

        # ë°ì´í„° ê¸°ë°˜ ê¸°ë³¸ ì‘ë‹µ ì¶”ê°€
        if data and query_type == QueryType.DATA_QUERY:
            brand_kpis = data.get("brand", {}).get("kpis", {})
            fallback = f"""í˜„ì¬ LANEIGE í˜„í™©:
- SoS: {brand_kpis.get('sos', 0)}%
- Top 10 ì œí’ˆ: {brand_kpis.get('top10_count', 0)}ê°œ
- í‰ê·  ìˆœìœ„: {brand_kpis.get('avg_rank', 0)}ìœ„

(ìƒì„¸ ë¶„ì„ì„ ìœ„í•´ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”)"""

        # Fallback ì‘ë‹µë„ Audit Trail ê¸°ë¡
        response_time_ms = (time.time() - start_time) * 1000
        log_chat_interaction(
            session_id=session_id,
            user_query=message,
            ai_response=f"[ERROR] {str(e)[:100]} | Fallback: {fallback[:200]}",
            query_type=query_type.value if hasattr(query_type, 'value') else str(query_type),
            confidence=0.0,
            entities=entities,
            sources=["fallback"],
            response_time_ms=response_time_ms
        )

        return ChatResponse(
            response=fallback,
            query_type=query_type.value if hasattr(query_type, 'value') else str(query_type),
            confidence=0.0,
            sources=[],
            suggestions=["ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”", "SoSê°€ ë­”ê°€ìš”?", "í˜„ì¬ ìˆœìœ„ ì•Œë ¤ì£¼ì„¸ìš”"],
            entities=entities
        )


@router.delete("/chat/memory/{session_id}")
async def clear_memory(session_id: str):
    """ì„¸ì…˜ ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"""
    if session_id in conversation_memory:
        del conversation_memory[session_id]
    return {"status": "ok", "message": f"Session {session_id} memory cleared"}


# ============= Chat API v3 (Simple Chat) =============

@router.post("/v3/chat", response_model=SimpleChatResponse)
@limiter.limit("30/minute")
async def chat_v3(request: SimpleChatRequest, req: Request):
    """
    Simple LLM Chat API (v3)

    ë‹¨ìˆœí™”ëœ êµ¬ì¡°:
    - LLMì´ ëª¨ë“  íŒë‹¨ ë‹´ë‹¹
    - Function Callingìœ¼ë¡œ ë„êµ¬ ì‚¬ìš©
    - ë¶ˆí•„ìš”í•œ ë ˆì´ì–´ ì œê±°
    """
    message = request.message.strip()
    session_id = request.session_id or "default"

    if not message:
        raise HTTPException(status_code=400, detail="Message is required")

    # í¬ë¡¤ë§ ìƒíƒœ ì²´í¬
    crawl_manager = await get_crawl_manager()
    crawl_notification = None
    crawl_started = False

    if crawl_manager.needs_crawl():
        crawl_started = await crawl_manager.start_crawl()

    if crawl_manager.should_notify(session_id):
        crawl_notification = crawl_manager.get_notification_message()
        crawl_manager.mark_notified(session_id)

    # Simple Chat Serviceë¡œ ì²˜ë¦¬
    chat_service = get_chat_service()
    result = await chat_service.chat(message, session_id)

    # í¬ë¡¤ë§ ì•Œë¦¼ ì¶”ê°€
    response_text = result["text"]
    if crawl_notification:
        response_text = f"{crawl_notification}\n\n---\n\n{response_text}"
    elif crawl_started:
        data_date = crawl_manager.get_data_date() or "ì—†ìŒ"
        response_text = (
            f"ğŸ“¡ **ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì˜¤ëŠ˜ ë°ì´í„° ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤.**\n"
            f"í˜„ì¬ ë°ì´í„°: {data_date}\n"
            f"ìˆ˜ì§‘ì´ ì™„ë£Œë˜ë©´ ì•Œë ¤ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n\n---\n\n{response_text}"
        )

    return SimpleChatResponse(
        text=response_text,
        suggestions=result.get("suggestions", []),
        tools_used=result.get("tools_used", []),
        sources=result.get("sources", []),
        data_date=result.get("data_date", "N/A"),
        processing_time_ms=result.get("processing_time_ms", 0)
    )


# ============= Chat API v2 (Orchestrator - deprecated) =============

@router.post("/v2/chat", response_model=OrchestratorChatResponse)
@limiter.limit("30/minute")
async def chat_v2(request: OrchestratorChatRequest, req: Request):
    """
    í†µí•© ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ê¸°ë°˜ ì±—ë´‡ API (v2)

    ë™ì‘ íë¦„:
    1. ì§ˆë¬¸ ìˆ˜ì‹ 
    2. ì‹œìŠ¤í…œ ìƒíƒœ ì ê²€ (ë°ì´í„° ì‹ ì„ ë„, ì‚¬ìš© ê°€ëŠ¥ ì—ì´ì „íŠ¸)
    3. LLMì´ ìƒí™© íŒë‹¨ â†’ ì—ì´ì „íŠ¸ ì„ íƒ
    4. ì—ì´ì „íŠ¸ ì‹¤í–‰ (ì—ëŸ¬ ì‹œ ì „ëµì— ë”°ë¼ ì²˜ë¦¬)
    5. ì‘ë‹µ ìƒì„±

    ì—ëŸ¬ ì „ëµ:
    - RETRY: ì¬ì‹œë„ (ìµœëŒ€ 2íšŒ)
    - FALLBACK: ìºì‹œ ë°ì´í„° ì‚¬ìš©
    - SKIP: ê±´ë„ˆë›°ê³  ê³„ì†
    - ABORT: ì¤‘ë‹¨ + ì‚¬ìš©ì ì•Œë¦¼
    """
    start_time = time.time()

    message = request.message.strip()
    session_id = request.session_id or "default"

    if not message:
        raise HTTPException(status_code=400, detail="Message is required")

    # í¬ë¡¤ë§ ìƒíƒœ ì²´í¬
    crawl_manager = await get_crawl_manager()
    crawl_notification = None
    crawl_started = False

    if crawl_manager.needs_crawl():
        crawl_started = await crawl_manager.start_crawl()
        if crawl_started:
            logging.info("Background crawl started for today's data")

    if crawl_manager.should_notify(session_id):
        crawl_notification = crawl_manager.get_notification_message()
        crawl_manager.mark_notified(session_id)

    try:
        # í†µí•© ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ë¡œ ì²˜ë¦¬
        orchestrator = await get_unified_orchestrator()

        # í˜„ì¬ ë©”íŠ¸ë¦­ ë°ì´í„° ë¡œë“œ
        data = load_dashboard_data()
        current_metrics = data if data else None

        # ì²˜ë¦¬
        response = await orchestrator.process(
            query=message,
            session_id=session_id,
            current_metrics=current_metrics,
            skip_cache=request.skip_cache
        )

        # ì‘ë‹µ í…ìŠ¤íŠ¸ êµ¬ì„±
        response_text = response.text

        # í¬ë¡¤ë§ ì•Œë¦¼ ì¶”ê°€
        if crawl_notification:
            response_text = f"{crawl_notification}\n\n---\n\n{response_text}"
        elif crawl_started:
            data_date = crawl_manager.get_data_date() or "ì—†ìŒ"
            response_text = (
                f"ğŸ“¡ **ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì˜¤ëŠ˜ ë°ì´í„° ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤.**\n"
                f"í˜„ì¬ ë°ì´í„°: {data_date}\n"
                f"ìˆ˜ì§‘ì´ ì™„ë£Œë˜ë©´ ì•Œë ¤ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n\n---\n\n{response_text}"
            )

        # ì‘ë‹µ ë³€í™˜
        return OrchestratorChatResponse(
            text=response_text,
            query_type=response.query_type,
            confidence_level=response.confidence_level.value,
            confidence_score=response.confidence_score,
            sources=response.sources,
            entities=response.entities,
            tools_called=response.tools_called,
            suggestions=response.suggestions,
            is_fallback=response.is_fallback,
            is_clarification=response.is_clarification,
            processing_time_ms=response.processing_time_ms
        )

    except Exception as e:
        logging.error(f"Orchestrator error: {e}")
        return OrchestratorChatResponse(
            text=f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            query_type="error",
            confidence_level="low",
            confidence_score=0.0,
            sources=[],
            entities={},
            tools_called=[],
            suggestions=["ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”"],
            is_fallback=True,
            is_clarification=False,
            processing_time_ms=(time.time() - start_time) * 1000
        )


# ============= Chat API v4 (Brain - LLM First) =============

@router.post("/v4/chat", response_model=BrainChatResponse)
@limiter.limit("30/minute")
async def chat_v4(request: BrainChatRequest, req: Request):
    """
    Level 4 Brain ê¸°ë°˜ ì±—ë´‡ API (v4)

    LLM-First ì ‘ê·¼:
    - ëª¨ë“  íŒë‹¨ì„ LLMì´ ìˆ˜í–‰
    - ê·œì¹™ ê¸°ë°˜ ë¹ ë¥¸ ê²½ë¡œ ì—†ìŒ
    - RAG + KG í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
    - ììœ¨ ìŠ¤ì¼€ì¤„ëŸ¬ì™€ í†µí•©
    """
    start_time = time.time()

    message = request.message.strip()
    session_id = request.session_id or "default"

    if not message:
        raise HTTPException(status_code=400, detail="Message is required")

    try:
        # Brain ì¸ìŠ¤í„´ìŠ¤ íšë“
        brain = await get_initialized_brain()

        # í˜„ì¬ ë©”íŠ¸ë¦­ ë°ì´í„° ë¡œë“œ
        data = load_dashboard_data()
        current_metrics = data if data else None

        # Brainìœ¼ë¡œ ì²˜ë¦¬ (LLM-First)
        response = await brain.process_query(
            query=message,
            session_id=session_id,
            current_metrics=current_metrics,
            skip_cache=request.skip_cache
        )

        processing_time = (time.time() - start_time) * 1000

        return BrainChatResponse(
            text=response.content,
            confidence=response.confidence,
            sources=response.sources,
            reasoning=response.reasoning,
            tools_used=response.tools_called if hasattr(response, 'tools_called') else [],
            processing_time_ms=processing_time,
            from_cache=response.from_cache if hasattr(response, 'from_cache') else False,
            brain_mode=brain.mode.value
        )

    except Exception as e:
        logging.error(f"Brain error: {e}")
        return BrainChatResponse(
            text=f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            confidence=0.0,
            sources=[],
            reasoning=None,
            tools_used=[],
            processing_time_ms=(time.time() - start_time) * 1000,
            from_cache=False,
            brain_mode="error"
        )
