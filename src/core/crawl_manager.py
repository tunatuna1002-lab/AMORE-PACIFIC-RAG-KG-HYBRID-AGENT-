"""
Crawl Manager
ì¼ì¼ í¬ë¡¤ë§ ìƒíƒœ ê´€ë¦¬ ë° ë°±ê·¸ë¼ìš´ë“œ í¬ë¡¤ë§ ì„œë¹„ìŠ¤

í”Œë¡œìš°:
1. ì²« ì§ˆë¬¸ ì‹œ ì˜¤ëŠ˜ ë°ì´í„° ì²´í¬
2. ì—†ìœ¼ë©´ ë°±ê·¸ë¼ìš´ë“œ í¬ë¡¤ë§ ì‹œìž‘
3. í¬ë¡¤ë§ ì¤‘ì—ë„ ê³¼ê±° ë°ì´í„°ë¡œ ì‘ë‹µ ê°€ëŠ¥
4. ì™„ë£Œ ì‹œ ë‹¤ìŒ ì‘ë‹µì— ì•Œë¦¼ í¬í•¨
"""

import asyncio
import json
import logging
from datetime import datetime, date, timezone, timedelta
from pathlib import Path
from typing import Optional, Dict, Any, Callable
from enum import Enum
from dataclasses import dataclass, field

logger = logging.getLogger(__name__)

# í•œêµ­ ì‹œê°„ëŒ€ (UTC+9)
KST = timezone(timedelta(hours=9))


class CrawlStatus(Enum):
    """í¬ë¡¤ë§ ìƒíƒœ"""
    IDLE = "idle"               # ëŒ€ê¸° ì¤‘
    RUNNING = "running"         # í¬ë¡¤ë§ ì§„í–‰ ì¤‘
    COMPLETED = "completed"     # ì™„ë£Œ
    FAILED = "failed"           # ì‹¤íŒ¨


@dataclass
class CrawlState:
    """í¬ë¡¤ë§ ìƒíƒœ ì •ë³´"""
    status: CrawlStatus = CrawlStatus.IDLE
    date: Optional[str] = None  # í¬ë¡¤ë§ ëŒ€ìƒ ë‚ ì§œ
    started_at: Optional[str] = None
    completed_at: Optional[str] = None
    progress: int = 0  # 0-100
    categories_done: int = 0
    categories_total: int = 0
    products_collected: int = 0
    error: Optional[str] = None

    # ì•Œë¦¼ í”Œëž˜ê·¸ (ì„¸ì…˜ë³„ë¡œ ê´€ë¦¬)
    notified_sessions: set = field(default_factory=set)

    def to_dict(self) -> Dict[str, Any]:
        return {
            "status": self.status.value,
            "date": self.date,
            "started_at": self.started_at,
            "completed_at": self.completed_at,
            "progress": self.progress,
            "categories_done": self.categories_done,
            "categories_total": self.categories_total,
            "products_collected": self.products_collected,
            "error": self.error
        }


class CrawlManager:
    """ì¼ì¼ í¬ë¡¤ë§ ê´€ë¦¬ìž"""

    STATE_FILE = "./data/crawl_state.json"
    DATA_FILE = "./data/dashboard_data.json"

    def __init__(self):
        self.state = CrawlState()
        self._crawl_task: Optional[asyncio.Task] = None
        self._on_complete_callback: Optional[Callable] = None
        self._load_state()

    def _load_state(self):
        """ì €ìž¥ëœ ìƒíƒœ ë¡œë“œ"""
        try:
            if Path(self.STATE_FILE).exists():
                with open(self.STATE_FILE, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    self.state = CrawlState(
                        status=CrawlStatus(data.get("status", "idle")),
                        date=data.get("date"),
                        started_at=data.get("started_at"),
                        completed_at=data.get("completed_at"),
                        progress=data.get("progress", 0),
                        categories_done=data.get("categories_done", 0),
                        categories_total=data.get("categories_total", 0),
                        products_collected=data.get("products_collected", 0),
                        error=data.get("error")
                    )
        except Exception as e:
            logger.warning(f"Failed to load crawl state: {e}")

    def _save_state(self):
        """ìƒíƒœ ì €ìž¥"""
        try:
            Path(self.STATE_FILE).parent.mkdir(parents=True, exist_ok=True)
            with open(self.STATE_FILE, "w", encoding="utf-8") as f:
                json.dump(self.state.to_dict(), f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"Failed to save crawl state: {e}")

    def get_kst_today(self) -> str:
        """í•œêµ­ ì‹œê°„ ê¸°ì¤€ ì˜¤ëŠ˜ ë‚ ì§œ ë°˜í™˜"""
        return datetime.now(KST).date().isoformat()

    def get_data_date(self) -> Optional[str]:
        """í˜„ìž¬ ë°ì´í„°ì˜ ë‚ ì§œ ë°˜í™˜"""
        try:
            if Path(self.DATA_FILE).exists():
                with open(self.DATA_FILE, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    return data.get("metadata", {}).get("data_date")
        except Exception as e:
            logger.warning(f"Failed to read data date: {e}")
        return None

    def is_today_data_available(self) -> bool:
        """ì˜¤ëŠ˜(í•œêµ­ì‹œê°„ ê¸°ì¤€) ë°ì´í„°ê°€ ìžˆëŠ”ì§€ í™•ì¸"""
        data_date = self.get_data_date()
        kst_today = self.get_kst_today()
        logger.info(f"Data date check: data={data_date}, kst_today={kst_today}")
        return data_date == kst_today

    async def check_sheets_data_exists(self, target_date: str) -> bool:
        """
        Google Sheetsì—ì„œ í•´ë‹¹ ë‚ ì§œì˜ ë°ì´í„°ê°€ ìžˆëŠ”ì§€ í™•ì¸

        Args:
            target_date: í™•ì¸í•  ë‚ ì§œ (YYYY-MM-DD)

        Returns:
            í•´ë‹¹ ë‚ ì§œ ë°ì´í„° ì¡´ìž¬ ì—¬ë¶€
        """
        try:
            from src.tools.sheets_writer import SheetsWriter
            sheets = SheetsWriter()
            await sheets.initialize()

            # ìµœê·¼ 1ì¼ ë°ì´í„°ë§Œ ê°€ì ¸ì™€ì„œ í™•ì¸
            records = await sheets.get_rank_history(days=1)

            # í•´ë‹¹ ë‚ ì§œ ë°ì´í„°ê°€ ìžˆëŠ”ì§€ í™•ì¸
            for record in records:
                if record.get("snapshot_date") == target_date:
                    logger.info(f"Found data for {target_date} in Google Sheets")
                    return True

            logger.info(f"No data found for {target_date} in Google Sheets")
            return False

        except Exception as e:
            logger.warning(f"Failed to check Sheets data: {e}")
            return False

    def is_crawling(self) -> bool:
        """í¬ë¡¤ë§ ì§„í–‰ ì¤‘ì¸ì§€ í™•ì¸"""
        return self.state.status == CrawlStatus.RUNNING

    def needs_crawl(self) -> bool:
        """í¬ë¡¤ë§ì´ í•„ìš”í•œì§€ í™•ì¸ (í•œêµ­ì‹œê°„ ê¸°ì¤€)"""
        kst_today = self.get_kst_today()

        # ì´ë¯¸ ì§„í–‰ ì¤‘ì´ë©´ í•„ìš” ì—†ìŒ
        if self.is_crawling():
            logger.info("Crawl not needed: already running")
            return False

        # ì˜¤ëŠ˜(KST) ë°ì´í„°ê°€ ìžˆìœ¼ë©´ í•„ìš” ì—†ìŒ
        if self.is_today_data_available():
            logger.info("Crawl not needed: today's data available")
            return False

        # ì˜¤ëŠ˜(KST) ì´ë¯¸ ì™„ë£Œí–ˆìœ¼ë©´ í•„ìš” ì—†ìŒ
        if (self.state.status == CrawlStatus.COMPLETED and
            self.state.date == kst_today):
            logger.info("Crawl not needed: already completed today")
            return False

        logger.info(f"Crawl needed: no data for {kst_today}")
        return True

    async def needs_crawl_with_sheets_check(self) -> bool:
        """
        í¬ë¡¤ë§ì´ í•„ìš”í•œì§€ í™•ì¸ (Google Sheetsê¹Œì§€ í™•ì¸)

        ë¡œì»¬ íŒŒì¼ ì²´í¬ í›„, í™•ì‹¤í•˜ì§€ ì•Šìœ¼ë©´ Sheetsê¹Œì§€ í™•ì¸
        """
        kst_today = self.get_kst_today()

        # ì´ë¯¸ ì§„í–‰ ì¤‘ì´ë©´ í•„ìš” ì—†ìŒ
        if self.is_crawling():
            return False

        # ë¡œì»¬ íŒŒì¼ì— ì˜¤ëŠ˜ ë°ì´í„°ê°€ ìžˆìœ¼ë©´ í•„ìš” ì—†ìŒ
        if self.is_today_data_available():
            return False

        # ì˜¤ëŠ˜ ì´ë¯¸ ì™„ë£Œí–ˆìœ¼ë©´ í•„ìš” ì—†ìŒ
        if (self.state.status == CrawlStatus.COMPLETED and
            self.state.date == kst_today):
            return False

        # Google Sheetsì—ì„œ ìµœì¢… í™•ì¸
        if await self.check_sheets_data_exists(kst_today):
            logger.info(f"Data exists in Sheets for {kst_today}, skipping crawl")
            return False

        return True

    def should_notify(self, session_id: str) -> bool:
        """í•´ë‹¹ ì„¸ì…˜ì— í¬ë¡¤ë§ ì™„ë£Œ ì•Œë¦¼ì´ í•„ìš”í•œì§€ í™•ì¸"""
        if self.state.status != CrawlStatus.COMPLETED:
            return False
        if self.state.date != self.get_kst_today():
            return False
        if session_id in self.state.notified_sessions:
            return False
        return True

    def mark_notified(self, session_id: str):
        """ì„¸ì…˜ì— ì•Œë¦¼ ì™„ë£Œ í‘œì‹œ"""
        self.state.notified_sessions.add(session_id)

    async def start_crawl(self, on_complete: Optional[Callable] = None) -> bool:
        """
        ë°±ê·¸ë¼ìš´ë“œ í¬ë¡¤ë§ ì‹œìž‘

        Returns:
            True if crawl started, False if already running
        """
        if self.is_crawling():
            logger.info("Crawl already in progress")
            return False

        self._on_complete_callback = on_complete
        self._crawl_task = asyncio.create_task(self._run_crawl())
        return True

    async def _run_crawl(self):
        """ì‹¤ì œ í¬ë¡¤ë§ ì‹¤í–‰"""
        from src.agents.crawler_agent import CrawlerAgent
        from src.agents.storage_agent import StorageAgent
        from src.tools.dashboard_exporter import DashboardExporter

        kst_today = self.get_kst_today()

        # ìƒíƒœ ì´ˆê¸°í™”
        self.state = CrawlState(
            status=CrawlStatus.RUNNING,
            date=kst_today,
            started_at=datetime.now(KST).isoformat(),
            categories_total=5  # configì—ì„œ ê°€ì ¸ì˜¤ë©´ ë” ì¢‹ìŒ
        )
        self._save_state()

        logger.info(f"Starting daily crawl for {kst_today} (KST)")

        try:
            # 1. í¬ë¡¤ë§ ì‹¤í–‰
            crawler = CrawlerAgent()
            await crawler.scraper.initialize()

            result = await crawler.execute()

            await crawler.scraper.close()

            if result.get("status") == "failed":
                raise Exception("All categories failed")

            self.state.products_collected = result.get("total_products", 0)
            self.state.categories_done = len(result.get("categories", {}))
            self.state.progress = 30
            self._save_state()

            logger.info(f"Crawl completed: {self.state.products_collected} products")

            # 2. Google Sheetsì— ë°ì´í„° ì €ìž¥
            logger.info("Saving data to Google Sheets...")
            storage = StorageAgent()
            storage_result = await storage.execute(result)

            self.state.progress = 60
            self._save_state()

            if storage_result.get("errors"):
                logger.warning(f"Storage warnings: {storage_result['errors']}")
            else:
                logger.info(f"Saved {storage_result.get('raw_records', 0)} records to Google Sheets")

            # 3. Dashboard ë°ì´í„° ìƒì„± (Google Sheetsì—ì„œ ì½ì–´ì˜´)
            logger.info("Starting Dashboard data export...")
            try:
                exporter = DashboardExporter()
                logger.info("DashboardExporter created")
                await exporter.initialize()
                logger.info("DashboardExporter initialized")
                await exporter.export_dashboard_data(self.DATA_FILE)
                logger.info(f"Dashboard data exported to {self.DATA_FILE}")
            except Exception as export_error:
                logger.error(f"Dashboard export failed: {export_error}")
                raise

            self.state.progress = 100
            self.state.status = CrawlStatus.COMPLETED
            self.state.completed_at = datetime.now(KST).isoformat()
            self.state.notified_sessions = set()  # ì•Œë¦¼ ì´ˆê¸°í™”
            self._save_state()

            logger.info(f"Dashboard data exported for {kst_today}")

            # SimpleChatService ìºì‹œ ë¬´íš¨í™”
            try:
                from src.core.simple_chat import get_chat_service
                chat_service = get_chat_service()
                chat_service.invalidate_cache()
                logger.info("Chat service cache invalidated")
            except Exception as e:
                logger.warning(f"Failed to invalidate chat cache: {e}")

            # ì™„ë£Œ ì½œë°± ì‹¤í–‰
            if self._on_complete_callback:
                try:
                    await self._on_complete_callback(self.state)
                except Exception as e:
                    logger.error(f"Complete callback error: {e}")

        except Exception as e:
            logger.error(f"Crawl failed: {e}")
            self.state.status = CrawlStatus.FAILED
            self.state.error = str(e)
            self.state.completed_at = datetime.now(KST).isoformat()
            self._save_state()

    def get_status_message(self) -> str:
        """í˜„ìž¬ ìƒíƒœ ë©”ì‹œì§€ ë°˜í™˜"""
        if self.state.status == CrawlStatus.IDLE:
            data_date = self.get_data_date()
            if data_date:
                return f"ë§ˆì§€ë§‰ ë°ì´í„°: {data_date}"
            return "ë°ì´í„° ì—†ìŒ"

        elif self.state.status == CrawlStatus.RUNNING:
            return f"ë°ì´í„° ìˆ˜ì§‘ ì¤‘... ({self.state.progress}%)"

        elif self.state.status == CrawlStatus.COMPLETED:
            return f"ì˜¤ëŠ˜ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ ({self.state.products_collected}ê°œ ì œí’ˆ)"

        elif self.state.status == CrawlStatus.FAILED:
            return f"ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {self.state.error}"

        return "ì•Œ ìˆ˜ ì—†ìŒ"

    def get_notification_message(self) -> str:
        """í¬ë¡¤ë§ ì™„ë£Œ ì•Œë¦¼ ë©”ì‹œì§€"""
        return (
            f"ðŸ“Š **ì˜¤ëŠ˜({self.state.date}) ë°ì´í„° ìˆ˜ì§‘ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!**\n\n"
            f"- ìˆ˜ì§‘ ì œí’ˆ: {self.state.products_collected}ê°œ\n"
            f"- ìˆ˜ì§‘ ì¹´í…Œê³ ë¦¬: {self.state.categories_done}ê°œ\n"
            f"- ì™„ë£Œ ì‹œê°„: {self.state.completed_at}\n\n"
            "ì´ì œ ìµœì‹  ë°ì´í„°ë¡œ ë¶„ì„ì„ ì§„í–‰í•©ë‹ˆë‹¤."
        )


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_crawl_manager: Optional[CrawlManager] = None


def get_crawl_manager() -> CrawlManager:
    """CrawlManager ì‹±ê¸€í†¤ ë°˜í™˜"""
    global _crawl_manager
    if _crawl_manager is None:
        _crawl_manager = CrawlManager()
    return _crawl_manager
