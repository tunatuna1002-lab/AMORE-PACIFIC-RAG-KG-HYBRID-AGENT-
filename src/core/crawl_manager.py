"""
Crawl Manager
ì¼ì¼ í¬ë¡¤ë§ ìƒíƒœ ê´€ë¦¬ ë° ë°±ê·¸ë¼ìš´ë“œ í¬ë¡¤ë§ ì„œë¹„ìŠ¤

í”Œë¡œìš°:
1. ì²« ì§ˆë¬¸ ì‹œ ì˜¤ëŠ˜ ë°ì´í„° ì²´í¬
2. ì—†ìœ¼ë©´ ë°±ê·¸ë¼ìš´ë“œ í¬ë¡¤ë§ ì‹œìž‘
3. í¬ë¡¤ë§ ì¤‘ì—ë„ ê³¼ê±° ë°ì´í„°ë¡œ ì‘ë‹µ ê°€ëŠ¥
4. ì™„ë£Œ ì‹œ ë‹¤ìŒ ì‘ë‹µì— ì•Œë¦¼ í¬í•¨
"""

import asyncio
import json
import logging
import os
from collections.abc import Callable
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from pathlib import Path
from typing import Any

from src.shared.constants import KST

logger = logging.getLogger(__name__)


def _get_data_dir() -> str:
    """Railway Volume ë˜ëŠ” ë¡œì»¬ ë°ì´í„° ë””ë ‰í† ë¦¬ ë°˜í™˜"""
    if os.environ.get("RAILWAY_ENVIRONMENT"):
        return "/data"
    return "./data"


# í•œêµ­ ì‹œê°„ëŒ€ (UTC+9)
class CrawlStatus(Enum):
    """í¬ë¡¤ë§ ìƒíƒœ"""

    IDLE = "idle"  # ëŒ€ê¸° ì¤‘
    RUNNING = "running"  # í¬ë¡¤ë§ ì§„í–‰ ì¤‘
    COMPLETED = "completed"  # ì™„ë£Œ
    FAILED = "failed"  # ì‹¤íŒ¨


@dataclass
class CrawlState:
    """í¬ë¡¤ë§ ìƒíƒœ ì •ë³´"""

    status: CrawlStatus = CrawlStatus.IDLE
    date: str | None = None  # í¬ë¡¤ë§ ëŒ€ìƒ ë‚ ì§œ
    started_at: str | None = None
    completed_at: str | None = None
    progress: int = 0  # 0-100
    categories_done: int = 0
    categories_total: int = 0
    products_collected: int = 0
    error: str | None = None

    # ì•Œë¦¼ í”Œëž˜ê·¸ (ì„¸ì…˜ë³„ë¡œ ê´€ë¦¬)
    notified_sessions: set = field(default_factory=set)

    def to_dict(self) -> dict[str, Any]:
        return {
            "status": self.status.value,
            "date": self.date,
            "started_at": self.started_at,
            "completed_at": self.completed_at,
            "progress": self.progress,
            "categories_done": self.categories_done,
            "categories_total": self.categories_total,
            "products_collected": self.products_collected,
            "error": self.error,
        }


class CrawlManager:
    """ì¼ì¼ í¬ë¡¤ë§ ê´€ë¦¬ìž"""

    STATE_FILE = f"{_get_data_dir()}/crawl_state.json"
    DATA_FILE = f"{_get_data_dir()}/dashboard_data.json"

    def __init__(self):
        self.state = CrawlState()
        self._crawl_task: asyncio.Task | None = None
        self._on_complete_callback: Callable | None = None
        self._load_state()

    def _load_state(self):
        """ì €ìž¥ëœ ìƒíƒœ ë¡œë“œ"""
        try:
            if Path(self.STATE_FILE).exists():
                with open(self.STATE_FILE, encoding="utf-8") as f:
                    data = json.load(f)
                    self.state = CrawlState(
                        status=CrawlStatus(data.get("status", "idle")),
                        date=data.get("date"),
                        started_at=data.get("started_at"),
                        completed_at=data.get("completed_at"),
                        progress=data.get("progress", 0),
                        categories_done=data.get("categories_done", 0),
                        categories_total=data.get("categories_total", 0),
                        products_collected=data.get("products_collected", 0),
                        error=data.get("error"),
                    )
        except Exception as e:
            logger.warning(f"Failed to load crawl state: {e}")

    def _save_state(self):
        """ìƒíƒœë¥¼ íŒŒì¼ì— ì›ìžì ìœ¼ë¡œ ì €ìž¥ (crash-safe)"""
        try:
            import tempfile

            Path(self.STATE_FILE).parent.mkdir(parents=True, exist_ok=True)

            # ì›ìžì  ì“°ê¸°: ìž„ì‹œ íŒŒì¼ì— ì“´ í›„ rename (crash-safe)
            dir_path = str(Path(self.STATE_FILE).parent)
            with tempfile.NamedTemporaryFile(
                mode="w", dir=dir_path, delete=False, suffix=".tmp", encoding="utf-8"
            ) as f:
                json.dump(self.state.to_dict(), f, ensure_ascii=False, indent=2)
                temp_path = f.name
            os.replace(temp_path, self.STATE_FILE)  # ì›ìžì  êµì²´
        except Exception as e:
            logger.error(f"Failed to save crawl state: {e}")
            # ìž„ì‹œ íŒŒì¼ ì •ë¦¬
            try:
                if "temp_path" in locals() and os.path.exists(temp_path):
                    os.remove(temp_path)
            except Exception as e:
                logger.debug(f"ìž„ì‹œ íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨ (ë¬´ì‹œë¨): {e}")

    def get_kst_today(self) -> str:
        """í•œêµ­ ì‹œê°„ ê¸°ì¤€ ì˜¤ëŠ˜ ë‚ ì§œ ë°˜í™˜"""
        return datetime.now(KST).date().isoformat()

    def get_data_date(self) -> str | None:
        """í˜„ìž¬ ë°ì´í„°ì˜ ë‚ ì§œ ë°˜í™˜"""
        try:
            if Path(self.DATA_FILE).exists():
                with open(self.DATA_FILE, encoding="utf-8") as f:
                    data = json.load(f)
                    return data.get("metadata", {}).get("data_date")
        except Exception as e:
            logger.warning(f"Failed to read data date: {e}")
        return None

    def is_today_data_available(self) -> bool:
        """ì˜¤ëŠ˜(í•œêµ­ì‹œê°„ ê¸°ì¤€) ë°ì´í„°ê°€ ìžˆëŠ”ì§€ í™•ì¸"""
        data_date = self.get_data_date()
        kst_today = self.get_kst_today()
        logger.info(f"Data date check: data={data_date}, kst_today={kst_today}")
        return data_date == kst_today

    async def check_sheets_data_exists(self, target_date: str) -> bool:
        """
        Google Sheetsì—ì„œ í•´ë‹¹ ë‚ ì§œì˜ ë°ì´í„°ê°€ ìžˆëŠ”ì§€ í™•ì¸

        Args:
            target_date: í™•ì¸í•  ë‚ ì§œ (YYYY-MM-DD)

        Returns:
            í•´ë‹¹ ë‚ ì§œ ë°ì´í„° ì¡´ìž¬ ì—¬ë¶€
        """
        try:
            from src.tools.storage.sheets_writer import SheetsWriter

            sheets = SheetsWriter()
            await sheets.initialize()

            # ìµœê·¼ 1ì¼ ë°ì´í„°ë§Œ ê°€ì ¸ì™€ì„œ í™•ì¸
            records = await sheets.get_rank_history(days=1)

            # í•´ë‹¹ ë‚ ì§œ ë°ì´í„°ê°€ ìžˆëŠ”ì§€ í™•ì¸
            for record in records:
                if record.get("snapshot_date") == target_date:
                    logger.info(f"Found data for {target_date} in Google Sheets")
                    return True

            logger.info(f"No data found for {target_date} in Google Sheets")
            return False

        except Exception as e:
            logger.warning(f"Failed to check Sheets data: {e}")
            return False

    def is_crawling(self) -> bool:
        """í¬ë¡¤ë§ ì§„í–‰ ì¤‘ì¸ì§€ í™•ì¸ (stale lock ê°ì§€ í¬í•¨)"""
        if self.state.status != CrawlStatus.RUNNING:
            return False

        # Stale lock ê°ì§€: 2ì‹œê°„ ì´ìƒ running ìƒíƒœë©´ ì£½ì€ ê²ƒìœ¼ë¡œ íŒë‹¨
        if self.state.started_at:
            try:
                started = datetime.fromisoformat(self.state.started_at)
                elapsed = datetime.now(KST) - started
                if elapsed > timedelta(hours=2):
                    logger.warning(
                        f"Stale crawl lock detected: started {self.state.started_at}, "
                        f"elapsed {elapsed}. Resetting to FAILED."
                    )
                    self.state.status = CrawlStatus.FAILED
                    self.state.error = f"Stale lock: process unresponsive for {elapsed}"
                    self.state.completed_at = datetime.now(KST).isoformat()
                    self._save_state()
                    return False
            except (ValueError, TypeError) as e:
                logger.warning(f"Failed to parse started_at: {e}, resetting stale lock")
                self.state.status = CrawlStatus.FAILED
                self.state.error = "Stale lock: invalid started_at timestamp"
                self.state.completed_at = datetime.now(KST).isoformat()
                self._save_state()
                return False

        return True

    def needs_crawl(self) -> bool:
        """í¬ë¡¤ë§ì´ í•„ìš”í•œì§€ í™•ì¸ (í•œêµ­ì‹œê°„ ê¸°ì¤€)"""
        kst_today = self.get_kst_today()

        # ì´ë¯¸ ì§„í–‰ ì¤‘ì´ë©´ í•„ìš” ì—†ìŒ
        if self.is_crawling():
            logger.info("Crawl not needed: already running")
            return False

        # ì˜¤ëŠ˜(KST) ë°ì´í„°ê°€ ìžˆìœ¼ë©´ í•„ìš” ì—†ìŒ
        if self.is_today_data_available():
            logger.info("Crawl not needed: today's data available")
            return False

        # ì˜¤ëŠ˜(KST) ì´ë¯¸ ì™„ë£Œí–ˆìœ¼ë©´ í•„ìš” ì—†ìŒ
        if self.state.status == CrawlStatus.COMPLETED and self.state.date == kst_today:
            logger.info("Crawl not needed: already completed today")
            return False

        logger.info(f"Crawl needed: no data for {kst_today}")
        return True

    async def needs_crawl_with_sheets_check(self) -> bool:
        """
        í¬ë¡¤ë§ì´ í•„ìš”í•œì§€ í™•ì¸ (Google Sheetsê¹Œì§€ í™•ì¸)

        ë¡œì»¬ íŒŒì¼ ì²´í¬ í›„, í™•ì‹¤í•˜ì§€ ì•Šìœ¼ë©´ Sheetsê¹Œì§€ í™•ì¸
        """
        kst_today = self.get_kst_today()

        # ì´ë¯¸ ì§„í–‰ ì¤‘ì´ë©´ í•„ìš” ì—†ìŒ
        if self.is_crawling():
            return False

        # ë¡œì»¬ íŒŒì¼ì— ì˜¤ëŠ˜ ë°ì´í„°ê°€ ìžˆìœ¼ë©´ í•„ìš” ì—†ìŒ
        if self.is_today_data_available():
            return False

        # ì˜¤ëŠ˜ ì´ë¯¸ ì™„ë£Œí–ˆìœ¼ë©´ í•„ìš” ì—†ìŒ
        if self.state.status == CrawlStatus.COMPLETED and self.state.date == kst_today:
            return False

        # Google Sheetsì—ì„œ ìµœì¢… í™•ì¸
        if await self.check_sheets_data_exists(kst_today):
            logger.info(f"Data exists in Sheets for {kst_today}, skipping crawl")
            return False

        return True

    def should_notify(self, session_id: str) -> bool:
        """í•´ë‹¹ ì„¸ì…˜ì— í¬ë¡¤ë§ ì™„ë£Œ ì•Œë¦¼ì´ í•„ìš”í•œì§€ í™•ì¸"""
        if self.state.status != CrawlStatus.COMPLETED:
            return False
        if self.state.date != self.get_kst_today():
            return False
        if session_id in self.state.notified_sessions:
            return False
        return True

    def mark_notified(self, session_id: str):
        """ì„¸ì…˜ì— ì•Œë¦¼ ì™„ë£Œ í‘œì‹œ"""
        self.state.notified_sessions.add(session_id)

    async def start_crawl(self, on_complete: Callable | None = None) -> bool:
        """
        ë°±ê·¸ë¼ìš´ë“œ í¬ë¡¤ë§ ì‹œìž‘

        Returns:
            True if crawl started, False if already running
        """
        if self.is_crawling():
            logger.info("Crawl already in progress")
            return False

        self._on_complete_callback = on_complete
        self._crawl_task = asyncio.create_task(self._run_crawl())
        return True

    async def _run_crawl(self):
        """ì‹¤ì œ í¬ë¡¤ë§ ì‹¤í–‰"""
        from src.infrastructure.container import Container
        from src.tools.exporters.dashboard_exporter import DashboardExporter

        kst_today = self.get_kst_today()

        # ìƒíƒœ ì´ˆê¸°í™”
        self.state = CrawlState(
            status=CrawlStatus.RUNNING,
            date=kst_today,
            started_at=datetime.now(KST).isoformat(),
            categories_total=5,  # configì—ì„œ ê°€ì ¸ì˜¤ë©´ ë” ì¢‹ìŒ
        )
        self._save_state()

        logger.info(f"Starting daily crawl for {kst_today} (KST)")

        try:
            # 1. í¬ë¡¤ë§ ì‹¤í–‰
            crawler = Container.get_crawler_agent()
            await crawler.scraper.initialize()

            result = await crawler.execute()

            await crawler.scraper.close()

            if result.get("status") == "failed":
                raise Exception("All categories failed")

            self.state.products_collected = result.get("total_products", 0)
            self.state.categories_done = len(result.get("categories", {}))
            self.state.progress = 30
            self._save_state()

            logger.info(f"Crawl completed: {self.state.products_collected} products")

            # í¬ë¡¤ë§ ì›ë³¸ ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ì €ìž¥ (Excel exportìš©)
            try:
                crawl_json_path = Path(f"{_get_data_dir()}/latest_crawl_result.json")

                # JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜ (datetime, Decimal ë“± ì²˜ë¦¬)
                def json_serializer(obj):
                    if hasattr(obj, "isoformat"):
                        return obj.isoformat()
                    if hasattr(obj, "__str__"):
                        return str(obj)
                    raise TypeError(f"Object of type {type(obj)} is not JSON serializable")

                with open(crawl_json_path, "w", encoding="utf-8") as f:
                    json.dump(result, f, ensure_ascii=False, indent=2, default=json_serializer)
                logger.info(f"Crawl result saved to {crawl_json_path}")

                # ë‚ ì§œë³„ ížˆìŠ¤í† ë¦¬ ë°ì´í„° ì €ìž¥ (raw_products í´ë”)
                raw_products_dir = Path(f"{_get_data_dir()}/raw_products")
                raw_products_dir.mkdir(parents=True, exist_ok=True)

                snapshot_date = result.get("snapshot_date", datetime.now(KST).strftime("%Y-%m-%d"))
                history_path = raw_products_dir / f"{snapshot_date}.json"

                # ëª¨ë“  ì¹´í…Œê³ ë¦¬ì˜ ì œí’ˆì„ í”Œëž« ë¦¬ìŠ¤íŠ¸ë¡œ ì €ìž¥
                all_products = []
                for cat_id, cat_data in result.get("categories", {}).items():
                    for product in cat_data.get("products", []):
                        product["category_id"] = cat_id
                        all_products.append(product)

                with open(history_path, "w", encoding="utf-8") as f:
                    json.dump(
                        all_products, f, ensure_ascii=False, indent=2, default=json_serializer
                    )
                logger.info(
                    f"Historical data saved to {history_path} ({len(all_products)} products)"
                )
            except Exception as save_error:
                logger.error(f"Failed to save crawl result JSON: {save_error}")

            # 2. Google Sheetsì— ë°ì´í„° ì €ìž¥
            logger.info("Saving data to Google Sheets...")
            storage = Container.get_storage_agent()
            storage_result = await storage.execute(result)

            self.state.progress = 60
            self._save_state()

            if storage_result.get("errors"):
                logger.warning(f"Storage warnings: {storage_result['errors']}")
            else:
                logger.info(
                    f"Saved {storage_result.get('raw_records', 0)} records to Google Sheets"
                )

            # 3. Dashboard ë°ì´í„° ìƒì„± (Google Sheetsì—ì„œ ì½ì–´ì˜´)
            logger.info("Starting Dashboard data export...")
            try:
                exporter = DashboardExporter()
                logger.info("DashboardExporter created")
                await exporter.initialize()
                logger.info("DashboardExporter initialized")
                await exporter.export_dashboard_data(self.DATA_FILE)
                logger.info(f"Dashboard data exported to {self.DATA_FILE}")
            except Exception as export_error:
                logger.error(f"Dashboard export failed (non-fatal): {export_error}")
                # í¬ë¡¤ë§+ì €ìž¥ì€ ì„±ê³µ â†’ export ì‹¤íŒ¨ëŠ” ì¹˜ëª…ì ì´ì§€ ì•Šìœ¼ë¯€ë¡œ ê³„ì† ì§„í–‰

            self.state.progress = 100
            self.state.status = CrawlStatus.COMPLETED
            self.state.completed_at = datetime.now(KST).isoformat()
            self.state.notified_sessions = set()  # ì•Œë¦¼ ì´ˆê¸°í™”
            self._save_state()

            logger.info(f"Dashboard data exported for {kst_today}")

            # Brain ìºì‹œ ë¬´íš¨í™”
            try:
                from src.core.brain import get_brain

                brain = await get_brain()
                if brain and hasattr(brain, "_response_pipeline") and brain._response_pipeline:
                    brain._response_pipeline._cache.clear()
                    logger.info("Brain response cache invalidated")
            except Exception as e:
                logger.warning(f"Failed to invalidate brain cache: {e}")

            # ì™„ë£Œ ì½œë°± ì‹¤í–‰
            if self._on_complete_callback:
                try:
                    await self._on_complete_callback(self.state)
                except Exception as e:
                    logger.error(f"Complete callback error: {e}")

        except Exception as e:
            logger.error(f"Crawl failed: {e}")
            self.state.status = CrawlStatus.FAILED
            self.state.error = str(e)
            self.state.completed_at = datetime.now(KST).isoformat()
            self._save_state()

    def get_status_message(self) -> str:
        """í˜„ìž¬ ìƒíƒœ ë©”ì‹œì§€ ë°˜í™˜"""
        if self.state.status == CrawlStatus.IDLE:
            data_date = self.get_data_date()
            if data_date:
                return f"ë§ˆì§€ë§‰ ë°ì´í„°: {data_date}"
            return "ë°ì´í„° ì—†ìŒ"

        elif self.state.status == CrawlStatus.RUNNING:
            return f"ë°ì´í„° ìˆ˜ì§‘ ì¤‘... ({self.state.progress}%)"

        elif self.state.status == CrawlStatus.COMPLETED:
            return f"ì˜¤ëŠ˜ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ ({self.state.products_collected}ê°œ ì œí’ˆ)"

        elif self.state.status == CrawlStatus.FAILED:
            return f"ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {self.state.error}"

        return "ì•Œ ìˆ˜ ì—†ìŒ"

    def get_notification_message(self) -> str:
        """í¬ë¡¤ë§ ì™„ë£Œ ì•Œë¦¼ ë©”ì‹œì§€"""
        return (
            f"ðŸ“Š **ì˜¤ëŠ˜({self.state.date}) ë°ì´í„° ìˆ˜ì§‘ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!**\n\n"
            f"- ìˆ˜ì§‘ ì œí’ˆ: {self.state.products_collected}ê°œ\n"
            f"- ìˆ˜ì§‘ ì¹´í…Œê³ ë¦¬: {self.state.categories_done}ê°œ\n"
            f"- ì™„ë£Œ ì‹œê°„: {self.state.completed_at}\n\n"
            "ì´ì œ ìµœì‹  ë°ì´í„°ë¡œ ë¶„ì„ì„ ì§„í–‰í•©ë‹ˆë‹¤."
        )


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ (ìŠ¤ë ˆë“œ ì•ˆì „)
_crawl_manager: CrawlManager | None = None
_crawl_manager_lock = asyncio.Lock()


async def get_crawl_manager() -> CrawlManager:
    """CrawlManager ì‹±ê¸€í†¤ ë°˜í™˜ (ìŠ¤ë ˆë“œ ì•ˆì „)"""
    global _crawl_manager
    async with _crawl_manager_lock:
        if _crawl_manager is None:
            _crawl_manager = CrawlManager()
    return _crawl_manager
