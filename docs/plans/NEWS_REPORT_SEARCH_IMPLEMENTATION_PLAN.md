# ë‰´ìŠ¤ ë° ë³´ê³ ì„œ ê²€ìƒ‰ ê¸°ëŠ¥ êµ¬í˜„ ê³„íš

> **ì‘ì„±ì¼**: 2026-01-26
> **ëª©í‘œ**: Tavily API, Apify API, ê³µê³µë°ì´í„° APIë¥¼ í™œìš©í•œ í†µí•© ë‰´ìŠ¤/ë³´ê³ ì„œ ê²€ìƒ‰ ì‹œìŠ¤í…œ êµ¬ì¶•

---

## 1. í˜„ì¬ ìƒíƒœ ë¶„ì„

### 1.1 API ì—°ë™ í˜„í™©

| API | êµ¬í˜„ ìƒíƒœ | ì½”ë“œ ìœ„ì¹˜ | ë¹„ê³  |
|-----|----------|-----------|------|
| **Tavily API** | âŒ ë¯¸êµ¬í˜„ | ì„¤ì •ë§Œ ì¡´ì¬ (`config/public_apis.json`) | ë‰´ìŠ¤ ê²€ìƒ‰ìš©, ~$10/ì›” |
| **Apify API** | âœ… êµ¬í˜„ë¨ | `src/tools/apify_amazon_scraper.py` | ê¸°ë³¸ ë¹„í™œì„±í™” |
| **YouTube Collector** | âœ… êµ¬í˜„ë¨ | `src/tools/youtube_collector.py` | ë¯¸í†µí•© |
| **ê³µê³µë°ì´í„°** | âš ï¸ í”„ë ˆì„ì›Œí¬ë§Œ | `src/tools/public_data_collector.py` | API í˜¸ì¶œ ë¯¸êµ¬í˜„ |
| **RSS í”¼ë“œ** | âœ… ì‘ë™ ì¤‘ | `src/tools/external_signal_collector.py` | 23ê°œ ì†ŒìŠ¤ |
| **Reddit API** | âœ… ì‘ë™ ì¤‘ | `src/tools/external_signal_collector.py` | JSON API (ë¬´ë£Œ) |

### 1.2 í•µì‹¬ ë¬¸ì œì 

1. **Tavily API ë¯¸êµ¬í˜„**: ì„¤ì • íŒŒì¼ì—ë§Œ ì¡´ì¬, ì‹¤ì œ ê²€ìƒ‰ ë¡œì§ ì—†ìŒ
2. **ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì†ì‹¤**: ì™¸ë¶€ ì‹ í˜¸ì˜ ë©”íƒ€ë°ì´í„°(ì‹ ë¢°ë„, ê´€ë ¨ì„±)ê°€ LLM í”„ë¡¬í”„íŠ¸ì— ì „ë‹¬ ì•ˆë¨
3. **ReferenceTracker ë¯¸í†µí•©**: ì™¸ë¶€ ì‹ í˜¸ê°€ ì°¸ê³ ìë£Œ ì„¹ì…˜ì— ìë™ ì¶”ê°€ ì•ˆë¨
4. **ì‹ í˜¸-ì§€í‘œ ìƒê´€ë¶„ì„ ë¶€ì¬**: ì™¸ë¶€ ì‹ í˜¸ì™€ SoS/HHI ë³€í™” ì—°ê²° ë¶„ì„ ì—†ìŒ

---

## 2. êµ¬í˜„ ëª©í‘œ

### 2.1 Phase 1: Tavily API í†µí•© (ìš°ì„ ìˆœìœ„: ë†’ìŒ)

**ëª©í‘œ**: ì‹¤ì‹œê°„ ë‰´ìŠ¤ ë° ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥ ì¶”ê°€

```
ì‚¬ìš©ì ì¿¼ë¦¬ â†’ Tavily ê²€ìƒ‰ â†’ ê²°ê³¼ ì •ì œ â†’ ì¸ì‚¬ì´íŠ¸ ìƒì„± â†’ ë³´ê³ ì„œ ë°˜ì˜
```

### 2.2 Phase 2: ì™¸ë¶€ ì‹ í˜¸ íŒŒì´í”„ë¼ì¸ ê°•í™” (ìš°ì„ ìˆœìœ„: ë†’ìŒ)

**ëª©í‘œ**: ìˆ˜ì§‘ëœ ì‹ í˜¸ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ë³´ê³ ì„œì— ì™„ì „íˆ ë°˜ì˜

```
ì™¸ë¶€ ì‹ í˜¸ ìˆ˜ì§‘ â†’ ë©”íƒ€ë°ì´í„° ë³´ì¡´ â†’ LLM í”„ë¡¬í”„íŠ¸ ì „ë‹¬ â†’ ReferenceTracker ìë™ ë“±ë¡
```

### 2.3 Phase 3: ê³µê³µë°ì´í„° API ì—°ë™ (ìš°ì„ ìˆœìœ„: ì¤‘ê°„)

**ëª©í‘œ**: í•œêµ­ ê³µê³µë°ì´í„° (ê´€ì„¸ì²­, ì‹ì•½ì²˜, KOSIS) ì‹¤ì œ ì—°ë™

```
ê³µê³µ API í˜¸ì¶œ â†’ ë°ì´í„° íŒŒì‹± â†’ ì‹œì¥ ë¶„ì„ í†µí•© â†’ ë³´ê³ ì„œ ë°˜ì˜
```

### 2.4 Phase 4: YouTube Collector í†µí•© (ìš°ì„ ìˆœìœ„: ì¤‘ê°„)

**ëª©í‘œ**: YouTube ë·°í‹° ì½˜í…ì¸  íŠ¸ë Œë“œ ë¶„ì„ ì—°ë™

```
YouTube ê²€ìƒ‰ â†’ ì˜ìƒ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ â†’ íŠ¸ë Œë“œ ë¶„ì„ â†’ ë³´ê³ ì„œ ë°˜ì˜
```

---

## 3. ìƒì„¸ êµ¬í˜„ ê³„íš

### 3.1 Phase 1: Tavily API í†µí•©

#### 3.1.1 ìƒˆ íŒŒì¼ ìƒì„±: `src/tools/tavily_search.py`

```python
"""
Tavily Search API í†µí•©
=====================
ì‹¤ì‹œê°„ ë‰´ìŠ¤ ë° ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥ ì œê³µ

Features:
- ë·°í‹°/í™”ì¥í’ˆ ê´€ë ¨ ë‰´ìŠ¤ ê²€ìƒ‰
- ë¸Œëœë“œë³„ ê²€ìƒ‰
- ê²€ìƒ‰ ê²°ê³¼ ì‹ ë¢°ë„ í‰ê°€
- ReferenceTracker ìë™ ì—°ë™
"""

from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from datetime import datetime
import httpx
import os

@dataclass
class TavilySearchResult:
    """Tavily ê²€ìƒ‰ ê²°ê³¼"""
    title: str
    url: str
    content: str
    score: float  # ê´€ë ¨ì„± ì ìˆ˜ (0-1)
    published_date: Optional[str]
    source: str  # ì¶œì²˜ ë„ë©”ì¸

class TavilySearchClient:
    """Tavily Search API í´ë¼ì´ì–¸íŠ¸"""

    BASE_URL = "https://api.tavily.com"

    # ë·°í‹° ì‚°ì—… ì‹ ë¢° ì†ŒìŠ¤ (ì‹ ë¢°ë„ ê°€ì¤‘ì¹˜ ì ìš©)
    TRUSTED_SOURCES = {
        "allure.com": 0.95,
        "wwd.com": 0.95,
        "beautyindependent.com": 0.90,
        "cosmeticsdesign.com": 0.90,
        "reuters.com": 0.95,
        "bloomberg.com": 0.95,
        "forbes.com": 0.85,
    }

    def __init__(self):
        self.api_key = os.getenv("TAVILY_API_KEY")
        if not self.api_key:
            raise ValueError("TAVILY_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        self.client = httpx.AsyncClient(timeout=30.0)

    async def search_news(
        self,
        query: str,
        search_depth: str = "advanced",  # basic or advanced
        max_results: int = 10,
        include_domains: List[str] = None,
        exclude_domains: List[str] = None,
        days: int = 7  # ìµœê·¼ Nì¼
    ) -> List[TavilySearchResult]:
        """
        ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤í–‰

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬ (ì˜ˆ: "LANEIGE lip sleeping mask review")
            search_depth: ê²€ìƒ‰ ê¹Šì´ (basic: ë¹ ë¦„, advanced: ì •í™•)
            max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜
            include_domains: í¬í•¨í•  ë„ë©”ì¸ ë¦¬ìŠ¤íŠ¸
            exclude_domains: ì œì™¸í•  ë„ë©”ì¸ ë¦¬ìŠ¤íŠ¸
            days: ê²€ìƒ‰ ê¸°ê°„ (ìµœê·¼ Nì¼)

        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        payload = {
            "api_key": self.api_key,
            "query": query,
            "search_depth": search_depth,
            "max_results": max_results,
            "include_answer": True,
            "include_raw_content": False,
            "days": days
        }

        if include_domains:
            payload["include_domains"] = include_domains
        if exclude_domains:
            payload["exclude_domains"] = exclude_domains

        response = await self.client.post(
            f"{self.BASE_URL}/search",
            json=payload
        )
        response.raise_for_status()
        data = response.json()

        results = []
        for item in data.get("results", []):
            result = TavilySearchResult(
                title=item.get("title", ""),
                url=item.get("url", ""),
                content=item.get("content", ""),
                score=item.get("score", 0.5),
                published_date=item.get("published_date"),
                source=self._extract_domain(item.get("url", ""))
            )
            results.append(result)

        return results

    async def search_beauty_news(
        self,
        brands: List[str] = None,
        topics: List[str] = None,
        days: int = 7
    ) -> List[TavilySearchResult]:
        """
        ë·°í‹° ì‚°ì—… íŠ¹í™” ë‰´ìŠ¤ ê²€ìƒ‰

        Args:
            brands: ë¸Œëœë“œëª… ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ["LANEIGE", "COSRX"])
            topics: í† í”½ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ["K-Beauty", "skincare trends"])
            days: ê²€ìƒ‰ ê¸°ê°„
        """
        queries = []

        # ë¸Œëœë“œë³„ ì¿¼ë¦¬ ìƒì„±
        if brands:
            for brand in brands:
                queries.append(f"{brand} beauty news")
                queries.append(f"{brand} skincare review")

        # í† í”½ë³„ ì¿¼ë¦¬ ìƒì„±
        if topics:
            for topic in topics:
                queries.append(f"{topic} 2026")

        # ê¸°ë³¸ ì¿¼ë¦¬
        if not queries:
            queries = [
                "K-Beauty trends 2026",
                "LANEIGE Amazon bestseller",
                "Korean skincare news"
            ]

        all_results = []
        for query in queries[:5]:  # API ë¹„ìš© ê³ ë ¤ ìµœëŒ€ 5ê°œ ì¿¼ë¦¬
            results = await self.search_news(
                query=query,
                days=days,
                include_domains=list(self.TRUSTED_SOURCES.keys())
            )
            all_results.extend(results)

        # ì¤‘ë³µ ì œê±° ë° ì‹ ë¢°ë„ ê¸°ë°˜ ì •ë ¬
        unique_results = self._deduplicate_and_rank(all_results)
        return unique_results

    def _extract_domain(self, url: str) -> str:
        """URLì—ì„œ ë„ë©”ì¸ ì¶”ì¶œ"""
        from urllib.parse import urlparse
        parsed = urlparse(url)
        return parsed.netloc.replace("www.", "")

    def _deduplicate_and_rank(
        self,
        results: List[TavilySearchResult]
    ) -> List[TavilySearchResult]:
        """ì¤‘ë³µ ì œê±° ë° ì‹ ë¢°ë„ ê¸°ë°˜ ë­í‚¹"""
        seen_urls = set()
        unique = []

        for result in results:
            if result.url not in seen_urls:
                seen_urls.add(result.url)
                # ì‹ ë¢°ë„ ê°€ì¤‘ì¹˜ ì ìš©
                trust_weight = self.TRUSTED_SOURCES.get(result.source, 0.7)
                result.score = result.score * trust_weight
                unique.append(result)

        # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        unique.sort(key=lambda x: x.score, reverse=True)
        return unique

    async def close(self):
        await self.client.aclose()
```

#### 3.1.2 ExternalSignalCollector í†µí•©

**íŒŒì¼**: `src/tools/external_signal_collector.py`

```python
# ê¸°ì¡´ í´ë˜ìŠ¤ì— ì¶”ê°€

class ExternalSignalCollector:
    def __init__(self):
        # ... ê¸°ì¡´ ì½”ë“œ ...
        self.tavily_client = None

    async def initialize(self):
        # ... ê¸°ì¡´ ì½”ë“œ ...
        # Tavily í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        try:
            from src.tools.tavily_search import TavilySearchClient
            self.tavily_client = TavilySearchClient()
            logger.info("Tavily Search API ì´ˆê¸°í™” ì™„ë£Œ")
        except ValueError as e:
            logger.warning(f"Tavily API ë¯¸ì„¤ì •: {e}")

    async def fetch_tavily_news(
        self,
        brands: List[str] = None,
        topics: List[str] = None,
        days: int = 7
    ) -> List[ExternalSignal]:
        """Tavily APIë¡œ ë‰´ìŠ¤ ê²€ìƒ‰"""
        if not self.tavily_client:
            logger.warning("Tavily í´ë¼ì´ì–¸íŠ¸ ë¯¸ì´ˆê¸°í™”")
            return []

        results = await self.tavily_client.search_beauty_news(
            brands=brands,
            topics=topics,
            days=days
        )

        signals = []
        for result in results:
            signal = ExternalSignal(
                source="tavily",
                tier=3,  # Tier 3: Authority
                title=result.title,
                url=result.url,
                content=result.content,
                date=result.published_date or datetime.now().isoformat(),
                reliability_score=result.score,
                relevance_score=result.score,
                metadata={
                    "domain": result.source,
                    "search_type": "news"
                }
            )
            signals.append(signal)

        return signals
```

---

### 3.2 Phase 2: ì™¸ë¶€ ì‹ í˜¸ íŒŒì´í”„ë¼ì¸ ê°•í™”

#### 3.2.1 PeriodInsightAgent ìˆ˜ì •

**íŒŒì¼**: `src/agents/period_insight_agent.py`

**í˜„ì¬ ë¬¸ì œ** (ë¼ì¸ 428-469):
```python
# í˜„ì¬ ì½”ë“œ - ë©”íƒ€ë°ì´í„° ì†ì‹¤
async def _generate_external_signals(self, signals):
    signal_count = len(signals)
    sources = [s.source for s in signals]
    # ì‹ ë¢°ë„, ê´€ë ¨ì„± ì ìˆ˜ê°€ ì „ë‹¬ ì•ˆë¨!
```

**ìˆ˜ì • ê³„íš**:
```python
async def _generate_external_signals(
    self,
    signals: List[ExternalSignal]
) -> Dict[str, Any]:
    """ì™¸ë¶€ ì‹ í˜¸ ë¶„ì„ (ë©”íƒ€ë°ì´í„° ì™„ì „ ë³´ì¡´)"""

    # ì‹ í˜¸ë¥¼ Tierë³„ë¡œ ê·¸ë£¹í™”
    tier_groups = {1: [], 2: [], 3: [], 4: []}
    for signal in signals:
        tier_groups[signal.tier].append(signal)

    # LLM í”„ë¡¬í”„íŠ¸ìš© ìƒì„¸ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
    signal_context = []
    for tier, tier_signals in tier_groups.items():
        tier_name = {
            1: "ë°”ì´ëŸ´ ì‹ í˜¸ (TikTok/Instagram)",
            2: "ê²€ì¦/ë¦¬ë·° (YouTube/Reddit)",
            3: "ê¶Œìœ„ ìˆëŠ” ì¶œì²˜ (ë‰´ìŠ¤/ì „ë¬¸ì§€)",
            4: "PR/ì‹¤ì‹œê°„ (Twitter/ë³´ë„ìë£Œ)"
        }.get(tier, "ê¸°íƒ€")

        for s in tier_signals:
            signal_context.append({
                "tier": tier,
                "tier_name": tier_name,
                "source": s.source,
                "title": s.title,
                "date": s.date,
                "reliability": s.reliability_score,  # ì‹ ë¢°ë„ ë³´ì¡´
                "relevance": s.relevance_score,      # ê´€ë ¨ì„± ë³´ì¡´
                "content_preview": s.content[:200] if s.content else "",
                "url": s.url
            })

    # LLM í”„ë¡¬í”„íŠ¸ì— ì „ì²´ ë©”íƒ€ë°ì´í„° ì „ë‹¬
    prompt = f"""
## ì™¸ë¶€ ì‹ í˜¸ ë¶„ì„

ì´ {len(signals)}ê°œì˜ ì™¸ë¶€ ì‹ í˜¸ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.

### ì‹ í˜¸ ìƒì„¸ (ì‹ ë¢°ë„/ê´€ë ¨ì„± ì ìˆ˜ í¬í•¨)

{json.dumps(signal_context, ensure_ascii=False, indent=2)}

### ë¶„ì„ ìš”ì²­
1. Tierë³„ í•µì‹¬ íŠ¸ë Œë“œ ìš”ì•½
2. ì‹ ë¢°ë„ ë†’ì€ ì¶œì²˜(0.8 ì´ìƒ)ì˜ í•µì‹¬ ë©”ì‹œì§€
3. LANEIGEì™€ ì§ì ‘ ê´€ë ¨ëœ ì‹ í˜¸ ê°•ì¡°
4. ì‹œì¥ ë™í–¥ê³¼ ì—°ê²°ë˜ëŠ” ì¸ì‚¬ì´íŠ¸
"""

    return {
        "signal_count": len(signals),
        "tier_breakdown": {k: len(v) for k, v in tier_groups.items()},
        "prompt_context": prompt,
        "raw_signals": signal_context  # ì›ë³¸ ë³´ì¡´
    }
```

#### 3.2.2 ReferenceTracker ìë™ ì—°ë™

**íŒŒì¼**: `src/tools/reference_tracker.py`

**ì¶”ê°€í•  ë©”ì„œë“œ**:
```python
def add_external_signals(
    self,
    signals: List[ExternalSignal],
    auto_categorize: bool = True
) -> int:
    """
    ì™¸ë¶€ ì‹ í˜¸ë¥¼ ì°¸ê³ ìë£Œì— ìë™ ì¶”ê°€

    Args:
        signals: ExternalSignal ê°ì²´ ë¦¬ìŠ¤íŠ¸
        auto_categorize: ìë™ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ì—¬ë¶€

    Returns:
        ì¶”ê°€ëœ ì°¸ê³ ìë£Œ ìˆ˜
    """
    added_count = 0

    # Tier â†’ ReferenceType ë§¤í•‘
    tier_to_type = {
        1: ReferenceType.SOCIAL,   # TikTok/Instagram
        2: ReferenceType.SOCIAL,   # YouTube/Reddit
        3: ReferenceType.ARTICLE,  # News/ì „ë¬¸ì§€
        4: ReferenceType.ARTICLE   # PR/Twitter
    }

    for signal in signals:
        ref_type = tier_to_type.get(signal.tier, ReferenceType.ARTICLE)

        # ì¤‘ë³µ ì²´í¬
        if self._is_duplicate(signal.url):
            continue

        reference = Reference(
            type=ref_type,
            title=signal.title,
            source=signal.source,
            url=signal.url,
            date=signal.date,
            metadata={
                "tier": signal.tier,
                "reliability_score": signal.reliability_score,
                "relevance_score": signal.relevance_score
            }
        )

        self.add_reference(reference)
        added_count += 1

    return added_count

def _is_duplicate(self, url: str) -> bool:
    """URL ê¸°ë°˜ ì¤‘ë³µ ì²´í¬"""
    for ref in self.references:
        if ref.url == url:
            return True
    return False
```

---

### 3.3 Phase 3: ê³µê³µë°ì´í„° API ì—°ë™

#### 3.3.1 PublicDataCollector ì™„ì„±

**íŒŒì¼**: `src/tools/public_data_collector.py`

**í˜„ì¬ ìƒíƒœ**: í”„ë ˆì„ì›Œí¬ë§Œ ì¡´ì¬, ì‹¤ì œ API í˜¸ì¶œ ì—†ìŒ

**êµ¬í˜„ ê³„íš**:

```python
class PublicDataCollector:
    """
    í•œêµ­ ê³µê³µë°ì´í„° API ì—°ë™

    ì§€ì› API:
    1. ê´€ì„¸ì²­ ìˆ˜ì¶œì… í†µê³„ (í™”ì¥í’ˆ HSì½”ë“œ: 3304)
    2. ì‹ì•½ì²˜ í™”ì¥í’ˆ ì›ë£Œ/ì œí’ˆ ë°ì´í„°
    3. KOSIS ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜
    """

    async def fetch_customs_export_data(
        self,
        hs_code: str = "3304",  # í™”ì¥í’ˆ
        country: str = "US",
        start_date: str = None,
        end_date: str = None
    ) -> Dict[str, Any]:
        """
        ê´€ì„¸ì²­ í™”ì¥í’ˆ ìˆ˜ì¶œ ë°ì´í„° ì¡°íšŒ

        Returns:
            {
                "period": "2025-12",
                "export_amount_usd": 1234567890,
                "yoy_change": 12.5,
                "top_items": [
                    {"name": "ë¦½ìŠ¤í‹±", "amount": ...},
                    ...
                ]
            }
        """
        api_key = os.getenv("DATA_GO_KR_API_KEY")
        if not api_key:
            raise ValueError("DATA_GO_KR_API_KEY í™˜ê²½ë³€ìˆ˜ í•„ìš”")

        # ì‹¤ì œ API í˜¸ì¶œ êµ¬í˜„
        url = "http://apis.data.go.kr/1220000/tradestatistics"
        params = {
            "serviceKey": api_key,
            "searchStDt": start_date,
            "searchEdDt": end_date,
            "hsCode": hs_code,
            "cntyCd": country
        }

        async with httpx.AsyncClient() as client:
            response = await client.get(url, params=params)
            data = response.json()

        return self._parse_customs_data(data)

    async def fetch_mfds_cosmetics_data(
        self,
        category: str = "ê¸°ëŠ¥ì„±í™”ì¥í’ˆ"
    ) -> Dict[str, Any]:
        """
        ì‹ì•½ì²˜ í™”ì¥í’ˆ í—ˆê°€/ë“±ë¡ ë°ì´í„°

        Returns:
            ìµœê·¼ í—ˆê°€ëœ í™”ì¥í’ˆ ë¦¬ìŠ¤íŠ¸, ì„±ë¶„ íŠ¸ë Œë“œ ë“±
        """
        api_key = os.getenv("MFDS_API_KEY")
        # ... êµ¬í˜„

    async def fetch_consumer_price_index(
        self,
        item_code: str = "í™”ì¥í’ˆ"
    ) -> Dict[str, Any]:
        """
        KOSIS ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜ (í™”ì¥í’ˆ)

        Returns:
            ë¬¼ê°€ì§€ìˆ˜ ì¶”ì´, ì „ë…„ ëŒ€ë¹„ ë³€í™”ìœ¨
        """
        api_key = os.getenv("KOSIS_API_KEY")
        # ... êµ¬í˜„
```

---

### 3.4 Phase 4: YouTube Collector í†µí•©

#### 3.4.1 Market Intelligence Engine ì—°ë™

**íŒŒì¼**: `src/tools/market_intelligence.py`

```python
class MarketIntelligenceEngine:
    """4ê³„ì¸µ ì‹œì¥ ì¸í…”ë¦¬ì „ìŠ¤ ì—”ì§„"""

    async def collect_all_signals(
        self,
        brands: List[str],
        period_days: int = 7
    ) -> Dict[str, Any]:
        """ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ì‹ í˜¸ ìˆ˜ì§‘"""

        results = {
            "tavily_news": [],
            "youtube_trends": [],
            "reddit_discussions": [],
            "rss_articles": [],
            "public_data": {}
        }

        # 1. Tavily ë‰´ìŠ¤ ê²€ìƒ‰
        if self.external_collector.tavily_client:
            results["tavily_news"] = await self.external_collector.fetch_tavily_news(
                brands=brands,
                days=period_days
            )

        # 2. YouTube íŠ¸ë Œë“œ (ê¸°ì¡´ youtube_collector.py í™œìš©)
        if self.youtube_collector:
            results["youtube_trends"] = await self.youtube_collector.search_beauty_videos(
                queries=[f"{brand} review" for brand in brands],
                max_results=20
            )

        # 3. Reddit í† ë¡ 
        results["reddit_discussions"] = await self.external_collector.fetch_reddit_trends(
            subreddits=["SkincareAddiction", "AsianBeauty", "MakeupAddiction"]
        )

        # 4. RSS ê¸°ì‚¬
        results["rss_articles"] = await self.external_collector.fetch_rss_articles(
            keywords=brands
        )

        # 5. ê³µê³µë°ì´í„° (í•œêµ­ ìˆ˜ì¶œ í†µê³„)
        if self.public_data_collector:
            results["public_data"] = await self.public_data_collector.fetch_customs_export_data()

        return results
```

---

## 4. íŒŒì¼ ë³€ê²½ ìš”ì•½

| íŒŒì¼ | ì‘ì—… | ì„¤ëª… |
|------|------|------|
| `src/tools/tavily_search.py` | **CREATE** | Tavily API í´ë¼ì´ì–¸íŠ¸ |
| `src/tools/external_signal_collector.py` | **MODIFY** | Tavily í†µí•©, ë©”íƒ€ë°ì´í„° ë³´ì¡´ |
| `src/agents/period_insight_agent.py` | **MODIFY** | LLM í”„ë¡¬í”„íŠ¸ì— ì „ì²´ ë©”íƒ€ë°ì´í„° ì „ë‹¬ |
| `src/tools/reference_tracker.py` | **MODIFY** | `add_external_signals()` ë©”ì„œë“œ ì¶”ê°€ |
| `src/tools/public_data_collector.py` | **MODIFY** | ì‹¤ì œ API í˜¸ì¶œ êµ¬í˜„ |
| `src/tools/market_intelligence.py` | **MODIFY** | YouTube Collector í†µí•© |
| `config/public_apis.json` | **MODIFY** | Tavily enabled: true ë³€ê²½ |

---

## 5. í™˜ê²½ë³€ìˆ˜ ìš”êµ¬ì‚¬í•­

```bash
# .env íŒŒì¼ í•„ìˆ˜ ì„¤ì •

# Tavily API (ë‰´ìŠ¤ ê²€ìƒ‰)
TAVILY_API_KEY=tvly-...

# Apify API (Amazon/YouTube ìŠ¤í¬ë˜í•‘)
APIFY_API_TOKEN=apify_api_...

# í•œêµ­ ê³µê³µë°ì´í„° í¬í„¸
DATA_GO_KR_API_KEY=...

# ì‹ì•½ì²˜ API (ì„ íƒ)
MFDS_API_KEY=...

# KOSIS API (ì„ íƒ)
KOSIS_API_KEY=...
```

---

## 6. í…ŒìŠ¤íŠ¸ ê³„íš

### 6.1 ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

```bash
# Tavily API í…ŒìŠ¤íŠ¸
python -m pytest tests/test_tavily_search.py -v

# ì™¸ë¶€ ì‹ í˜¸ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
python -m pytest tests/test_external_signal_pipeline.py -v

# ì°¸ê³ ìë£Œ ìë™ ë“±ë¡ í…ŒìŠ¤íŠ¸
python -m pytest tests/test_reference_tracker_integration.py -v
```

### 6.2 í†µí•© í…ŒìŠ¤íŠ¸

```bash
# ì „ì²´ ë³´ê³ ì„œ ìƒì„± í…ŒìŠ¤íŠ¸
curl -X POST http://localhost:8001/api/export/analyst-report \
  -H "Content-Type: application/json" \
  -d '{
    "start_date": "2026-01-14",
    "end_date": "2026-01-25",
    "include_external_signals": true
  }'
```

### 6.3 ê²€ì¦ í•­ëª©

- [ ] Tavily ê²€ìƒ‰ ê²°ê³¼ê°€ ë³´ê³ ì„œ Section 5ì— ë°˜ì˜ë˜ëŠ”ì§€
- [ ] ì™¸ë¶€ ì‹ í˜¸ ì‹ ë¢°ë„/ê´€ë ¨ì„± ì ìˆ˜ê°€ ë³´ê³ ì„œì— í‘œì‹œë˜ëŠ”ì§€
- [ ] ReferenceTrackerê°€ ì™¸ë¶€ ì‹ í˜¸ë¥¼ Section 8ì— ìë™ ì¶”ê°€í•˜ëŠ”ì§€
- [ ] YouTube íŠ¸ë Œë“œê°€ ë¶„ì„ì— í¬í•¨ë˜ëŠ”ì§€
- [ ] ê³µê³µë°ì´í„° (ìˆ˜ì¶œ í†µê³„)ê°€ ì‹œì¥ ë¶„ì„ì— ë°˜ì˜ë˜ëŠ”ì§€

---

## 7. ì˜ˆìƒ ì¼ì •

| Phase | ì˜ˆìƒ ì†Œìš” | ìš°ì„ ìˆœìœ„ |
|-------|----------|----------|
| Phase 1: Tavily API í†µí•© | 2-3ì‹œê°„ | ğŸ”´ ë†’ìŒ |
| Phase 2: íŒŒì´í”„ë¼ì¸ ê°•í™” | 2-3ì‹œê°„ | ğŸ”´ ë†’ìŒ |
| Phase 3: ê³µê³µë°ì´í„° ì—°ë™ | 3-4ì‹œê°„ | ğŸŸ¡ ì¤‘ê°„ |
| Phase 4: YouTube í†µí•© | 1-2ì‹œê°„ | ğŸŸ¡ ì¤‘ê°„ |
| í…ŒìŠ¤íŠ¸ ë° ê²€ì¦ | 2ì‹œê°„ | ğŸ”´ ë†’ìŒ |

**ì´ ì˜ˆìƒ ì†Œìš”**: 10-14ì‹œê°„

---

## 8. ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Market Intelligence Engine                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚ Tavily API  â”‚  â”‚  Apify API  â”‚  â”‚ ê³µê³µë°ì´í„°   â”‚             â”‚
â”‚  â”‚ (ë‰´ìŠ¤ ê²€ìƒ‰)  â”‚  â”‚ (Amazon/YT) â”‚  â”‚ (ê´€ì„¸ì²­ ë“±)  â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚         â”‚                â”‚                â”‚                     â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                          â–¼                                      â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚              â”‚ ExternalSignalCollectorâ”‚                         â”‚
â”‚              â”‚ (ì‹ í˜¸ ìˆ˜ì§‘ + ì •ê·œí™”)    â”‚                         â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                          â”‚                                      â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚         â–¼                â–¼                â–¼                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚ RSS Feeds   â”‚  â”‚ Reddit API  â”‚  â”‚ Manual Inputâ”‚             â”‚
â”‚  â”‚ (23ê°œ ì†ŒìŠ¤)  â”‚  â”‚ (JSON API)  â”‚  â”‚ (TikTok ë“±) â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚              â”‚ PeriodInsightAgent    â”‚                         â”‚
â”‚              â”‚ (LLM ì¸ì‚¬ì´íŠ¸ ìƒì„±)    â”‚â—„â”€â”€â”€â”€ ë©”íƒ€ë°ì´í„° ì™„ì „ ë³´ì¡´  â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                          â”‚                                      â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚              â”‚ ReferenceTracker      â”‚â—„â”€â”€â”€â”€ ìë™ ì°¸ê³ ìë£Œ ë“±ë¡   â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                          â”‚                                      â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚              â”‚ DOCX Report Generator â”‚                         â”‚
â”‚              â”‚ (ì• ë„ë¦¬ìŠ¤íŠ¸ ë³´ê³ ì„œ)    â”‚                         â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 9. ë¦¬ìŠ¤í¬ ë° ëŒ€ì‘

| ë¦¬ìŠ¤í¬ | ì˜í–¥ | ëŒ€ì‘ ë°©ì•ˆ |
|--------|------|----------|
| Tavily API ë¹„ìš© ì´ˆê³¼ | ì¤‘ê°„ | ì¼ì¼ ì¿¼ë¦¬ ì œí•œ (50íšŒ), ìºì‹± ì ìš© |
| ê³µê³µë°ì´í„° API ë¶ˆì•ˆì • | ë‚®ìŒ | Fallback ë°ì´í„° ì¤€ë¹„, ì¬ì‹œë„ ë¡œì§ |
| API ì‘ë‹µ ì§€ì—° | ì¤‘ê°„ | íƒ€ì„ì•„ì›ƒ ì„¤ì •, ë¹„ë™ê¸° ë³‘ë ¬ ì²˜ë¦¬ |
| ë°ì´í„° í’ˆì§ˆ ì´ìŠˆ | ì¤‘ê°„ | ì‹ ë¢°ë„ ê¸°ë°˜ í•„í„°ë§, ê²€ì¦ ë¡œì§ |

---

## 10. ì„±ê³µ ê¸°ì¤€

1. **Tavily ê²€ìƒ‰ ë™ì‘**: ë¸Œëœë“œëª…ìœ¼ë¡œ ê²€ìƒ‰ ì‹œ ê´€ë ¨ ë‰´ìŠ¤ 5ê°œ ì´ìƒ ë°˜í™˜
2. **ë©”íƒ€ë°ì´í„° ë³´ì¡´**: ë³´ê³ ì„œì— ì‹ ë¢°ë„/ê´€ë ¨ì„± ì ìˆ˜ í‘œì‹œ
3. **ì°¸ê³ ìë£Œ ìë™í™”**: ì™¸ë¶€ ì‹ í˜¸ â†’ Section 8 ìë™ ì¶”ê°€
4. **ë³´ê³ ì„œ í’ˆì§ˆ**: ì™¸ë¶€ ì‹ í˜¸ ì„¹ì…˜ì´ Tierë³„ë¡œ êµ¬ì¡°í™”ë˜ì–´ í‘œì‹œ
5. **ì‘ë‹µ ì‹œê°„**: ì „ì²´ ë³´ê³ ì„œ ìƒì„± 3ë¶„ ì´ë‚´

---

*ì´ ê³„íšì€ ì‚¬ìš©ìì˜ ìš”ì²­ì— ë”°ë¼ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤. êµ¬í˜„ ì‹œì‘ ì „ ì¶”ê°€ ìš”êµ¬ì‚¬í•­ì´ ìˆìœ¼ë©´ ë§ì”€í•´ì£¼ì„¸ìš”.*
