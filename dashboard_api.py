"""
Dashboard API Server
====================
ëŒ€ì‹œë³´ë“œìš© FastAPI ë°±ì—”ë“œ ì„œë²„ (ë©”ì¸ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸)

## í•µì‹¬ ê¸°ëŠ¥
- ì±—ë´‡ API (ChatGPT + RAG + Ontology ì—°ë™)
- DOCX ì¸ì‚¬ì´íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±
- ëŒ€í™” ë©”ëª¨ë¦¬ ì§€ì› (ì„¸ì…˜ë³„ TTL ê¸°ë°˜)
- Audit Trail ë¡œê¹…

## ì•„í‚¤í…ì²˜ íë¦„
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           FastAPI Server                                â”‚
â”‚   dashboard_api.py (PORT 8001)                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  /api/chat â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º HybridChatbotAgent â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º LLM (GPT-4.1)  â”‚
â”‚                                   â”‚                                     â”‚
â”‚                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚                         â–¼                   â–¼                          â”‚
â”‚                  KnowledgeGraph      DocumentRetriever                  â”‚
â”‚                  (ì˜¨í†¨ë¡œì§€)          (RAG ê°€ì´ë“œë¼ì¸)                   â”‚
â”‚                                                                         â”‚
â”‚  /api/crawl/start â”€â”€â”€â”€â–º UnifiedBrain â”€â”€â”€â”€â–º AmazonScraper               â”‚
â”‚                              â”‚              (Playwright)                â”‚
â”‚                              â–¼                                          â”‚
â”‚                        MetricCalculator                                 â”‚
â”‚                              â”‚                                          â”‚
â”‚                              â–¼                                          â”‚
â”‚                       SheetsWriter / SQLite                             â”‚
â”‚                                                                         â”‚
â”‚  /api/data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º dashboard_data.json (ìºì‹œëœ ë°ì´í„°)            â”‚
â”‚                                                                         â”‚
â”‚  /dashboard â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º amore_unified_dashboard_v4.html                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ì£¼ìš” ì—”ë“œí¬ì¸íŠ¸
- GET  /           : í—¬ìŠ¤ì²´í¬
- GET  /api/data   : ëŒ€ì‹œë³´ë“œ ë°ì´í„° JSON
- POST /api/chat   : ì±—ë´‡ v1 (RAG)
- POST /api/v2/chat: ì±—ë´‡ v2 (Unified Brain)
- POST /api/v3/chat: ì±—ë´‡ v3 (Simple Chat)
- POST /api/crawl/start: í¬ë¡¤ë§ ì‹œì‘ (API Key í•„ìš”)
- GET  /dashboard  : ëŒ€ì‹œë³´ë“œ UI

## í™˜ê²½ ë³€ìˆ˜
- OPENAI_API_KEY: OpenAI API í‚¤ (í•„ìˆ˜)
- API_KEY: ë³´í˜¸ëœ ì—”ë“œí¬ì¸íŠ¸ìš© ì¸ì¦í‚¤
- AUTO_START_SCHEDULER: ì„œë²„ ì‹œì‘ ì‹œ ìŠ¤ì¼€ì¤„ëŸ¬ ìë™ ì‹œì‘ (default: true)
"""

import asyncio
import json
import logging
import os
from datetime import datetime
from io import BytesIO
from pathlib import Path
from typing import Any

from docx import Document
from docx.enum.table import WD_TABLE_ALIGNMENT
from docx.enum.text import WD_ALIGN_PARAGRAPH
from docx.shared import Pt
from dotenv import load_dotenv
from fastapi import Depends, HTTPException, Request
from fastapi.responses import FileResponse, JSONResponse, StreamingResponse
from litellm import acompletion

# App Factory (ë¯¸ë“¤ì›¨ì–´, ë¼ìš°í„°, ì •ì  íŒŒì¼ ë“±ë¡ í¬í•¨)
from src.api.app_factory import create_app

# ê³µí†µ ì˜ì¡´ì„± (ì¸ì¦, ì„¸ì…˜, í—¬í¼ ë“±)
from src.api.dependencies import (
    add_to_memory,
    build_data_context,
    conversation_memory,
    generate_dynamic_suggestions,
    get_base_url,
    get_conversation_history,
    get_rag_context,
    limiter,
    load_dashboard_data,
    log_chat_interaction,
    rag_router,
    verify_api_key,
)

# Pydantic ëª¨ë¸
from src.api.models import (
    AlertSendRequest,
    AlertSettingsRequest,
    BrainChatRequest,
    BrainChatResponse,
    ChatRequest,
    ChatResponse,
    DealsRequest,
    DealsResponse,
    ExportRequest,
    SubscribeRequest,
    UpdateAlertSettingsRequest,
)

# Core ëª¨ë“ˆ (ì§ì ‘ ì˜ì¡´)
from src.core.brain import get_initialized_brain
from src.core.crawl_manager import get_crawl_manager
from src.core.state_manager import get_state_manager
from src.rag.router import QueryType
from src.tools.storage.sqlite_storage import get_sqlite_storage

load_dotenv()

logger = logging.getLogger(__name__)

# App ìƒì„± (app_factoryì—ì„œ ë¯¸ë“¤ì›¨ì–´, ë¼ìš°í„°, ì •ì  íŒŒì¼ ë“±ë¡ ì™„ë£Œ)
app = create_app()


# ê¸€ë¡œë²Œ ì˜ˆì™¸ í•¸ë“¤ëŸ¬ - ì—ëŸ¬ ë°œìƒ ì‹œ Telegram ì•Œë¦¼
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    """ëª¨ë“  ì˜ˆì™¸ë¥¼ ì¡ì•„ì„œ Telegram ì•Œë¦¼ ì „ì†¡"""

    error_detail = f"{type(exc).__name__}: {str(exc)[:200]}"
    endpoint = f"{request.method} {request.url.path}"

    # ë¡œê¹…
    logger.error(f"Unhandled exception at {endpoint}: {error_detail}")

    # Telegram ì•Œë¦¼ (ë¹„ë™ê¸°, ì‹¤íŒ¨í•´ë„ ë¬´ì‹œ)
    try:
        from src.tools.notifications.telegram_bot import notify_error

        asyncio.create_task(notify_error(exc, context=f"API: {endpoint}"))
    except Exception:
        pass  # Telegram ì•Œë¦¼ ì‹¤íŒ¨ëŠ” ë¬´ì‹œ

    # í´ë¼ì´ì–¸íŠ¸ì—ê²ŒëŠ” ì¼ë°˜ ì—ëŸ¬ ì‘ë‹µ
    return JSONResponse(
        status_code=500,
        content={"error": "Internal server error", "detail": error_detail},
    )


# ============= ì„œë²„ ì‹œì‘ ì‹œ ìë™ ìŠ¤ì¼€ì¤„ëŸ¬ =============

# Railway ë°°í¬ ì‹œ healthcheck íƒ€ì„ì•„ì›ƒ ë°©ì§€: ê¸°ë³¸ê°’ false
# ë¡œì»¬ ê°œë°œ ì‹œ AUTO_START_SCHEDULER=true ë¡œ ì„¤ì •í•˜ë©´ ìŠ¤ì¼€ì¤„ëŸ¬ ìë™ ì‹œì‘
AUTO_START_SCHEDULER = os.getenv("AUTO_START_SCHEDULER", "false").lower() == "true"


@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ì„¤ì • ê²€ì¦, ìë™ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ ë° ì¦‰ì‹œ í¬ë¡¤ë§ ì²´í¬

    âš ï¸ ì¤‘ìš”: í¬ë¡¤ë§ì€ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰í•˜ì—¬ healthcheck íƒ€ì„ì•„ì›ƒ ë°©ì§€
    """
    # 0. ì„¤ì • ê²€ì¦ (í•„ìˆ˜ ì„¤ì • ëˆ„ë½ ì‹œ ê²½ê³ , ì„œë²„ëŠ” ê³„ì† ì‹œì‘)
    try:
        from src.infrastructure.config.config_manager import AppConfig

        config = AppConfig.from_env_validated(fail_fast=False)
        logging.info(
            f"ì„¤ì • ê²€ì¦ ì™„ë£Œ (port={config.port}, scheduler={config.auto_start_scheduler})"
        )
    except Exception as e:
        logging.error(f"ì„¤ì • ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")

    # 1. í¬ë¡¤ë§ í•„ìš” ì—¬ë¶€ ì²´í¬ í›„ ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ (ë¹„ë¸”ë¡œí‚¹)
    try:
        crawl_manager = await get_crawl_manager()
        if crawl_manager.needs_crawl():
            logging.info(
                f"ì„œë²„ ì‹œì‘: ì˜¤ëŠ˜({crawl_manager.get_kst_today()}) ë°ì´í„° ì—†ìŒ â†’ í¬ë¡¤ë§ ë°±ê·¸ë¼ìš´ë“œ ì‹œì‘"
            )
            # âš ï¸ await ëŒ€ì‹  create_taskë¡œ ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ (healthcheck ë¸”ë¡œí‚¹ ë°©ì§€)
            asyncio.create_task(crawl_manager.start_crawl())
        else:
            logging.info(
                f"ì„œë²„ ì‹œì‘: ì˜¤ëŠ˜ ë°ì´í„° ìˆìŒ ë˜ëŠ” í¬ë¡¤ë§ ì¤‘ (data_date={crawl_manager.get_data_date()})"
            )
    except Exception as e:
        logging.error(f"ì„œë²„ ì‹œì‘ í¬ë¡¤ë§ ì²´í¬ ì‹¤íŒ¨: {e}")

    # 2. ììœ¨ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ (ë§¤ì¼ 06:00 ì •ê¸° í¬ë¡¤ë§ìš©)
    if AUTO_START_SCHEDULER:
        try:
            brain = await get_initialized_brain()
            await brain.start_scheduler()
            logging.info("ììœ¨ ìŠ¤ì¼€ì¤„ëŸ¬ ìë™ ì‹œì‘ ì™„ë£Œ (ë§¤ì¼ í•œêµ­ì‹œê°„ 06:00 í¬ë¡¤ë§)")
        except Exception as e:
            logging.error(f"ììœ¨ ìŠ¤ì¼€ì¤„ëŸ¬ ìë™ ì‹œì‘ ì‹¤íŒ¨: {e}")

    # 3. Export Job Queue Worker ì‹œì‘ (ë¹„ë™ê¸° ë‚´ë³´ë‚´ê¸°ìš©)
    try:
        from src.tools.exporters.export_handlers import register_all_handlers
        from src.tools.utilities.job_queue import get_job_queue

        queue = get_job_queue()
        await queue.initialize()
        register_all_handlers(queue)
        await queue.start_worker()
        logging.info("Export Job Queue Worker ì‹œì‘ ì™„ë£Œ")
    except Exception as e:
        logging.error(f"Export Job Queue Worker ì‹œì‘ ì‹¤íŒ¨: {e}")

    # 4. Telegram Admin Bot ì•Œë¦¼ (ì„œë²„ ì‹œì‘)
    try:
        from src.tools.notifications.telegram_bot import get_bot

        bot = get_bot()
        if bot.is_enabled():
            await bot.send_alert("ğŸš€ ì„œë²„ ì‹œì‘ë¨", level="info")
            logging.info("Telegram Admin Bot í™œì„±í™”ë¨")
    except Exception as e:
        logging.debug(f"Telegram Bot ì•Œë¦¼ ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")


# ============= API Endpoints (helpers imported from src.api.dependencies) =============


@app.get("/api/data")
async def get_data():
    """ëŒ€ì‹œë³´ë“œ ë°ì´í„° ì¡°íšŒ"""
    data = load_dashboard_data()
    if not data:
        raise HTTPException(status_code=404, detail="Dashboard data not found")
    return data


@app.post("/api/chat", response_model=ChatResponse, dependencies=[Depends(verify_api_key)])
@limiter.limit("10/minute")  # ë¶„ë‹¹ 10íšŒ ì œí•œ (ë³´ì•ˆ ê°•í™”)
async def chat(request: Request, body: ChatRequest):
    """
    ChatGPT + RAG + Ontology í†µí•© ì±—ë´‡ API

    1. ì§ˆë¬¸ ë¶„ì„ (RAGRouter)
    2. ì—”í‹°í‹° ì¶”ì¶œ (Ontology ê¸°ë°˜)
    3. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ (RAG)
    4. ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    5. ëŒ€í™” ê¸°ë¡ ì°¸ì¡°
    6. LLM ì‘ë‹µ ìƒì„±
    7. Audit Trail ë¡œê¹…
    """
    import time

    start_time = time.time()

    message = body.message.strip()
    session_id = body.session_id or "default"

    if not message:
        raise HTTPException(status_code=400, detail="Message is required")

    # 1. ì§ˆë¬¸ ë¶„ë¥˜ (RAGRouter ì‚¬ìš©)
    route_result = rag_router.route(message)
    query_type = route_result["query_type"]
    confidence = route_result["confidence"]

    # 2. ì—”í‹°í‹° ì¶”ì¶œ (Ontology ê¸°ë°˜)
    entities = rag_router.extract_entities(message)

    # 3. ëª…í™•í™” í•„ìš” ì—¬ë¶€ í™•ì¸
    clarification = rag_router.needs_clarification(route_result, entities)
    if clarification and confidence < 0.5:
        # ëª…í™•í™” ìš”ì²­
        add_to_memory(session_id, "user", message)
        add_to_memory(session_id, "assistant", clarification)

        return ChatResponse(
            response=clarification,
            query_type=query_type.value if hasattr(query_type, "value") else str(query_type),
            confidence=confidence,
            sources=[],
            suggestions=[
                "ì˜ˆ, ì „ì²´ ë¸Œëœë“œ ë¶„ì„í•´ì£¼ì„¸ìš”",
                "LANEIGEë§Œ ë¶„ì„í•´ì£¼ì„¸ìš”",
                "Lip Care ì¹´í…Œê³ ë¦¬ë§Œ",
            ],
            entities=entities,
        )

    # 4. RAG ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
    rag_context, sources = await get_rag_context(message, query_type)

    # 5. ë°ì´í„° ë¡œë“œ ë° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    data = load_dashboard_data()
    data_context = build_data_context(data, query_type, entities)

    # 6. ëŒ€í™” ê¸°ë¡ ì¡°íšŒ
    conversation_history = get_conversation_history(session_id)

    # 7. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    system_prompt = """ë‹¹ì‹ ì€ AMORE Pacificì˜ LANEIGE ë¸Œëœë“œ Amazon ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì—­í• :
- Amazon US ë² ìŠ¤íŠ¸ì…€ëŸ¬ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì¸ì‚¬ì´íŠ¸ ì œê³µ
- LANEIGE ë¸Œëœë“œì˜ ì‹œì¥ í¬ì§€ì…˜ ë¶„ì„
- ê²½ìŸì‚¬ ëŒ€ë¹„ ì „ëµì  ê¶Œê³  ì œê³µ
- ì§€í‘œ ì •ì˜ ë° í•´ì„ ê°€ì´ë“œ ì œê³µ

Ontology ì—”í‹°í‹° ì´í•´:
- Brand: ë¸Œëœë“œ ì •ë³´ (LANEIGE, ê²½ìŸì‚¬ ë“±)
- Product: ì œí’ˆ ì •ë³´ (ASIN, ìˆœìœ„, í‰ì , ê°€ê²© ë“±)
- Category: ì¹´í…Œê³ ë¦¬ (Lip Care, Skin Care ë“±)
- BrandMetrics: SoS, í‰ê· ìˆœìœ„, ì œí’ˆìˆ˜ ë“±
- ProductMetrics: ìˆœìœ„ë³€ë™ì„±, ì—°ì†ì²´ë¥˜ì¼, í‰ì ì¶”ì„¸ ë“±
- MarketMetrics: HHI(ì‹œì¥ì§‘ì¤‘ë„), êµì²´ìœ¨ ë“±

ì‘ë‹µ ê°€ì´ë“œë¼ì¸:
1. ë°ì´í„°ì— ê¸°ë°˜í•œ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ ì¸ìš©
2. RAG ë¬¸ì„œì˜ ì •ì˜/í•´ì„ ê¸°ì¤€ í™œìš©
3. ì´ì „ ëŒ€í™” ë§¥ë½ ê³ ë ¤
4. ê°„ê²°í•˜ê³  ì•¡ì…˜ ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸ ì œê³µ
5. ë¶ˆí™•ì‹¤í•œ ê²½ìš° ëª…í™•íˆ ë°í ê²ƒ
6. ë‹¨ì •ì  í‘œí˜„ í”¼í•˜ê¸°
7. í•œêµ­ì–´ë¡œ ì‘ë‹µ

ì§ˆë¬¸ ìœ í˜•ë³„ ì‘ë‹µ ìŠ¤íƒ€ì¼:
- ì •ì˜(DEFINITION): ì§€í‘œì˜ ì •ì˜, ì‚°ì¶œì‹, ì˜ë¯¸ë¥¼ ì„¤ëª…
- í•´ì„(INTERPRETATION): ìˆ˜ì¹˜ì˜ ì˜ë¯¸, ì¢‹ê³  ë‚˜ì¨ì˜ ê¸°ì¤€ ì„¤ëª…
- ì¡°í•©(COMBINATION): ì—¬ëŸ¬ ì§€í‘œë¥¼ í•¨ê»˜ í•´ì„, ì‹œë‚˜ë¦¬ì˜¤ë³„ ì•¡ì…˜ ì œì•ˆ
- ë°ì´í„°ì¡°íšŒ(DATA_QUERY): í˜„ì¬ ìˆ˜ì¹˜ì™€ ë³€ë™ í˜„í™© ì•ˆë‚´
- ë¶„ì„(ANALYSIS): ì¢…í•© ë¶„ì„ê³¼ ì „ëµì  ê¶Œê³  ì œê³µ
"""

    # 8. ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    user_prompt = f"""## ì‚¬ìš©ì ì§ˆë¬¸
{message}

## ì§ˆë¬¸ ìœ í˜•
{query_type.value if hasattr(query_type, 'value') else str(query_type)} (ì‹ ë¢°ë„: {confidence:.1%})

## ì¶”ì¶œëœ ì—”í‹°í‹°
- ë¸Œëœë“œ: {', '.join(entities.get('brands', [])) or 'ì—†ìŒ'}
- ì¹´í…Œê³ ë¦¬: {', '.join(entities.get('categories', [])) or 'ì—†ìŒ'}
- ì§€í‘œ: {', '.join(entities.get('indicators', [])) or 'ì—†ìŒ'}
- ê¸°ê°„: {entities.get('time_range') or 'ì—†ìŒ'}

## RAG ì°¸ì¡° ë¬¸ì„œ
{rag_context if rag_context else 'ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ'}

## í˜„ì¬ ë°ì´í„°
{data_context}

## ì´ì „ ëŒ€í™”
{conversation_history if conversation_history else 'ì´ì „ ëŒ€í™” ì—†ìŒ'}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
- ì§ˆë¬¸ ìœ í˜•ì— ë§ëŠ” ì‘ë‹µ ìŠ¤íƒ€ì¼ì„ ì‚¬ìš©í•˜ì„¸ìš”.
- RAG ë¬¸ì„œì— ê´€ë ¨ ì •ì˜/í•´ì„ì´ ìˆìœ¼ë©´ ì¸ìš©í•˜ì„¸ìš”.
- ì´ì „ ëŒ€í™” ë§¥ë½ì´ ìˆìœ¼ë©´ ê³ ë ¤í•˜ì„¸ìš”.
"""

    try:
        response = await acompletion(
            model="gpt-4.1-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.3,
            max_tokens=1000,
        )

        answer = response.choices[0].message.content

        # 9. ëŒ€í™” ë©”ëª¨ë¦¬ì— ì €ì¥
        add_to_memory(session_id, "user", message)
        add_to_memory(session_id, "assistant", answer)

        # 10. ë™ì  í›„ì† ì§ˆë¬¸ ì œì•ˆ (v2 - ê°œì„  ë²„ì „)
        suggestions = generate_dynamic_suggestions(query_type, entities, answer, message)

        # 11. Audit Trail ë¡œê¹…
        response_time_ms = (time.time() - start_time) * 1000
        log_chat_interaction(
            session_id=session_id,
            user_query=message,
            ai_response=answer,
            query_type=query_type.value if hasattr(query_type, "value") else str(query_type),
            confidence=confidence,
            entities=entities,
            sources=sources,
            response_time_ms=response_time_ms,
        )

        return ChatResponse(
            response=answer,
            query_type=query_type.value if hasattr(query_type, "value") else str(query_type),
            confidence=confidence,
            sources=sources,
            suggestions=suggestions,
            entities=entities,
        )

    except Exception as e:
        logger.error(f"LLM Error: {e}")

        # Fallback ì‘ë‹µ
        fallback = route_result.get("fallback_message") or rag_router.get_fallback_response(
            "unknown"
        )

        # ë°ì´í„° ê¸°ë°˜ ê¸°ë³¸ ì‘ë‹µ ì¶”ê°€
        if data and query_type == QueryType.DATA_QUERY:
            brand_kpis = data.get("brand", {}).get("kpis", {})
            fallback = f"""í˜„ì¬ LANEIGE í˜„í™©:
- SoS: {brand_kpis.get('sos', 0)}%
- Top 10 ì œí’ˆ: {brand_kpis.get('top10_count', 0)}ê°œ
- í‰ê·  ìˆœìœ„: {brand_kpis.get('avg_rank', 0)}ìœ„

(ìƒì„¸ ë¶„ì„ì„ ìœ„í•´ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”)"""

        # Fallback ì‘ë‹µë„ Audit Trail ê¸°ë¡
        response_time_ms = (time.time() - start_time) * 1000
        log_chat_interaction(
            session_id=session_id,
            user_query=message,
            ai_response=f"[ERROR] {str(e)[:100]} | Fallback: {fallback[:200]}",
            query_type=query_type.value if hasattr(query_type, "value") else str(query_type),
            confidence=0.0,
            entities=entities,
            sources=["fallback"],
            response_time_ms=response_time_ms,
        )

        return ChatResponse(
            response=fallback,
            query_type=query_type.value if hasattr(query_type, "value") else str(query_type),
            confidence=0.0,
            sources=[],
            suggestions=["ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”", "SoSê°€ ë­”ê°€ìš”?", "í˜„ì¬ ìˆœìœ„ ì•Œë ¤ì£¼ì„¸ìš”"],
            entities=entities,
        )


@app.delete("/api/chat/memory/{session_id}")
async def clear_memory(session_id: str):
    """ì„¸ì…˜ ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"""
    if session_id in conversation_memory:
        del conversation_memory[session_id]
    return {"status": "ok", "message": f"Session {session_id} memory cleared"}


@app.get("/api/crawl/status")
async def get_crawl_status():
    """
    í¬ë¡¤ë§ ìƒíƒœ ì¡°íšŒ

    Returns:
        - status: idle/running/completed/failed
        - date: í¬ë¡¤ë§ ëŒ€ìƒ ë‚ ì§œ
        - progress: ì§„í–‰ë¥  (0-100)
        - data_date: í˜„ì¬ ë°ì´í„° ë‚ ì§œ
        - needs_crawl: í¬ë¡¤ë§ í•„ìš” ì—¬ë¶€
    """
    crawl_manager = await get_crawl_manager()
    return {
        **crawl_manager.state.to_dict(),
        "data_date": crawl_manager.get_data_date(),
        "needs_crawl": crawl_manager.needs_crawl(),
        "is_today_available": crawl_manager.is_today_data_available(),
        "status_message": crawl_manager.get_status_message(),
    }


@app.post("/api/crawl/start", dependencies=[Depends(verify_api_key)])
async def start_crawl():
    """
    ìˆ˜ë™ìœ¼ë¡œ í¬ë¡¤ë§ ì‹œì‘ (API Key í•„ìš”)

    Returns:
        - started: í¬ë¡¤ë§ ì‹œì‘ ì—¬ë¶€
        - message: ìƒíƒœ ë©”ì‹œì§€
    """
    crawl_manager = await get_crawl_manager()

    if crawl_manager.is_crawling():
        return {
            "started": False,
            "message": "í¬ë¡¤ë§ì´ ì´ë¯¸ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤.",
            "status": crawl_manager.state.to_dict(),
        }

    if crawl_manager.is_today_data_available():
        return {
            "started": False,
            "message": "ì˜¤ëŠ˜ ë°ì´í„°ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.",
            "status": crawl_manager.state.to_dict(),
        }

    started = await crawl_manager.start_crawl()
    return {
        "started": started,
        "message": "í¬ë¡¤ë§ì„ ì‹œì‘í–ˆìŠµë‹ˆë‹¤." if started else "í¬ë¡¤ë§ ì‹œì‘ ì‹¤íŒ¨",
        "status": crawl_manager.state.to_dict(),
    }


# ============= Historical Data API =============

from datetime import UTC, timedelta

from src.api.dependencies import get_sheets_writer


@app.get("/api/historical")
async def get_historical_data(
    start_date: str, end_date: str, category_id: str | None = None, brand: str | None = "LANEIGE"
):
    """
    íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„° ì¡°íšŒ (SQLite ìš°ì„ , Google Sheets fallback)

    Args:
        start_date: ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)
        end_date: ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)
        category_id: ì¹´í…Œê³ ë¦¬ í•„í„° (ì„ íƒ)
        brand: ë¸Œëœë“œ í•„í„° (ê¸°ë³¸ê°’: LANEIGE)

    Returns:
        - data: ë‚ ì§œë³„ ì§€í‘œ ë°ì´í„°
        - sos_history: SoS ì¶”ì´ ë°ì´í„°
        - raw_data: ìˆœìœ„ ì¶”ì´ ë°ì´í„°
    """
    try:
        records = []
        data_source = None

        # 1ì°¨: SQLiteì—ì„œ ì¡°íšŒ (ë¹ ë¦„)
        try:
            sqlite = get_sqlite_storage()
            await sqlite.initialize()
            records = await sqlite.get_raw_data(
                start_date=start_date,
                end_date=end_date,
                category_id=category_id,
                limit=50000,  # ì¶©ë¶„íˆ í° limit
            )
            if records:
                data_source = "sqlite"
                logging.info(
                    f"Historical: loaded {len(records)} records from SQLite ({start_date} ~ {end_date})"
                )
        except Exception as sqlite_err:
            logging.warning(f"Historical: SQLite ì¡°íšŒ ì‹¤íŒ¨: {sqlite_err}")

        # 2ì°¨: SQLite ì‹¤íŒ¨/ë¹ˆ ê²°ê³¼ ì‹œ Google Sheets fallback
        if not records:
            try:
                sheets_writer = get_sheets_writer()
                if not sheets_writer._initialized:
                    await sheets_writer.initialize()
                records = await sheets_writer.get_raw_data(
                    start_date=start_date, end_date=end_date, category_id=category_id
                )
                if records:
                    data_source = "sheets"
                    logging.info(
                        f"Historical: loaded {len(records)} records from Sheets ({start_date} ~ {end_date})"
                    )
            except Exception as sheets_err:
                logging.warning(f"Historical: Google Sheets ì¡°íšŒ ì‹¤íŒ¨: {sheets_err}")

        if not records:
            # ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ë°ì´í„° ì—†ìŒ - ë¡œì»¬ JSON íŒŒì¼ì—ì„œ ì‹œë„
            return await _get_historical_from_local(start_date, end_date, brand)

        # ë‚ ì§œ ë²”ìœ„ ê³„ì‚°
        start_dt = datetime.strptime(start_date, "%Y-%m-%d")
        end_dt = datetime.strptime(end_date, "%Y-%m-%d")
        days = (end_dt - start_dt).days + 1

        # ë‚ ì§œë³„ ë°ì´í„° ì§‘ê³„ (íŠ¹ì • ë¸Œëœë“œ í•„í„°ë§)
        daily_data = {}
        brand_lower = brand.lower() if brand else ""
        for record in records:
            snapshot_date = record.get("snapshot_date", "")
            if not snapshot_date or snapshot_date < start_date or snapshot_date > end_date:
                continue

            # íŠ¹ì • ë¸Œëœë“œ í•„í„°ë§ (SoS ì¶”ì´ ê³„ì‚°ìš©)
            record_brand = record.get("brand", "")
            if brand_lower and record_brand.lower() != brand_lower:
                continue

            if snapshot_date not in daily_data:
                daily_data[snapshot_date] = {
                    "date": snapshot_date,
                    "products": [],
                    "total_count": 0,
                    "top10_count": 0,
                }

            rank = int(record.get("rank", 0)) if record.get("rank") else 0
            daily_data[snapshot_date]["products"].append(
                {
                    "asin": record.get("asin", ""),
                    "product_name": record.get("product_name", ""),
                    "brand": record_brand,
                    "rank": rank,
                    "price": record.get("price", ""),
                    "rating": record.get("rating", ""),
                }
            )
            daily_data[snapshot_date]["total_count"] += 1
            if rank <= 10:
                daily_data[snapshot_date]["top10_count"] += 1

        # SoS ì¶”ì´ ê³„ì‚° (Top 100 ê¸°ì¤€, í•´ë‹¹ ë¸Œëœë“œ ê¸°ì¤€)
        sos_history = []
        raw_data = []
        for date_str in sorted(daily_data.keys()):
            day_data = daily_data[date_str]
            products = day_data["products"]

            # SoS = (ë¸Œëœë“œ ì œí’ˆ ìˆ˜ / 100) * 100
            sos = round(len(products) / 100 * 100, 1) if products else 0
            sos_history.append(
                {
                    "date": date_str,
                    "sos": sos,
                    "product_count": len(products),
                    "top10_count": day_data["top10_count"],
                }
            )

            # í‰ê·  ìˆœìœ„ (ìˆëŠ” ê²½ìš°)
            if products:
                avg_rank = round(sum(p["rank"] for p in products) / len(products), 1)
                raw_data.append(
                    {
                        "date": date_str,
                        "rank": avg_rank,
                        "best_rank": min(p["rank"] for p in products),
                        "worst_rank": max(p["rank"] for p in products),
                    }
                )

        # available_dates ê³„ì‚°
        available_dates = sorted(daily_data.keys())

        # brand_metrics ê³„ì‚° (ì „ì²´ ê¸°ê°„ í†µí•© - ëª¨ë“  ë¸Œëœë“œ í¬í•¨)
        brand_metrics = await _calculate_brand_metrics_for_period(records, daily_data, brand)

        # rank_history ìƒì„± (Product View ì°¨íŠ¸ìš©)
        # í˜•ì‹: { "2026-01-14": { "products": [{ "name": "...", "rank": 5, "price": 21.5 }, ...] } }
        rank_history = {}
        for record in records:
            snapshot_date = record.get("snapshot_date", "")
            if not snapshot_date or snapshot_date < start_date or snapshot_date > end_date:
                continue

            if snapshot_date not in rank_history:
                rank_history[snapshot_date] = {"products": []}

            rank = int(record.get("rank", 0)) if record.get("rank") else 0
            price_val = record.get("price", 0)
            try:
                price = float(str(price_val).replace("$", "").replace(",", "")) if price_val else 0
            except (ValueError, TypeError):
                price = 0

            rank_history[snapshot_date]["products"].append(
                {
                    "name": record.get("product_name", ""),
                    "product_name": record.get("product_name", ""),
                    "brand": record.get("brand", ""),
                    "asin": record.get("asin", ""),
                    "rank": rank,
                    "price": price,
                    "rating": record.get("rating", ""),
                    "discount_percent": record.get("discount_percent", 0),
                }
            )

        # ì „ì²´ ë°ì´í„°ì˜ ì‚¬ìš© ê°€ëŠ¥í•œ ë‚ ì§œ ë²”ìœ„ ì¡°íšŒ (SQLiteì—ì„œ)
        available_date_range = {"min": None, "max": None}
        try:
            sqlite = get_sqlite_storage()
            stats = sqlite.get_stats()
            if "date_range" in stats:
                available_date_range = stats["date_range"]
        except Exception:
            pass

        return {
            "success": True,
            "available_dates": available_dates,
            "available_date_range": available_date_range,
            "data_source": data_source,
            "brand_metrics": brand_metrics,
            "rank_history": rank_history,
            "data": {
                "sos_history": sos_history,
                "raw_data": raw_data,
                "daily_data": list(daily_data.values()),
                "period": {"start": start_date, "end": end_date, "days": days},
                "brand": brand,
            },
        }

    except Exception as e:
        logging.error(f"Historical data error: {e}")
        # í´ë°±: ë¡œì»¬ ë°ì´í„°ì—ì„œ ì‹œë„
        return await _get_historical_from_local(start_date, end_date, brand)


async def _calculate_brand_metrics_for_period(
    records: list[dict], daily_data: dict, target_brand: str
) -> list[dict]:
    """
    ê¸°ê°„ ë‚´ ëª¨ë“  ë¸Œëœë“œì˜ ë©”íŠ¸ë¦­ ê³„ì‚° (SoS Ã— Avg Rank ì°¨íŠ¸ìš©)

    Note:
        ê¸°ê°„ ì¡°íšŒ ì‹œ ë™ì¼ ASINì´ ì—¬ëŸ¬ ë‚ ì§œì— ì¤‘ë³µ ë“±ì¥í•˜ë¯€ë¡œ,
        ASIN ê¸°ì¤€ ìœ ë‹ˆí¬ ì¹´ìš´íŠ¸ë¥¼ ì ìš©í•˜ì—¬ ì •í™•í•œ ì œí’ˆ ìˆ˜ ê³„ì‚°

    Returns:
        ë¸Œëœë“œë³„ SoS, í‰ê·  ìˆœìœ„, ì œí’ˆ ìˆ˜ ë“±
    """
    # ì „ì²´ ì œí’ˆ ë°ì´í„° ì§‘ê³„ (ëª¨ë“  ë¸Œëœë“œ)
    brand_data = {}
    brand_unique_asins: dict[str, set] = {}  # ASIN ì¤‘ë³µ ì œê±°ìš©

    for record in records:
        brand_name = record.get("brand", "Unknown")
        asin = record.get("asin", "")
        rank = int(record.get("rank", 0)) if record.get("rank") else 0

        # Unknown ë¸Œëœë“œ ë° ë¹ˆ ë¸Œëœë“œ ì œì™¸ (ëŒ€ì‹œë³´ë“œì—ì„œ ì˜ë¯¸ ì—†ìŒ)
        if not brand_name or brand_name.lower() == "unknown" or rank == 0:
            continue

        if brand_name not in brand_data:
            brand_data[brand_name] = {
                "brand": brand_name,
                "ranks": [],
                "prices": [],
                "product_count": 0,
            }
            brand_unique_asins[brand_name] = set()

        # ìˆœìœ„ëŠ” ëª¨ë“  ë ˆì½”ë“œì—ì„œ ìˆ˜ì§‘ (í‰ê·  ê³„ì‚°ìš©)
        brand_data[brand_name]["ranks"].append(rank)

        # ê°€ê²© ìˆ˜ì§‘ (ìœ íš¨í•œ USD ê°€ê²© ë²”ìœ„ë§Œ)
        price = record.get("price")
        if price is not None:
            try:
                price_val = float(price)
                if 0.5 <= price_val <= 500:
                    brand_data[brand_name]["prices"].append(price_val)
            except (ValueError, TypeError):
                pass

        # ì œí’ˆ ìˆ˜ëŠ” ASIN ê¸°ì¤€ ìœ ë‹ˆí¬ ì¹´ìš´íŠ¸ (ì¤‘ë³µ ì œê±°)
        if asin and asin not in brand_unique_asins[brand_name]:
            brand_unique_asins[brand_name].add(asin)
            brand_data[brand_name]["product_count"] += 1
        elif not asin:
            # ASINì´ ì—†ëŠ” ê²½ìš° ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì¹´ìš´íŠ¸ (í´ë°±)
            brand_data[brand_name]["product_count"] += 1

    # ì´ ìœ ë‹ˆí¬ ì œí’ˆ ìˆ˜ (ëª¨ë“  ë¸Œëœë“œ - Unknown ì œì™¸ í›„)
    total_products = sum(b["product_count"] for b in brand_data.values())

    # ë©”íŠ¸ë¦­ ê³„ì‚°
    brand_metrics = []
    for brand_name, data in brand_data.items():
        if not data["ranks"]:
            continue

        sos = round(data["product_count"] / max(total_products, 100) * 100, 2)
        avg_rank = round(sum(data["ranks"]) / len(data["ranks"]), 1)

        # í‰ê·  ê°€ê²© ê³„ì‚°
        prices = data.get("prices", [])
        avg_price = round(sum(prices) / len(prices), 2) if prices else None

        # ë²„ë¸” í¬ê¸°: ì œí’ˆ ìˆ˜ ê¸°ë°˜ (ìµœì†Œ 5, ìµœëŒ€ 25)
        bubble_size = max(5, min(25, data["product_count"] * 2))

        is_laneige = target_brand.upper() in brand_name.upper()

        brand_metrics.append(
            {
                "brand": brand_name,
                "sos": sos,
                "avg_rank": avg_rank,
                "product_count": data["product_count"],
                "avg_price": avg_price,
                "bubble_size": bubble_size,
                "is_laneige": is_laneige,
            }
        )

    # SoS ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    brand_metrics.sort(key=lambda x: x["sos"], reverse=True)

    # ìƒìœ„ 10ê°œ ì¶”ì¶œ
    top_10 = brand_metrics[:10]

    # LANEIGEê°€ top_10ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ (brand_dataì— ìˆëŠ”ì§€ê°€ ì•„ë‹ˆë¼ top_10ì— ìˆëŠ”ì§€!)
    laneige_in_top10 = any(b.get("is_laneige") for b in top_10)

    # LANEIGEê°€ top_10ì— ì—†ìœ¼ë©´ ì¶”ê°€ (ë°ì´í„°ê°€ ì¡´ì¬í•  ê²½ìš°)
    if not laneige_in_top10 and target_brand:
        # brand_dataì—ì„œ LANEIGE ì°¾ê¸° (ëŒ€ì†Œë¬¸ì ë³€í˜• ëª¨ë‘ ì‹œë„)
        laneige_data = None
        for key in [
            target_brand,
            target_brand.upper(),
            target_brand.lower(),
            target_brand.capitalize(),
        ]:
            if key in brand_data:
                laneige_data = brand_data[key]
                break

        if laneige_data and laneige_data["ranks"]:
            sos = round(laneige_data["product_count"] / max(total_products, 100) * 100, 2)
            avg_rank = round(sum(laneige_data["ranks"]) / len(laneige_data["ranks"]), 1)
            l_prices = laneige_data.get("prices", [])
            l_avg_price = round(sum(l_prices) / len(l_prices), 2) if l_prices else None
            bubble_size = max(5, min(25, laneige_data["product_count"] * 2))
            top_10.append(
                {
                    "brand": target_brand,
                    "sos": sos,
                    "avg_rank": avg_rank,
                    "product_count": laneige_data["product_count"],
                    "avg_price": l_avg_price,
                    "bubble_size": bubble_size,
                    "is_laneige": True,
                }
            )
            # ë‹¤ì‹œ ì •ë ¬ í›„ ìƒìœ„ 11ê°œ ìœ ì§€ (LANEIGE í¬í•¨ ë³´ì¥)
            top_10.sort(key=lambda x: x["sos"], reverse=True)

    # Summer Fridays íŠ¹ë³„ ì²˜ë¦¬ (ê³ ê° ìš”ì²­ tracked competitor)
    # top_10ì— ì—†ìœ¼ë©´ ê°•ì œ ì¶”ê°€
    TRACKED_COMPETITORS = ["Summer Fridays"]
    for tracked_brand in TRACKED_COMPETITORS:
        tracked_in_top = any(b.get("brand") == tracked_brand for b in top_10)
        if not tracked_in_top and tracked_brand in brand_data:
            tracked_data = brand_data[tracked_brand]
            if tracked_data["ranks"]:
                sos = round(tracked_data["product_count"] / max(total_products, 100) * 100, 2)
                avg_rank = round(sum(tracked_data["ranks"]) / len(tracked_data["ranks"]), 1)
                t_prices = tracked_data.get("prices", [])
                t_avg_price = round(sum(t_prices) / len(t_prices), 2) if t_prices else None
                bubble_size = max(5, min(25, tracked_data["product_count"] * 2))
                top_10.append(
                    {
                        "brand": tracked_brand,
                        "sos": sos,
                        "avg_rank": avg_rank,
                        "product_count": tracked_data["product_count"],
                        "avg_price": t_avg_price,
                        "bubble_size": bubble_size,
                        "is_laneige": False,
                        "is_tracked": True,  # tracked competitor í‘œì‹œ
                    }
                )
        # ë°ì´í„°ê°€ ì—†ì–´ë„ placeholder ì¶”ê°€ (UIì—ì„œ "-" ëŒ€ì‹  "ë°ì´í„° ì—†ìŒ" í‘œì‹œ ê°€ëŠ¥)
        elif not tracked_in_top:
            top_10.append(
                {
                    "brand": tracked_brand,
                    "sos": 0,
                    "avg_rank": None,
                    "product_count": 0,
                    "bubble_size": 5,
                    "is_laneige": False,
                    "is_tracked": True,
                    "no_data": True,  # í•´ë‹¹ ê¸°ê°„ ë°ì´í„° ì—†ìŒ í‘œì‹œ
                }
            )

    # ìµœì¢… ì •ë ¬ (SoS ë‚´ë¦¼ì°¨ìˆœ, trackedëŠ” í•˜ë‹¨ì— ìœ ì§€)
    top_10.sort(key=lambda x: (not x.get("is_tracked", False), x["sos"]), reverse=True)

    return top_10


def _get_brand_metrics_from_dashboard(dashboard_data: dict | None, target_brand: str) -> list[dict]:
    """
    ëŒ€ì‹œë³´ë“œ ë°ì´í„°ì—ì„œ ë¸Œëœë“œ ë©”íŠ¸ë¦­ ì¶”ì¶œ (ë¡œì»¬ í´ë°±ìš©)
    """
    if not dashboard_data:
        return []

    # ëŒ€ì‹œë³´ë“œì˜ brand_matrix ë°ì´í„° ì‚¬ìš©
    brand_matrix = dashboard_data.get("charts", {}).get("brand_matrix", [])
    if brand_matrix:
        return brand_matrix

    # ê²½ìŸì‚¬ ë°ì´í„°ì—ì„œ ìƒì„±
    competitors = dashboard_data.get("brand", {}).get("competitors", [])
    if not competitors:
        return []

    brand_metrics = []
    for comp in competitors:
        brand_metrics.append(
            {
                "brand": comp.get("brand", "Unknown"),
                "sos": comp.get("sos", 0),
                "avg_rank": comp.get("avg_rank", 50),
                "product_count": comp.get("product_count", 0),
                "bubble_size": max(5, min(25, comp.get("product_count", 0) * 2)),
                "is_laneige": target_brand.upper() in comp.get("brand", "").upper(),
            }
        )

    return brand_metrics


async def _get_historical_from_local(
    start_date: str, end_date: str, brand: str = "LANEIGE"
) -> dict[str, Any]:
    """
    ë¡œì»¬ JSON íŒŒì¼ì—ì„œ íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„° ì¡°íšŒ (í´ë°±)

    data/ í´ë”ì˜ ë‚ ì§œë³„ JSON íŒŒì¼ì´ë‚˜ dashboard_data.jsonì˜ íˆìŠ¤í† ë¦¬ ë°ì´í„° í™œìš©
    """
    try:
        # ë©”ì¸ ëŒ€ì‹œë³´ë“œ ë°ì´í„° ë¡œë“œ
        data = load_dashboard_data()
        sos_history = []
        raw_data = []

        # 1. ëŒ€ì‹œë³´ë“œ ë°ì´í„°ì—ì„œ í˜„ì¬ SoS/ìˆœìœ„ ì •ë³´ ì¶”ì¶œ
        if data:
            brand_kpis = data.get("brand", {}).get("kpis", {})
            current_sos = brand_kpis.get("sos", 0)
            data_date = data.get("metadata", {}).get(
                "data_date", datetime.now().strftime("%Y-%m-%d")
            )

            # í˜„ì¬ ë‚ ì§œê°€ ìš”ì²­ ë²”ìœ„ì— í¬í•¨ë˜ë©´ ì¶”ê°€
            if start_date <= data_date <= end_date:
                sos_history.append(
                    {
                        "date": data_date,
                        "sos": current_sos,
                        "product_count": brand_kpis.get("product_count", 0),
                        "top10_count": brand_kpis.get("top10_count", 0),
                    }
                )

                avg_rank = brand_kpis.get("avg_rank", 0)
                if avg_rank:
                    raw_data.append(
                        {
                            "date": data_date,
                            "rank": avg_rank,
                            "best_rank": brand_kpis.get("best_rank", avg_rank),
                            "worst_rank": brand_kpis.get("worst_rank", avg_rank),
                        }
                    )

        # 2. latest_crawl_result.jsonì—ì„œ ë°ì´í„° ì¶”ì¶œ
        latest_crawl_path = Path("./data/latest_crawl_result.json")
        if latest_crawl_path.exists():
            try:
                with open(latest_crawl_path, encoding="utf-8") as f:
                    crawl_data = json.load(f)

                # ëª¨ë“  ì¹´í…Œê³ ë¦¬ì—ì„œ ë¸Œëœë“œ ì œí’ˆ ì°¾ê¸°
                brand_products = []
                crawl_date = None

                for _cat_id, cat_data in crawl_data.get("categories", {}).items():
                    for product in cat_data.get("products", []):
                        product_brand = product.get("brand", "")
                        product_name = product.get("product_name", "")

                        # ë¸Œëœë“œ ë§¤ì¹­ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ, ë¶€ë¶„ ë§¤ì¹­)
                        if (
                            brand.upper() in product_brand.upper()
                            or brand.upper() in product_name.upper()
                        ):
                            brand_products.append(product)
                            if not crawl_date:
                                crawl_date = product.get("snapshot_date")

                if brand_products and crawl_date and start_date <= crawl_date <= end_date:
                    # ì¤‘ë³µ ì œê±° í™•ì¸
                    if not any(h["date"] == crawl_date for h in sos_history):
                        # ì¹´í…Œê³ ë¦¬ë³„ ì´ ì œí’ˆ ìˆ˜ (Top 100 ê¸°ì¤€)
                        total_products = sum(
                            len(cat.get("products", []))
                            for cat in crawl_data.get("categories", {}).values()
                        )

                        sos = round(len(brand_products) / max(total_products, 100) * 100, 2)
                        avg_rank = round(
                            sum(p.get("rank", 0) for p in brand_products) / len(brand_products), 1
                        )

                        sos_history.append(
                            {
                                "date": crawl_date,
                                "sos": sos,
                                "product_count": len(brand_products),
                                "top10_count": sum(
                                    1 for p in brand_products if p.get("rank", 100) <= 10
                                ),
                            }
                        )
                        raw_data.append(
                            {
                                "date": crawl_date,
                                "rank": avg_rank,
                                "best_rank": min(p.get("rank", 100) for p in brand_products),
                                "worst_rank": max(p.get("rank", 100) for p in brand_products),
                            }
                        )

            except (json.JSONDecodeError, ValueError) as e:
                logging.warning(f"Failed to parse latest_crawl_result.json: {e}")

        # 3. raw_products í´ë”ì—ì„œ ë‚ ì§œë³„ ë°ì´í„° ê²€ìƒ‰ (ê¸°ì¡´ ë¡œì§)
        raw_data_dir = Path("./data/raw_products")
        if raw_data_dir.exists():
            for json_file in raw_data_dir.glob("*.json"):
                try:
                    file_date = json_file.stem  # íŒŒì¼ëª…ì´ YYYY-MM-DD í˜•ì‹ì´ë¼ê³  ê°€ì •
                    if start_date <= file_date <= end_date:
                        with open(json_file, encoding="utf-8") as f:
                            daily_raw = json.load(f)

                        # ë¸Œëœë“œ ì œí’ˆë§Œ í•„í„°ë§
                        brand_products = [
                            p
                            for p in daily_raw
                            if brand.upper() in p.get("brand", "").upper()
                            or brand.upper() in p.get("product_name", "").upper()
                        ]

                        if brand_products:
                            sos = round(len(brand_products) / 100 * 100, 1)
                            avg_rank = round(
                                sum(p.get("rank", 0) for p in brand_products) / len(brand_products),
                                1,
                            )

                            # ì¤‘ë³µ ì œê±°
                            if not any(h["date"] == file_date for h in sos_history):
                                sos_history.append(
                                    {
                                        "date": file_date,
                                        "sos": sos,
                                        "product_count": len(brand_products),
                                        "top10_count": sum(
                                            1 for p in brand_products if p.get("rank", 100) <= 10
                                        ),
                                    }
                                )
                                raw_data.append(
                                    {
                                        "date": file_date,
                                        "rank": avg_rank,
                                        "best_rank": min(
                                            p.get("rank", 100) for p in brand_products
                                        ),
                                        "worst_rank": max(
                                            p.get("rank", 100) for p in brand_products
                                        ),
                                    }
                                )
                except (json.JSONDecodeError, ValueError):
                    continue

        # ë‚ ì§œìˆœ ì •ë ¬
        sos_history.sort(key=lambda x: x["date"])
        raw_data.sort(key=lambda x: x["date"])

        # available_dates ê³„ì‚°
        available_dates = [h["date"] for h in sos_history]

        # brand_metrics ê³„ì‚° (í˜„ì¬ ëŒ€ì‹œë³´ë“œ ë°ì´í„°ì—ì„œ)
        brand_metrics = _get_brand_metrics_from_dashboard(data, brand)

        # rank_history ìƒì„± (CPI ì°¨íŠ¸ìš© - ëª¨ë“  ë¸Œëœë“œ ì œí’ˆ í¬í•¨)
        rank_history = {}
        latest_crawl_path = Path("./data/latest_crawl_result.json")
        if latest_crawl_path.exists():
            try:
                with open(latest_crawl_path, encoding="utf-8") as f:
                    crawl_data = json.load(f)
                for _cat_id, cat_data in crawl_data.get("categories", {}).items():
                    for product in cat_data.get("products", []):
                        snap_date = product.get("snapshot_date", "")
                        if not snap_date or snap_date < start_date or snap_date > end_date:
                            continue
                        if snap_date not in rank_history:
                            rank_history[snap_date] = {"products": []}
                        price_val = product.get("price", 0)
                        try:
                            price = (
                                float(str(price_val).replace("$", "").replace(",", ""))
                                if price_val
                                else 0
                            )
                        except (ValueError, TypeError):
                            price = 0
                        rank_history[snap_date]["products"].append(
                            {
                                "name": product.get("product_name", ""),
                                "brand": product.get("brand", ""),
                                "rank": product.get("rank", 0),
                                "price": price,
                            }
                        )
            except (json.JSONDecodeError, ValueError) as e:
                logging.warning(f"Failed to build rank_history from local: {e}")

        if not sos_history:
            return {
                "success": False,
                "error": "No historical data found for the specified period",
                "available_dates": [],
                "brand_metrics": [],
                "rank_history": rank_history,
                "data": None,
            }

        return {
            "success": True,
            "available_dates": available_dates,
            "brand_metrics": brand_metrics,
            "rank_history": rank_history,
            "data": {
                "sos_history": sos_history,
                "raw_data": raw_data,
                "period": {"start": start_date, "end": end_date},
                "brand": brand,
                "source": "local",
            },
        }

    except Exception as e:
        logging.error(f"Local historical data error: {e}")
        return {
            "success": False,
            "error": str(e),
            "available_dates": [],
            "brand_metrics": [],
            "data": None,
        }


@app.post("/api/export/docx")
async def export_docx(request: ExportRequest):
    """
    ì¸ì‚¬ì´íŠ¸ ë¦¬í¬íŠ¸ DOCX ìƒì„± ë° ë‹¤ìš´ë¡œë“œ
    """
    data = load_dashboard_data()
    if not data:
        raise HTTPException(status_code=404, detail="Dashboard data not found")

    # DOCX ë¬¸ì„œ ìƒì„±
    doc = Document()

    # ìŠ¤íƒ€ì¼ ì„¤ì •
    style = doc.styles["Normal"]
    font = style.font
    font.name = "Arial"
    font.size = Pt(11)

    # ===== í‘œì§€ =====
    title = doc.add_heading("AMORE INSIGHT Report", 0)
    title.alignment = WD_ALIGN_PARAGRAPH.CENTER

    subtitle = doc.add_paragraph("LANEIGE Amazon US ë¶„ì„ ë¦¬í¬íŠ¸")
    subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER

    # ë‚ ì§œ
    metadata = data.get("metadata", {})
    date_para = doc.add_paragraph()
    date_para.alignment = WD_ALIGN_PARAGRAPH.CENTER
    date_para.add_run(
        f"ë¶„ì„ ê¸°ì¤€ì¼: {metadata.get('data_date', datetime.now().strftime('%Y-%m-%d'))}"
    )
    date_para.add_run(f"\nìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M')}")

    doc.add_page_break()

    # ===== 1. Executive Summary =====
    doc.add_heading("1. Executive Summary", level=1)

    brand_kpis = data.get("brand", {}).get("kpis", {})
    home_status = data.get("home", {}).get("status", {})

    summary_text = f"""
LANEIGE ë¸Œëœë“œëŠ” Amazon US ì‹œì¥ì—ì„œ {home_status.get('exposure', 'N/A')} ìƒíƒœì…ë‹ˆë‹¤.

â€¢ Share of Shelf (SoS): {brand_kpis.get('sos', 0)}%
â€¢ Top 10 ì§„ì… ì œí’ˆ: {brand_kpis.get('top10_count', 0)}ê°œ
â€¢ í‰ê·  ìˆœìœ„: {brand_kpis.get('avg_rank', 0)}ìœ„
â€¢ ì‹œì¥ ì§‘ì¤‘ë„ (HHI): {brand_kpis.get('hhi', 0)}

í˜„ì¬ ì‹œì¥ í¬ì§€ì…˜: {home_status.get('position', 'N/A')}
ì£¼ì˜ í•„ìš” ì œí’ˆ: {home_status.get('warning_count', 0)}ê°œ
"""
    doc.add_paragraph(summary_text)

    # ===== 2. ì œí’ˆë³„ í˜„í™© =====
    doc.add_heading("2. LANEIGE ì œí’ˆ í˜„í™©", level=1)

    products = data.get("products", {})
    if products:
        # í…Œì´ë¸” ìƒì„±
        table = doc.add_table(rows=1, cols=5)
        table.style = "Table Grid"
        table.alignment = WD_TABLE_ALIGNMENT.CENTER

        # í—¤ë”
        header_cells = table.rows[0].cells
        headers = ["ì œí’ˆëª…", "ìˆœìœ„", "ë³€ë™", "í‰ì ", "ë³€ë™ì„±"]
        for i, header in enumerate(headers):
            header_cells[i].text = header
            header_cells[i].paragraphs[0].runs[0].bold = True

        # ë°ì´í„° í–‰
        for _asin, product in products.items():
            row = table.add_row().cells
            row[0].text = product.get("name", "")[:40]
            row[1].text = f"#{product.get('rank', 'N/A')}"
            row[2].text = product.get("rank_delta", "-")
            row[3].text = str(product.get("rating", "-"))
            row[4].text = product.get("volatility_status", "-")

    doc.add_paragraph()

    # ===== 3. ê²½ìŸì‚¬ ë¶„ì„ =====
    doc.add_heading("3. ê²½ìŸì‚¬ ë¶„ì„", level=1)

    competitors = data.get("brand", {}).get("competitors", [])
    if competitors:
        table = doc.add_table(rows=1, cols=4)
        table.style = "Table Grid"

        header_cells = table.rows[0].cells
        headers = ["ë¸Œëœë“œ", "SoS (%)", "í‰ê·  ìˆœìœ„", "ì œí’ˆ ìˆ˜"]
        for i, header in enumerate(headers):
            header_cells[i].text = header
            header_cells[i].paragraphs[0].runs[0].bold = True

        for comp in competitors[:10]:
            row = table.add_row().cells
            row[0].text = comp.get("brand", "")
            row[1].text = str(comp.get("sos", 0))
            row[2].text = str(comp.get("avg_rank", "-"))
            row[3].text = str(comp.get("product_count", 0))

    doc.add_paragraph()

    # ===== 4. ì•¡ì…˜ ì•„ì´í…œ =====
    doc.add_heading("4. ì•¡ì…˜ ì•„ì´í…œ", level=1)

    action_items = data.get("home", {}).get("action_items", [])
    if action_items:
        for item in action_items:
            priority_marker = "ğŸ”´" if item.get("priority") == "P1" else "ğŸŸ "
            para = doc.add_paragraph()
            para.add_run(f"{priority_marker} [{item.get('priority')}] ").bold = True
            para.add_run(f"{item.get('product_name', '')}\n")
            para.add_run(f"   ì‹ í˜¸: {item.get('signal', '')}\n")
            para.add_run(f"   ê¶Œì¥ ì•¡ì…˜: {item.get('action_tag', '')}")
    else:
        doc.add_paragraph("í˜„ì¬ íŠ¹ë³„í•œ ì•¡ì…˜ ì•„ì´í…œì´ ì—†ìŠµë‹ˆë‹¤.")

    # ===== 5. ì „ëµì  ê¶Œê³ ì‚¬í•­ =====
    if request.include_strategy:
        doc.add_heading("5. ì „ëµì  ê¶Œê³ ì‚¬í•­", level=1)

        # ChatGPTë¡œ ì „ëµ ìƒì„± (RAG ì»¨í…ìŠ¤íŠ¸ í™œìš©)
        try:
            # RAGì—ì„œ ì „ëµ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
            strategy_context, _ = await get_rag_context("ì „ëµ ì•¡ì…˜ ê¶Œê³ ", QueryType.ANALYSIS)

            strategy_prompt = f"""ë‹¤ìŒ ë°ì´í„°ì™€ ê°€ì´ë“œë¼ì¸ì„ ë°”íƒ•ìœ¼ë¡œ LANEIGE ë¸Œëœë“œì˜ ì „ëµì  ê¶Œê³ ì‚¬í•­ 3ê°€ì§€ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

ë°ì´í„°:
- SoS: {brand_kpis.get('sos', 0)}%
- Top 10 ì œí’ˆ: {brand_kpis.get('top10_count', 0)}ê°œ
- í‰ê·  ìˆœìœ„: {brand_kpis.get('avg_rank', 0)}ìœ„
- ì£¼ìš” ê²½ìŸì‚¬: {', '.join([c['brand'] for c in competitors[:3]])}

ì°¸ê³  ê°€ì´ë“œë¼ì¸:
{strategy_context if strategy_context else 'ê¸°ë³¸ ì „ëµ ê¸°ì¤€ ì ìš©'}

ê° ê¶Œê³ ì‚¬í•­ì€ 1-2ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
"""
            response = await acompletion(
                model="gpt-4.1-mini",
                messages=[
                    {
                        "role": "system",
                        "content": "ë‹¹ì‹ ì€ ë·°í‹° ì´ì»¤ë¨¸ìŠ¤ ì „ë¬¸ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤. ê°„ê²°í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì „ëµì„ ì œì•ˆí•©ë‹ˆë‹¤.",
                    },
                    {"role": "user", "content": strategy_prompt},
                ],
                temperature=0.3,
                max_tokens=500,
            )

            strategy_text = response.choices[0].message.content
            doc.add_paragraph(strategy_text)

        except Exception:
            # í´ë°± ì „ëµ
            doc.add_paragraph("""
1. Top 10 ìœ ì§€ ì „ëµ: í˜„ì¬ ìƒìœ„ê¶Œ ì œí’ˆì˜ ë¦¬ë·° ê´€ë¦¬ ë° ì¬ê³  í™•ë³´ë¥¼ í†µí•œ í¬ì§€ì…˜ ìœ ì§€

2. ê²½ìŸì‚¬ ëª¨ë‹ˆí„°ë§: e.l.f., Maybelline ë“± ì£¼ìš” ê²½ìŸì‚¬ì˜ ê°€ê²© ë° í”„ë¡œëª¨ì…˜ ë™í–¥ íŒŒì•…

3. ì‹ ê·œ ì§„ì… ê¸°íšŒ: Lip Care ì¹´í…Œê³ ë¦¬ ì™¸ Face Powder, Toner ë“± í™•ì¥ ê°€ëŠ¥ì„± ê²€í† 
""")

    # ===== í‘¸í„° =====
    doc.add_paragraph()
    footer = doc.add_paragraph()
    footer.alignment = WD_ALIGN_PARAGRAPH.CENTER
    footer.add_run("Â© 2025 AMORE Pacific - Confidential").italic = True

    # BytesIOë¡œ ì €ì¥
    buffer = BytesIO()
    doc.save(buffer)
    buffer.seek(0)

    # íŒŒì¼ëª… ìƒì„±
    filename = f"AMORE_Insight_Report_{datetime.now().strftime('%Y%m%d_%H%M')}.docx"

    return StreamingResponse(
        buffer,
        media_type="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
        headers={"Content-Disposition": f"attachment; filename={filename}"},
    )


@app.post("/api/export/excel")
async def export_excel(request: Request):
    """
    ì—‘ì…€ ë°ì´í„° ë‚´ë³´ë‚´ê¸° (JSON íŒŒì¼ â†’ Excel)

    ë°ì´í„° ì†ŒìŠ¤:
    - Railway: /data/latest_crawl_result.json (Volume)
    - Local: ./data/latest_crawl_result.json
    """

    import pandas as pd

    try:
        # Parse request body
        body = await request.json()
        start_date = body.get("start_date")
        end_date = body.get("end_date")
        _include_metrics = body.get("include_metrics", True)  # reserved for future use

        # ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì •
        data_dir = Path("./data")

        # ========================================
        # 1ì°¨: SQLiteì—ì„œ ê¸°ê°„ë³„ ë°ì´í„° ì¡°íšŒ (ê°€ì¥ ë¹ ë¦„)
        # ========================================
        all_records = []
        data_source = None

        if start_date and end_date:
            # 1-1. SQLite ì‹œë„
            try:
                from src.tools.storage.sqlite_storage import get_sqlite_storage

                sqlite = get_sqlite_storage()
                await sqlite.initialize()

                # limitì„ í¬ê²Œ ì„¤ì • (5ê°œ ì¹´í…Œê³ ë¦¬ Ã— 100ê°œ Ã— ê¸°ê°„ì¼ìˆ˜)
                start_dt = datetime.strptime(start_date, "%Y-%m-%d")
                end_dt = datetime.strptime(end_date, "%Y-%m-%d")
                days = (end_dt - start_dt).days + 1
                max_records = 500 * days  # ì¶©ë¶„í•œ ì—¬ìœ 

                records = await sqlite.get_raw_data(
                    start_date=start_date, end_date=end_date, limit=max_records
                )

                if records:
                    all_records = records
                    data_source = "sqlite"
                    logging.info(
                        f"Excel export: loaded {len(all_records)} records from SQLite ({start_date} ~ {end_date})"
                    )

            except Exception as sqlite_err:
                logging.warning(f"Excel export: SQLite ì¡°íšŒ ì‹¤íŒ¨: {sqlite_err}")

            # 1-2. SQLite ì‹¤íŒ¨ ì‹œ Google Sheets ì‹œë„
            if not all_records:
                try:
                    sheets_writer = get_sheets_writer()
                    if not sheets_writer._initialized:
                        await sheets_writer.initialize()

                    records = await sheets_writer.get_raw_data(days=days)

                    if records:
                        for record in records:
                            snapshot_date = record.get("snapshot_date", "")
                            if snapshot_date and start_date <= snapshot_date <= end_date:
                                all_records.append(record)

                        if all_records:
                            data_source = "sheets"
                            logging.info(
                                f"Excel export: loaded {len(all_records)} records from Google Sheets ({start_date} ~ {end_date})"
                            )

                except Exception as sheets_err:
                    logging.warning(f"Excel export: Google Sheets ì¡°íšŒ ì‹¤íŒ¨: {sheets_err}")

        # ========================================
        # 2ì°¨: ë¡œì»¬ JSON íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ (í´ë°±)
        # ========================================
        crawl_data = None
        json_path = None

        if not data_source:
            possible_paths = [
                data_dir / "latest_crawl_result.json",
                data_dir / "dashboard_data.json",
            ]

            for path in possible_paths:
                if path.exists():
                    json_path = path
                    break

            if json_path is None:
                raise HTTPException(
                    status_code=404, detail="í¬ë¡¤ë§ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € í¬ë¡¤ë§ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”."
                )

            with open(json_path, encoding="utf-8") as f:
                crawl_data = json.load(f)

            logging.info(f"Excel export: loaded data from {json_path}")

        # ë°ì´í„° ì†ŒìŠ¤ ìœ í˜• íŒë‹¨
        # 1. data_source: SQLite ë˜ëŠ” Google Sheetsì—ì„œ ê¸°ê°„ë³„ ë°ì´í„° ë¡œë“œë¨
        # 2. is_crawl_data: latest_crawl_result.jsonì˜ raw ë°ì´í„°
        # 3. is_dashboard_data: dashboard_data.jsonì˜ ì§‘ê³„ ë°ì´í„°
        is_crawl_data = False
        is_dashboard_data = False

        if crawl_data:
            if "categories" in crawl_data:
                first_cat = next(iter(crawl_data["categories"].values()), {})
                is_crawl_data = isinstance(first_cat, dict) and (
                    "rank_records" in first_cat or "products" in first_cat
                )
            is_dashboard_data = "metadata" in crawl_data and "brand" in crawl_data

        # ì¶œë ¥ ê²½ë¡œ (Railway í™˜ê²½ ê³ ë ¤)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = data_dir / "exports"
        output_dir.mkdir(parents=True, exist_ok=True)
        output_path = output_dir / f"AMORE_Data_{timestamp}.xlsx"

        sheets_created = []
        total_rows = 0

        # ì¹´í…Œê³ ë¦¬ ë§¤í•‘
        categories_info = {
            "beauty": "Beauty & Personal Care",
            "skin_care": "Skin Care",
            "lip_care": "Lip Care",
            "lip_makeup": "Lip Makeup",
            "face_powder": "Face Powder",
        }

        with pd.ExcelWriter(str(output_path), engine="openpyxl") as writer:
            # Google Sheets RawDataì™€ ë™ì¼í•œ ì»¬ëŸ¼ ìˆœì„œ
            RAWDATA_COLUMNS = [
                "snapshot_date",
                "category_id",
                "rank",
                "asin",
                "product_name",
                "brand",
                "price",
                "list_price",
                "discount_percent",
                "rating",
                "reviews_count",
                "badge",
                "coupon_text",
                "is_subscribe_save",
                "promo_badges",
                "product_url",
            ]

            # ========================================
            # Case 1: SQLite/Google Sheetsì—ì„œ ê¸°ê°„ë³„ ë°ì´í„° ë¡œë“œë¨
            # ========================================
            if data_source and all_records:
                source_name = "SQLite" if data_source == "sqlite" else "Google Sheets"
                logging.info(
                    f"Excel export: using {source_name} data ({len(all_records)} records, {start_date} ~ {end_date})"
                )

                df_all = pd.DataFrame(all_records)

                if not df_all.empty:
                    # 1. RawData ì‹œíŠ¸ - ì „ì²´ ë°ì´í„°
                    available_cols = [c for c in RAWDATA_COLUMNS if c in df_all.columns]
                    df_raw = df_all[available_cols].copy()
                    df_raw = df_raw.sort_values(["snapshot_date", "category_id", "rank"])
                    df_raw.to_excel(writer, sheet_name="RawData", index=False)
                    sheets_created.append("RawData")
                    total_rows += len(df_raw)

                    # 2. ë‚ ì§œë³„ ìš”ì•½ ì‹œíŠ¸
                    if "snapshot_date" in df_all.columns:
                        date_summary = []
                        for date in sorted(df_all["snapshot_date"].unique()):
                            df_date = df_all[df_all["snapshot_date"] == date]
                            laneige_count = (
                                len(df_date[df_date["brand"].str.upper() == "LANEIGE"])
                                if "brand" in df_date.columns
                                else 0
                            )
                            date_summary.append(
                                {
                                    "ë‚ ì§œ": date,
                                    "ì´ ì œí’ˆ ìˆ˜": len(df_date),
                                    "LANEIGE ì œí’ˆ ìˆ˜": laneige_count,
                                    "LANEIGE SoS (%)": round(laneige_count / len(df_date) * 100, 1)
                                    if len(df_date) > 0
                                    else 0,
                                }
                            )
                        if date_summary:
                            df_summary = pd.DataFrame(date_summary)
                            df_summary.to_excel(writer, sheet_name="Daily Summary", index=False)
                            sheets_created.append("Daily Summary")
                            total_rows += len(df_summary)

                    # 3. ì¹´í…Œê³ ë¦¬ë³„ ì‹œíŠ¸
                    if "category_id" in df_all.columns:
                        for cat_id in df_all["category_id"].unique():
                            df_cat = df_all[df_all["category_id"] == cat_id].copy()
                            if df_cat.empty:
                                continue

                            display_cols = [
                                "snapshot_date",
                                "rank",
                                "asin",
                                "product_name",
                                "brand",
                                "price",
                                "rating",
                                "reviews_count",
                                "badge",
                            ]
                            available_display = [c for c in display_cols if c in df_cat.columns]
                            df_display = df_cat[available_display].sort_values(
                                ["snapshot_date", "rank"]
                            )

                            sheet_name = categories_info.get(cat_id, cat_id)[:31]
                            df_display.to_excel(writer, sheet_name=sheet_name, index=False)
                            sheets_created.append(sheet_name)
                            total_rows += len(df_display)

                    # 4. LANEIGE ì œí’ˆ ì „ìš© ì‹œíŠ¸
                    if "brand" in df_all.columns:
                        df_laneige = df_all[df_all["brand"].str.upper() == "LANEIGE"].copy()
                        if not df_laneige.empty:
                            laneige_cols = [
                                "snapshot_date",
                                "category_id",
                                "rank",
                                "asin",
                                "product_name",
                                "price",
                                "rating",
                                "reviews_count",
                                "badge",
                            ]
                            available_laneige = [c for c in laneige_cols if c in df_laneige.columns]
                            df_laneige = df_laneige[available_laneige].sort_values(
                                ["snapshot_date", "category_id", "rank"]
                            )
                            df_laneige.to_excel(writer, sheet_name="LANEIGE Products", index=False)
                            sheets_created.append("LANEIGE Products")
                            total_rows += len(df_laneige)

            # ========================================
            # Case 2: ëŒ€ì‹œë³´ë“œ ë°ì´í„° í˜•ì‹ (ì§‘ê³„ ë°ì´í„°ë§Œ)
            # ========================================
            elif is_dashboard_data and not is_crawl_data:
                logging.info("Excel export: using dashboard_data.json (aggregated data only)")

                # 1. Overview ì‹œíŠ¸
                metadata = crawl_data.get("metadata", {})
                data_source = crawl_data.get("data_source", {})
                overview_data = [
                    {"í•­ëª©": "ë°ì´í„° ë‚ ì§œ", "ê°’": metadata.get("data_date", "N/A")},
                    {"í•­ëª©": "ìƒì„± ì‹œê°", "ê°’": metadata.get("generated_at", "N/A")},
                    {"í•­ëª©": "ì´ ì œí’ˆ ìˆ˜", "ê°’": metadata.get("total_products", 0)},
                    {"í•­ëª©": "LANEIGE ì œí’ˆ ìˆ˜", "ê°’": metadata.get("laneige_products", 0)},
                    {"í•­ëª©": "í”Œë«í¼", "ê°’": data_source.get("platform", "Amazon US")},
                ]
                df_overview = pd.DataFrame(overview_data)
                df_overview.to_excel(writer, sheet_name="Overview", index=False)
                sheets_created.append("Overview")
                total_rows += len(df_overview)

                # 2. Brand KPIs ì‹œíŠ¸
                brand_kpis = crawl_data.get("brand", {}).get("kpis", {})
                if brand_kpis:
                    kpi_data = [
                        {"KPI": "SoS (Share of Shelf)", "ê°’": f"{brand_kpis.get('sos', 0)}%"},
                        {"KPI": "SoS ë³€í™”", "ê°’": brand_kpis.get("sos_delta", "N/A")},
                        {"KPI": "Top 10 ì œí’ˆ ìˆ˜", "ê°’": brand_kpis.get("top10_count", 0)},
                        {"KPI": "í‰ê·  ìˆœìœ„", "ê°’": brand_kpis.get("avg_rank", 0)},
                        {"KPI": "HHI (ì‹œì¥ ì§‘ì¤‘ë„)", "ê°’": brand_kpis.get("hhi", 0)},
                    ]
                    df_kpis = pd.DataFrame(kpi_data)
                    df_kpis.to_excel(writer, sheet_name="LANEIGE KPIs", index=False)
                    sheets_created.append("LANEIGE KPIs")
                    total_rows += len(df_kpis)

                # 3. Competitors ì‹œíŠ¸
                competitors = crawl_data.get("brand", {}).get("competitors", [])
                if competitors:
                    df_comp = pd.DataFrame(competitors)
                    column_mapping = {
                        "brand": "Brand",
                        "sos": "SoS (%)",
                        "avg_rank": "Avg Rank",
                        "product_count": "Product Count",
                        "avg_price": "Avg Price ($)",
                    }
                    existing_cols = {
                        k: v for k, v in column_mapping.items() if k in df_comp.columns
                    }
                    df_comp = df_comp.rename(columns=existing_cols)
                    df_comp.to_excel(writer, sheet_name="Competitors", index=False)
                    sheets_created.append("Competitors")
                    total_rows += len(df_comp)

                # 4. Action Items ì‹œíŠ¸
                action_items = crawl_data.get("home", {}).get("action_items", [])
                if action_items:
                    df_actions = pd.DataFrame(action_items)
                    df_actions.to_excel(writer, sheet_name="Action Items", index=False)
                    sheets_created.append("Action Items")
                    total_rows += len(df_actions)

                # 5. Category View ì‹œíŠ¸
                category_data = crawl_data.get("category", {})
                if category_data:
                    for cat_id, cat_info in category_data.items():
                        top_products = cat_info.get("top_products", [])
                        if top_products:
                            df_cat = pd.DataFrame(top_products)
                            sheet_name = categories_info.get(cat_id, cat_id)[:31]
                            df_cat.to_excel(writer, sheet_name=sheet_name, index=False)
                            sheets_created.append(sheet_name)
                            total_rows += len(df_cat)

            # ========================================
            # Case 3: ë¡œì»¬ í¬ë¡¤ë§ ì›ë³¸ ë°ì´í„°
            # ========================================
            else:
                logging.info("Excel export: using latest_crawl_result.json (raw crawl data)")

                # ì „ì²´ RawData ìˆ˜ì§‘ (ì¹´í…Œê³ ë¦¬ë³„ rank_records)
                all_records = []
                for cat_id, cat_data in crawl_data.get("categories", {}).items():
                    records = cat_data.get("rank_records", cat_data.get("products", []))
                    for record in records:
                        # category_id ì¶”ê°€ (ì—†ëŠ” ê²½ìš°)
                        if "category_id" not in record:
                            record["category_id"] = cat_id
                        all_records.append(record)

                if not all_records:
                    logging.warning("Excel export: no rank_records found in crawl data")

                if all_records:
                    df_all = pd.DataFrame(all_records)

                    # ë‚ ì§œ í•„í„° ì ìš© (ì„ íƒ ê¸°ê°„)
                    if start_date and "snapshot_date" in df_all.columns:
                        df_all = df_all[df_all["snapshot_date"] >= start_date]
                    if end_date and "snapshot_date" in df_all.columns:
                        df_all = df_all[df_all["snapshot_date"] <= end_date]

                    if not df_all.empty:
                        # 1. RawData ì‹œíŠ¸ - Google Sheetsì™€ ë™ì¼í•œ ì „ì²´ ë°ì´í„°
                        available_cols = [c for c in RAWDATA_COLUMNS if c in df_all.columns]
                        df_raw = df_all[available_cols].copy()
                        df_raw = df_raw.sort_values(["category_id", "rank"])
                        df_raw.to_excel(writer, sheet_name="RawData", index=False)
                        sheets_created.append("RawData")
                        total_rows += len(df_raw)

                        # 2. ì¹´í…Œê³ ë¦¬ë³„ ì‹œíŠ¸ (ìš”ì•½ ë³´ê¸°ìš©)
                        for cat_id in df_all["category_id"].unique():
                            df_cat = df_all[df_all["category_id"] == cat_id].copy()
                            if df_cat.empty:
                                continue

                            # í•µì‹¬ ì»¬ëŸ¼ë§Œ ì„ íƒí•˜ì—¬ ê°€ë…ì„± í–¥ìƒ
                            display_cols = [
                                "rank",
                                "asin",
                                "product_name",
                                "brand",
                                "price",
                                "rating",
                                "reviews_count",
                                "badge",
                            ]
                            available_display = [c for c in display_cols if c in df_cat.columns]
                            df_display = df_cat[available_display].sort_values("rank")

                            # ì‹œíŠ¸ ì´ë¦„ (31ì ì œí•œ)
                            sheet_name = categories_info.get(cat_id, cat_id)[:31]
                            df_display.to_excel(writer, sheet_name=sheet_name, index=False)
                            sheets_created.append(sheet_name)
                            total_rows += len(df_display)

                        # 3. LANEIGE ì œí’ˆ ì „ìš© ì‹œíŠ¸
                        df_laneige = df_all[df_all["brand"].str.upper() == "LANEIGE"].copy()
                        if not df_laneige.empty:
                            laneige_cols = [
                                "snapshot_date",
                                "category_id",
                                "rank",
                                "asin",
                                "product_name",
                                "price",
                                "rating",
                                "reviews_count",
                                "badge",
                            ]
                            available_laneige = [c for c in laneige_cols if c in df_laneige.columns]
                            df_laneige = df_laneige[available_laneige].sort_values(
                                ["category_id", "rank"]
                            )
                            df_laneige.to_excel(writer, sheet_name="LANEIGE Products", index=False)
                            sheets_created.append("LANEIGE Products")
                            total_rows += len(df_laneige)

                        # 4. Summary ì‹œíŠ¸ - ë¸Œëœë“œë³„ ì§‘ê³„
                        if "brand" in df_all.columns:
                            agg_dict = {"asin": "count"}
                            if "rank" in df_all.columns:
                                agg_dict["rank"] = "mean"
                            if "price" in df_all.columns:
                                agg_dict["price"] = "mean"
                            if "rating" in df_all.columns:
                                agg_dict["rating"] = "mean"

                            summary = df_all.groupby("brand").agg(agg_dict).reset_index()
                            col_names = ["Brand", "Product Count"]
                            if "rank" in agg_dict:
                                col_names.append("Avg Rank")
                            if "price" in agg_dict:
                                col_names.append("Avg Price")
                            if "rating" in agg_dict:
                                col_names.append("Avg Rating")
                            summary.columns = col_names

                            summary = summary.sort_values("Product Count", ascending=False).head(30)
                            for col in ["Avg Rank", "Avg Price", "Avg Rating"]:
                                if col in summary.columns:
                                    summary[col] = summary[col].round(2)

                            summary.to_excel(writer, sheet_name="Summary", index=False)
                            sheets_created.append("Summary")
                            total_rows += len(summary)

            # 4. ì‹œíŠ¸ê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ì•ˆë‚´ ì‹œíŠ¸ ìƒì„±
            if not sheets_created:
                data_source_info = (
                    "SQLite"
                    if data_source == "sqlite"
                    else (
                        "Google Sheets"
                        if data_source == "sheets"
                        else (str(json_path) if json_path else "N/A")
                    )
                )
                no_data_info = [
                    {"í•­ëª©": "ìš”ì²­ ê¸°ê°„", "ê°’": f"{start_date or 'N/A'} ~ {end_date or 'N/A'}"},
                    {"í•­ëª©": "ê²°ê³¼", "ê°’": "í•´ë‹¹ ê¸°ê°„ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"},
                    {"í•­ëª©": "ë°ì´í„° ì†ŒìŠ¤", "ê°’": data_source_info},
                    {"í•­ëª©": "ì•ˆë‚´", "ê°’": "í¬ë¡¤ë§ ì‹¤í–‰ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”"},
                ]
                df_no_data = pd.DataFrame(no_data_info)
                df_no_data.to_excel(writer, sheet_name="No Data", index=False)
                sheets_created.append("No Data")

        logging.info(f"Excel exported: {output_path} ({total_rows} rows, sheets: {sheets_created})")

        # íŒŒì¼ ë°˜í™˜
        return FileResponse(
            path=str(output_path),
            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            headers={"Content-Disposition": f"attachment; filename={output_path.name}"},
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Excel export error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Excel ë‚´ë³´ë‚´ê¸° ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤") from e


# ============= Alert Settings API =============

from src.api.dependencies import get_app_state_manager
from src.core.state_manager import EmailSubscription


@app.get("/api/v3/alert-settings")
async def get_alert_settings():
    """
    í˜„ì¬ ì•Œë¦¼ ì„¤ì • ì¡°íšŒ

    ì°¸ê³ : í˜„ì¬ëŠ” ë‹¨ì¼ ì‚¬ìš©ì ì„¤ì •ë§Œ ì§€ì› (ì²« ë²ˆì§¸ ë“±ë¡ëœ ì´ë©”ì¼)
    """
    state_manager = get_app_state_manager()
    subscriptions = state_manager.get_all_subscriptions()

    if not subscriptions:
        return {"email": "", "consent": False, "alert_types": [], "consent_date": None}

    # ì²« ë²ˆì§¸ êµ¬ë… ë°˜í™˜
    email, sub = next(iter(subscriptions.items()))
    return {
        "email": email,
        "consent": sub.consent,
        "alert_types": sub.alert_types,
        "consent_date": sub.consent_date.isoformat() if sub.consent_date else None,
    }


@app.post("/api/v3/alert-settings", dependencies=[Depends(verify_api_key)])
@limiter.limit("5/minute")  # ë¶„ë‹¹ 5íšŒ ì œí•œ (ìŠ¤íŒ¸ ë°©ì§€)
async def save_alert_settings(request: Request, settings: AlertSettingsRequest):
    """
    ì•Œë¦¼ ì„¤ì • ì €ì¥

    ë³´ì•ˆ: API Key + Rate Limiting (IPë‹¹ ë¶„ë‹¹ 5íšŒ)
    ì¤‘ìš”: consentê°€ Trueì¼ ë•Œë§Œ ì´ë©”ì¼ ë“±ë¡
    """
    state_manager = get_app_state_manager()

    if not settings.email:
        raise HTTPException(status_code=400, detail="ì´ë©”ì¼ ì£¼ì†Œê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    if settings.consent:
        # ì´ë©”ì¼ ë“±ë¡ (ëª…ì‹œì  ë™ì˜)
        success = state_manager.register_email(
            email=settings.email, consent=True, alert_types=settings.alert_types
        )

        if not success:
            raise HTTPException(status_code=400, detail="ì´ë©”ì¼ ë“±ë¡ ì‹¤íŒ¨")

        return {"status": "ok", "message": "ì•Œë¦¼ ì„¤ì •ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤."}
    else:
        # ë™ì˜ ì—†ìœ¼ë©´ ì—…ë°ì´íŠ¸ë§Œ (ì•Œë¦¼ ìœ í˜• ë³€ê²½)
        success = state_manager.update_email_subscription(
            email=settings.email, alert_types=settings.alert_types
        )

        return {"status": "ok", "message": "ì„¤ì •ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤."}


@app.post("/api/v3/alert-settings/revoke", dependencies=[Depends(verify_api_key)])
@limiter.limit("5/minute")  # ë¶„ë‹¹ 5íšŒ ì œí•œ
async def revoke_alert_consent(request: Request):
    """
    ì•Œë¦¼ ë™ì˜ ì² íšŒ

    ë³´ì•ˆ: API Key + Rate Limiting
    ì²« ë²ˆì§¸ ë“±ë¡ëœ ì´ë©”ì¼ì˜ ë™ì˜ë¥¼ ì² íšŒí•©ë‹ˆë‹¤.
    """
    state_manager = get_app_state_manager()
    subscriptions = state_manager.get_all_subscriptions()

    if not subscriptions:
        return {"status": "ok", "message": "ì² íšŒí•  ë™ì˜ê°€ ì—†ìŠµë‹ˆë‹¤."}

    # ì²« ë²ˆì§¸ ì´ë©”ì¼ ì² íšŒ
    email = next(iter(subscriptions.keys()))
    state_manager.revoke_email_consent(email)

    return {"status": "ok", "message": "ë™ì˜ê°€ ì² íšŒë˜ì—ˆìŠµë‹ˆë‹¤."}


# =============================================================================
# v4 Alert Settings API (ë‰´ë‹‰ ìŠ¤íƒ€ì¼ êµ¬ë… í”Œë¡œìš°)
# =============================================================================


@app.post("/api/v4/subscribe")
@limiter.limit("3/minute")
async def subscribe_v4(request: Request, body: SubscribeRequest):
    """
    êµ¬ë… ì‹œì‘ (v4 í†µí•© ì—”ë“œí¬ì¸íŠ¸)

    - ì‹ ê·œ ì´ë©”ì¼: JWT ì¸ì¦ ë©”ì¼ ë°œì†¡ + alert_types ì„ì‹œ ì €ì¥
    - ê¸°ì¡´ ì´ë©”ì¼ (already_verified): í˜„ì¬ êµ¬ë… ì„¤ì • ë°˜í™˜
    """
    import re

    email = body.email.strip()
    email_regex = r"^[^\s@]+@[^\s@]+\.[^\s@]+$"
    if not email or not re.match(email_regex, email):
        raise HTTPException(status_code=400, detail="ì˜¬ë°”ë¥¸ ì´ë©”ì¼ ì£¼ì†Œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    if not body.alert_types:
        raise HTTPException(status_code=400, detail="ìµœì†Œ í•˜ë‚˜ ì´ìƒì˜ ì•Œë¦¼ ìœ í˜•ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")

    state_manager = get_state_manager()
    existing = state_manager.get_subscription(email)

    # ì´ë¯¸ ì¸ì¦ëœ ì´ë©”ì¼
    if existing and existing.verified:
        return {
            "success": True,
            "already_verified": True,
            "message": "ì´ë¯¸ ê°€ì…í•œ ì´ë©”ì¼ì´ì—ìš”.",
            "current_settings": {
                "alert_types": existing.alert_types,
                "active": existing.active,
                "consent": existing.consent,
            },
        }

    # ì‹ ê·œ ì´ë©”ì¼ - JWT ì¸ì¦ ë©”ì¼ ë°œì†¡
    try:
        token = create_email_verification_token(email)

        base_url = get_base_url()
        verify_url = f"{base_url}/api/alerts/confirm-email?token={token}&email={email}"

        from src.tools.notifications.email_sender import EmailSender

        email_sender = EmailSender()

        if not email_sender.is_enabled():
            raise HTTPException(status_code=503, detail="ì´ë©”ì¼ ì„œë¹„ìŠ¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        result = await email_sender.send_verification_email(
            recipient=email, verify_url=verify_url, token=token
        )

        if result.success:
            # ì¸ì¦ ì „ì´ì§€ë§Œ ì„ íƒí•œ alert_typesë¥¼ ë¯¸ë¦¬ ì €ì¥ (ì¸ì¦ ì™„ë£Œ ì‹œ ì ìš©)
            if not existing:
                # ìƒˆ êµ¬ë… ìƒì„± (ì•„ì§ ë¯¸ì¸ì¦, ë¯¸ë™ì˜ ìƒíƒœ)
                sub = EmailSubscription(
                    email=email,
                    consent=False,
                    alert_types=body.alert_types,
                    active=False,
                    verified=False,
                )
                state_manager._email_subscriptions[email] = sub
                state_manager._save_subscriptions()
            else:
                # ê¸°ì¡´ ë¯¸ì¸ì¦ êµ¬ë… ì—…ë°ì´íŠ¸
                existing.alert_types = body.alert_types
                state_manager._save_subscriptions()

            logging.info(f"[v4] Verification email sent to {email}, alert_types={body.alert_types}")
            return {
                "success": True,
                "already_verified": False,
                "message": "ì¸ì¦ ì´ë©”ì¼ì´ ë°œì†¡ë˜ì—ˆìŠµë‹ˆë‹¤. (30ë¶„ ë‚´ ì¸ì¦í•´ì£¼ì„¸ìš”)",
            }
        else:
            raise HTTPException(status_code=500, detail=f"ì´ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨: {result.message}")

    except ValueError as e:
        logging.error(f"JWT configuration error: {e}")
        raise HTTPException(status_code=500, detail=str(e)) from e
    except HTTPException:
        raise
    except Exception as e:
        logging.error(f"[v4] Subscribe error: {e}")
        raise HTTPException(status_code=500, detail=str(e)) from e


@app.get("/api/v4/alert-settings")
async def get_alert_settings_v4(email: str | None = None):
    """
    ì•Œë¦¼ ì„¤ì • ì¡°íšŒ (v4)

    Args:
        email: ì¡°íšŒí•  ì´ë©”ì¼ (ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ êµ¬ë…ì)
    """
    state_manager = get_state_manager()

    if email:
        sub = state_manager.get_subscription(email)
        if not sub:
            return {"found": False, "email": email, "message": "ë“±ë¡ë˜ì§€ ì•Šì€ ì´ë©”ì¼ì…ë‹ˆë‹¤."}
        return {
            "found": True,
            "email": sub.email,
            "consent": sub.consent,
            "alert_types": sub.alert_types,
            "active": sub.active,
            "verified": sub.verified,
            "verified_at": sub.verified_at.isoformat() if sub.verified_at else None,
            "consent_date": sub.consent_date.isoformat() if sub.consent_date else None,
        }

    # email ë¯¸ì§€ì • ì‹œ ê¸°ì¡´ v3 ë™ì‘ (ì²« ë²ˆì§¸ êµ¬ë…ì)
    subscriptions = state_manager.get_all_subscriptions()
    if not subscriptions:
        return {"found": False, "email": "", "consent": False, "alert_types": []}

    email_key, sub = next(iter(subscriptions.items()))
    return {
        "found": True,
        "email": email_key,
        "consent": sub.consent,
        "alert_types": sub.alert_types,
        "active": sub.active,
        "verified": sub.verified,
        "verified_at": sub.verified_at.isoformat() if sub.verified_at else None,
        "consent_date": sub.consent_date.isoformat() if sub.consent_date else None,
    }


@app.put("/api/v4/alert-settings")
@limiter.limit("5/minute")
async def update_alert_settings_v4(request: Request, body: UpdateAlertSettingsRequest):
    """
    ì•Œë¦¼ ì„¤ì • ìˆ˜ì • (v4) - ê¸°ì¡´ êµ¬ë…ì ì „ìš©

    ì¸ì¦ ì™„ë£Œëœ ì´ë©”ì¼ë§Œ ìˆ˜ì • ê°€ëŠ¥
    """
    email = body.email.strip()
    if not email:
        raise HTTPException(status_code=400, detail="ì´ë©”ì¼ ì£¼ì†Œê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    state_manager = get_state_manager()
    sub = state_manager.get_subscription(email)

    if not sub:
        raise HTTPException(status_code=404, detail="ë“±ë¡ë˜ì§€ ì•Šì€ ì´ë©”ì¼ì…ë‹ˆë‹¤.")

    if not sub.verified:
        raise HTTPException(status_code=403, detail="ì´ë©”ì¼ ì¸ì¦ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    # alert_types ì—…ë°ì´íŠ¸
    success = state_manager.update_email_subscription(
        email=email, alert_types=body.alert_types, active=True
    )

    # consentë„ Trueë¡œ ì„¤ì • (ì„¤ì • ìˆ˜ì • = ë™ì˜ ìœ ì§€)
    if success and not sub.consent:
        sub.consent = True
        sub.consent_date = datetime.now()
        state_manager._save_subscriptions()

    if success:
        return {
            "status": "ok",
            "message": "ì•Œë¦¼ ì„¤ì •ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "alert_types": body.alert_types,
        }
    else:
        raise HTTPException(status_code=500, detail="ì„¤ì • ì—…ë°ì´íŠ¸ ì‹¤íŒ¨")


@app.delete("/api/v4/alert-settings")
@limiter.limit("5/minute")
async def delete_alert_settings_v4(request: Request, email: str):
    """
    êµ¬ë… í•´ì§€ (v4)

    Args:
        email: í•´ì§€í•  ì´ë©”ì¼ ì£¼ì†Œ
    """
    if not email:
        raise HTTPException(status_code=400, detail="ì´ë©”ì¼ ì£¼ì†Œê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    state_manager = get_state_manager()
    sub = state_manager.get_subscription(email)

    if not sub:
        raise HTTPException(status_code=404, detail="ë“±ë¡ë˜ì§€ ì•Šì€ ì´ë©”ì¼ì…ë‹ˆë‹¤.")

    state_manager.revoke_email_consent(email)
    return {"status": "ok", "message": "êµ¬ë…ì´ í•´ì§€ë˜ì—ˆìŠµë‹ˆë‹¤."}


@app.get("/api/v3/alerts")
async def get_alerts(limit: int = 50, alert_type: str | None = None):
    """
    ì•Œë¦¼ ëª©ë¡ ì¡°íšŒ

    Args:
        limit: ìµœëŒ€ ê°œìˆ˜
        alert_type: í•„í„°í•  ì•Œë¦¼ ìœ í˜•
    """
    from src.agents.alert_agent import AlertAgent

    state_manager = get_app_state_manager()
    alert_agent = AlertAgent(state_manager)

    return {
        "alerts": alert_agent.get_alerts(limit=limit, alert_type=alert_type),
        "pending_count": alert_agent.get_pending_count(),
        "stats": alert_agent.get_stats(),
    }


# ============= Level 4 Brain API (v4) =============


@app.post("/api/v4/chat", response_model=BrainChatResponse, dependencies=[Depends(verify_api_key)])
@limiter.limit("10/minute")  # ë¶„ë‹¹ 10íšŒ ì œí•œ (ë³´ì•ˆ ê°•í™”)
async def chat_v4(request: Request, body: BrainChatRequest):
    """
    Level 4 Brain ê¸°ë°˜ ì±—ë´‡ API (v4)

    LLM-First ì ‘ê·¼:
    - ëª¨ë“  íŒë‹¨ì„ LLMì´ ìˆ˜í–‰
    - ê·œì¹™ ê¸°ë°˜ ë¹ ë¥¸ ê²½ë¡œ ì—†ìŒ
    - RAG + KG í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
    - ììœ¨ ìŠ¤ì¼€ì¤„ëŸ¬ì™€ í†µí•©
    """
    import time

    start_time = time.time()

    message = body.message.strip()
    session_id = body.session_id or "default"

    if not message:
        raise HTTPException(status_code=400, detail="Message is required")

    try:
        # Brain ì¸ìŠ¤í„´ìŠ¤ íšë“
        brain = await get_initialized_brain()

        # í˜„ì¬ ë©”íŠ¸ë¦­ ë°ì´í„° ë¡œë“œ
        data = load_dashboard_data()
        current_metrics = data if data else None

        # Brainìœ¼ë¡œ ì²˜ë¦¬ (LLM-First)
        response = await brain.process_query(
            query=message,
            session_id=session_id,
            current_metrics=current_metrics,
            skip_cache=body.skip_cache,
        )

        processing_time = (time.time() - start_time) * 1000

        return BrainChatResponse(
            text=response.text,
            confidence=response.confidence_score,
            sources=response.sources if isinstance(response.sources, list) else [],
            reasoning=response.query_type,
            tools_used=response.tools_called,
            processing_time_ms=processing_time,
            from_cache=False,
            brain_mode=brain.mode.value,
            suggestions=response.suggestions,
            query_type=response.query_type,
        )

    except Exception as e:
        logging.error(f"Brain error: {e}")
        return BrainChatResponse(
            text=f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            confidence=0.0,
            sources=[],
            reasoning=None,
            tools_used=[],
            processing_time_ms=(time.time() - start_time) * 1000,
            from_cache=False,
            brain_mode="error",
        )


@app.post("/api/v4/chat/stream")
@limiter.limit("10/minute")
async def chat_v4_stream(request: Request, body: BrainChatRequest):
    """
    Level 4 Brain ê¸°ë°˜ SSE ìŠ¤íŠ¸ë¦¬ë° ì±—ë´‡ API (v4)

    v3ì˜ SSE ìŠ¤íŠ¸ë¦¬ë°ê³¼ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤ë¡œ v4 Brainì˜ ì²˜ë¦¬ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    ReAct + OWL + PromptGuard + ë„êµ¬ í˜¸ì¶œì„ ëª¨ë‘ ì§€ì›í•©ë‹ˆë‹¤.

    ì´ë²¤íŠ¸ íƒ€ì…:
    - status: ì²˜ë¦¬ ë‹¨ê³„ ì•Œë¦¼
    - tool_call: ë„êµ¬ í˜¸ì¶œ ì •ë³´
    - text: ì‘ë‹µ í…ìŠ¤íŠ¸
    - done: ì™„ë£Œ (ë©”íƒ€ë°ì´í„° í¬í•¨)
    - error: ì˜¤ë¥˜ ë°œìƒ
    """
    message = body.message.strip()
    session_id = body.session_id or "default"

    if not message:
        raise HTTPException(status_code=400, detail="Message is required")

    try:
        brain = await get_initialized_brain()
        data = load_dashboard_data()
        current_metrics = data if data else None

        async def generate():
            try:
                async for chunk in brain.process_query_stream(
                    query=message,
                    session_id=session_id,
                    current_metrics=current_metrics,
                ):
                    event_data = json.dumps(chunk, ensure_ascii=False)
                    yield f"data: {event_data}\n\n"
            except Exception as e:
                logger.error(f"v4 SSE stream error: {e}")
                error_data = json.dumps({"type": "error", "content": str(e)}, ensure_ascii=False)
                yield f"data: {error_data}\n\n"

        return StreamingResponse(
            generate(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no",
            },
        )

    except Exception as e:
        logger.error(f"v4 chat stream init error: {e}")
        raise HTTPException(status_code=500, detail=str(e)) from e


# ============= Amazon Deals API =============

from src.tools.scrapers.deals_scraper import get_deals_scraper


@app.get("/api/deals")
async def get_deals_data(brand: str | None = None, hours: int = 24, limit: int = 100):
    """
    ì €ì¥ëœ Deals ë°ì´í„° ì¡°íšŒ

    Args:
        brand: ë¸Œëœë“œ í•„í„° (ì„ íƒ)
        hours: ìµœê·¼ Nì‹œê°„ ë°ì´í„° (ê¸°ë³¸: 24ì‹œê°„)
        limit: ìµœëŒ€ ê°œìˆ˜

    Returns:
        - deals: ë”œ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        - summary: ìš”ì•½ í†µê³„
    """
    try:
        storage = get_sqlite_storage()
        await storage.initialize()

        # ê²½ìŸì‚¬ ë”œ ì¡°íšŒ
        deals = await storage.get_competitor_deals(brand=brand, hours=hours)

        # ìµœëŒ€ ê°œìˆ˜ ì œí•œ
        deals = deals[:limit] if len(deals) > limit else deals

        # ìš”ì•½ í†µê³„
        summary = await storage.get_deals_summary(days=7)

        return {
            "success": True,
            "deals": deals,
            "count": len(deals),
            "summary": summary,
            "filters": {"brand": brand, "hours": hours},
        }

    except Exception as e:
        logging.error(f"Deals data error: {e}")
        return {"success": False, "deals": [], "count": 0, "error": str(e)}


@app.get("/api/deals/summary")
async def get_deals_summary(days: int = 7):
    """
    Deals ìš”ì•½ í†µê³„

    Args:
        days: ë¶„ì„ ê¸°ê°„ (ì¼)

    Returns:
        - by_brand: ë¸Œëœë“œë³„ ë”œ í˜„í™©
        - by_date: ì¼ë³„ ì¶”ì´
    """
    try:
        storage = get_sqlite_storage()
        await storage.initialize()

        summary = await storage.get_deals_summary(days=days)

        return {"success": True, **summary}

    except Exception as e:
        logging.error(f"Deals summary error: {e}")
        return {"success": False, "error": str(e)}


@app.post("/api/deals/scrape", dependencies=[Depends(verify_api_key)])
async def scrape_deals(request: DealsRequest):
    """
    Amazon Deals í˜ì´ì§€ í¬ë¡¤ë§ (API Key í•„ìš”)

    ê²½ìŸì‚¬ í• ì¸ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤.

    Args:
        max_items: ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜
        beauty_only: ë·°í‹° ì¹´í…Œê³ ë¦¬ë§Œ í•„í„°ë§

    Returns:
        - deals: ìˆ˜ì§‘ëœ ë”œ ë°ì´í„°
        - competitor_deals: ê²½ìŸì‚¬ ë”œ
        - lightning_count: Lightning Deal ìˆ˜
    """
    try:
        scraper = await get_deals_scraper()

        # í¬ë¡¤ë§ ì‹¤í–‰
        result = await scraper.scrape_deals(
            max_items=request.max_items, beauty_only=request.beauty_only
        )

        if result["success"]:
            # SQLiteì— ì €ì¥
            storage = get_sqlite_storage()
            await storage.initialize()

            # ëª¨ë“  ë”œ ì €ì¥
            if result["deals"]:
                await storage.save_deals(result["deals"], is_competitor=False)

            # ê²½ìŸì‚¬ ë”œì€ is_competitor=Trueë¡œ ë³„ë„ ì €ì¥
            if result["competitor_deals"]:
                await storage.save_deals(result["competitor_deals"], is_competitor=True)

                # ì•Œë¦¼ ì„œë¹„ìŠ¤ë¡œ ì•Œë¦¼ ì²˜ë¦¬
                try:
                    alert_service = get_alert_service()
                    alerts = await alert_service.process_deals_for_alerts(
                        result["competitor_deals"]
                    )

                    # DBì— ì•Œë¦¼ ì €ì¥
                    for alert in alerts:
                        await storage.save_deal_alert(alert)

                    logging.info(
                        f"Processed {len(alerts)} alerts from {len(result['competitor_deals'])} competitor deals"
                    )
                except Exception as alert_err:
                    logging.error(f"Alert processing error: {alert_err}")
                    # ì•Œë¦¼ ì‹¤íŒ¨í•´ë„ í¬ë¡¤ë§ ê²°ê³¼ëŠ” ë°˜í™˜

            logging.info(
                f"Deals scraped: {result['count']} total, {len(result['competitor_deals'])} competitors"
            )

        return DealsResponse(
            success=result["success"],
            count=result["count"],
            lightning_count=result["lightning_count"],
            competitor_count=len(result["competitor_deals"]),
            snapshot_datetime=result["snapshot_datetime"],
            deals=result["deals"],
            competitor_deals=result["competitor_deals"],
            error=result.get("error"),
        )

    except Exception as e:
        logging.error(f"Deals scrape error: {e}")
        return DealsResponse(
            success=False,
            count=0,
            lightning_count=0,
            competitor_count=0,
            snapshot_datetime=datetime.now().isoformat(),
            deals=[],
            competitor_deals=[],
            error=str(e),
        )


@app.get("/api/deals/alerts")
async def get_deals_alerts(limit: int = 50, unsent_only: bool = False):
    """
    Deals ì•Œë¦¼ ëª©ë¡ ì¡°íšŒ

    Args:
        limit: ìµœëŒ€ ê°œìˆ˜
        unsent_only: ë¯¸ë°œì†¡ ì•Œë¦¼ë§Œ ì¡°íšŒ

    Returns:
        - alerts: ì•Œë¦¼ ëª©ë¡
        - count: ì´ ê°œìˆ˜
    """
    try:
        storage = get_sqlite_storage()
        await storage.initialize()

        if unsent_only:
            alerts = await storage.get_unsent_alerts(limit=limit)
        else:
            # ëª¨ë“  ì•Œë¦¼ ì¡°íšŒ (ìµœê·¼ 7ì¼)
            with storage.get_connection() as conn:
                cursor = conn.execute(
                    """
                    SELECT * FROM deals_alerts
                    ORDER BY alert_datetime DESC
                    LIMIT ?
                """,
                    (limit,),
                )
                alerts = [dict(row) for row in cursor.fetchall()]

        return {"success": True, "alerts": alerts, "count": len(alerts)}

    except Exception as e:
        logging.error(f"Deals alerts error: {e}")
        return {"success": False, "alerts": [], "count": 0, "error": str(e)}


@app.post("/api/deals/export")
async def export_deals_report(days: int = 7, format: str = "excel"):
    """
    Deals ë¦¬í¬íŠ¸ ë‚´ë³´ë‚´ê¸°

    Args:
        days: ë¶„ì„ ê¸°ê°„ (ì¼)
        format: ì¶œë ¥ í˜•ì‹ (excel, json)

    Returns:
        - ì—‘ì…€: íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        - JSON: ë°ì´í„° ë°˜í™˜
    """
    try:
        storage = get_sqlite_storage()
        await storage.initialize()

        if format == "json":
            # JSON í˜•ì‹ ë°˜í™˜
            summary = await storage.get_deals_summary(days=days)

            # ì „ì²´ ë”œ ë°ì´í„°
            with storage.get_connection() as conn:
                cutoff_date = (datetime.now() - timedelta(days=days)).strftime("%Y-%m-%d")
                cursor = conn.execute(
                    """
                    SELECT * FROM deals
                    WHERE DATE(snapshot_datetime) >= ?
                    ORDER BY snapshot_datetime DESC
                """,
                    (cutoff_date,),
                )
                all_deals = [dict(row) for row in cursor.fetchall()]

            return {
                "success": True,
                "summary": summary,
                "deals": all_deals,
                "export_date": datetime.now().isoformat(),
                "period_days": days,
            }

        else:  # Excel
            # ì—‘ì…€ íŒŒì¼ ìƒì„±
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = f"./data/exports/Deals_Report_{timestamp}.xlsx"

            result = storage.export_deals_report(output_path=output_path, days=days)

            if not result.get("success"):
                raise HTTPException(status_code=500, detail=result.get("error", "Export failed"))

            file_path = Path(result["file_path"])
            if not file_path.exists():
                raise HTTPException(status_code=500, detail="Generated file not found")

            return FileResponse(
                path=str(file_path),
                media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                headers={"Content-Disposition": f"attachment; filename={file_path.name}"},
            )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Deals export error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Deals ë‚´ë³´ë‚´ê¸° ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤") from e


# ============= ì•Œë¦¼ ì„œë¹„ìŠ¤ API =============

from src.tools.notifications.alert_service import get_alert_service


@app.get("/api/alerts/status")
async def get_alert_service_status():
    """ì•Œë¦¼ ì„œë¹„ìŠ¤ ìƒíƒœ ì¡°íšŒ"""
    try:
        service = get_alert_service()
        return {"success": True, **service.get_status()}
    except Exception as e:
        logging.error(f"Alert service status error: {e}")
        return {"success": False, "error": str(e)}


@app.post("/api/alerts/send")
async def send_pending_alerts(request: AlertSendRequest | None = None):
    """
    ë¯¸ë°œì†¡ ì•Œë¦¼ ë°œì†¡

    íŠ¹ì • alert_idsë¥¼ ì§€ì •í•˜ë©´ í•´ë‹¹ ì•Œë¦¼ë§Œ, ì—†ìœ¼ë©´ ë¯¸ë°œì†¡ ì „ì²´ ë°œì†¡
    """
    try:
        storage = get_sqlite_storage()
        await storage.initialize()

        alert_service = get_alert_service()

        # ë¯¸ë°œì†¡ ì•Œë¦¼ ì¡°íšŒ
        unsent_alerts = await storage.get_unsent_alerts(limit=50)

        if not unsent_alerts:
            return {"success": True, "message": "No pending alerts to send", "sent_count": 0}

        # íŠ¹ì • ID í•„í„°ë§
        if request and request.alert_ids:
            unsent_alerts = [a for a in unsent_alerts if a.get("id") in request.alert_ids]

        if not unsent_alerts:
            return {"success": True, "message": "No matching alerts found", "sent_count": 0}

        # ì•Œë¦¼ ë°œì†¡
        sent_count = 0
        for alert in unsent_alerts:
            result = await alert_service.send_single_alert(alert)

            # ì„±ê³µ ì‹œ ë°œì†¡ ì™„ë£Œ í‘œì‹œ
            if result.get("slack") or result.get("email"):
                await storage.mark_alert_sent(alert["id"])
                sent_count += 1

        return {
            "success": True,
            "sent_count": sent_count,
            "total_pending": len(unsent_alerts),
            "channels": {
                "slack": alert_service._slack_enabled,
                "email": alert_service._email_enabled,
            },
        }

    except Exception as e:
        logging.error(f"Alert send error: {e}")
        return {"success": False, "error": str(e), "sent_count": 0}


@app.post("/api/alerts/test")
async def send_test_alert():
    """í…ŒìŠ¤íŠ¸ ì•Œë¦¼ ë°œì†¡"""
    try:
        alert_service = get_alert_service()

        test_alert = {
            "alert_datetime": datetime.now().isoformat(),
            "brand": "TEST BRAND",
            "asin": "B000TEST01",
            "product_name": "Test Product - Alert System Verification",
            "deal_type": "lightning",
            "discount_percent": 50.0,
            "deal_price": 19.99,
            "original_price": 39.99,
            "time_remaining": "2h 30m",
            "claimed_percent": 45,
            "product_url": "https://amazon.com/dp/B000TEST01",
            "alert_type": "lightning_deal",
            "alert_message": "Test Alert - ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì•Œë¦¼ì…ë‹ˆë‹¤",
        }

        result = await alert_service.send_single_alert(test_alert)

        return {
            "success": True,
            "test_alert": test_alert,
            "send_result": result,
            "message": "Test alert sent successfully"
            if any(result.values())
            else "No channels enabled",
        }

    except Exception as e:
        logging.error(f"Test alert error: {e}")
        return {"success": False, "error": str(e)}


# ============= Email Verification API =============

import jwt

# JWT ì„¤ì •
JWT_SECRET_KEY = os.getenv("JWT_SECRET_KEY")
JWT_ALGORITHM = "HS256"
EMAIL_VERIFICATION_EXPIRES_MINUTES = 30  # 30ë¶„ ë§Œë£Œ


def create_email_verification_token(
    email: str, expires_minutes: int = EMAIL_VERIFICATION_EXPIRES_MINUTES
) -> str:
    """
    ì´ë©”ì¼ ì¸ì¦ìš© JWT í† í° ìƒì„±

    Args:
        email: ì¸ì¦í•  ì´ë©”ì¼ ì£¼ì†Œ
        expires_minutes: í† í° ë§Œë£Œ ì‹œê°„ (ë¶„)

    Returns:
        JWT í† í° ë¬¸ìì—´
    """
    if not JWT_SECRET_KEY:
        raise ValueError("JWT_SECRET_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    payload = {
        "email": email,
        "purpose": "email_verification",
        "exp": datetime.now(UTC) + timedelta(minutes=expires_minutes),
        "iat": datetime.now(UTC),
    }
    return jwt.encode(payload, JWT_SECRET_KEY, algorithm=JWT_ALGORITHM)


def verify_jwt_email_token(token: str) -> dict:
    """
    JWT ì´ë©”ì¼ ì¸ì¦ í† í° ê²€ì¦

    Args:
        token: JWT í† í°

    Returns:
        {"valid": True, "email": "..."} ë˜ëŠ” {"valid": False, "error": "..."}
    """
    if not JWT_SECRET_KEY:
        return {"valid": False, "error": "JWT_SECRET_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}

    try:
        payload = jwt.decode(token, JWT_SECRET_KEY, algorithms=[JWT_ALGORITHM])

        # purpose ê²€ì¦
        if payload.get("purpose") != "email_verification":
            return {"valid": False, "error": "ìœ íš¨í•˜ì§€ ì•Šì€ í† í°ì…ë‹ˆë‹¤."}

        return {"valid": True, "email": payload["email"]}

    except jwt.ExpiredSignatureError:
        return {"valid": False, "error": "ì¸ì¦ í† í°ì´ ë§Œë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì¸ì¦í•´ì£¼ì„¸ìš”."}
    except jwt.InvalidTokenError:
        return {"valid": False, "error": "ìœ íš¨í•˜ì§€ ì•Šì€ ì¸ì¦ í† í°ì…ë‹ˆë‹¤."}


@app.post("/api/alerts/send-verification")
@limiter.limit("3/minute")  # ë¶„ë‹¹ 3íšŒ ì œí•œ (ìŠ¤íŒ¸ ë°©ì§€)
async def send_verification_email(request: Request):
    """
    ì´ë©”ì¼ ì¸ì¦ ìš”ì²­ - ì¸ì¦ ì´ë©”ì¼ ë°œì†¡ (JWT ë°©ì‹)

    ë³´ì•ˆ: Rate Limitìœ¼ë¡œ ìŠ¤íŒ¸ ë°©ì§€ (ë¶„ë‹¹ 3íšŒ)
    ì‚¬ìš©ìê°€ ì´ë©”ì¼ì„ ì…ë ¥í•˜ê³  'ì¸ì¦í•˜ê¸°' ë²„íŠ¼ì„ ëˆ„ë¥´ë©´
    í•´ë‹¹ ì´ë©”ì¼ë¡œ JWT í† í°ì´ í¬í•¨ëœ ì¸ì¦ ë§í¬ë¥¼ ë°œì†¡í•©ë‹ˆë‹¤.

    JWT í† í°ì€ 30ë¶„ê°„ ìœ íš¨í•˜ë©°, ì„œë²„ ì¬ì‹œì‘ê³¼ ë¬´ê´€í•˜ê²Œ ê²€ì¦ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    """
    try:
        body = await request.json()
        email = body.get("email", "").strip()

        # ì´ë©”ì¼ í˜•ì‹ ê²€ì¦
        import re

        email_regex = r"^[^\s@]+@[^\s@]+\.[^\s@]+$"
        if not email or not re.match(email_regex, email):
            raise HTTPException(status_code=400, detail="ì˜¬ë°”ë¥¸ ì´ë©”ì¼ ì£¼ì†Œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

        # ì´ë¯¸ ì¸ì¦ëœ ì´ë©”ì¼ì¸ì§€ í™•ì¸
        state_manager = get_state_manager()
        existing = state_manager.get_subscription(email)
        if existing and existing.verified:
            return {
                "success": True,
                "already_verified": True,
                "message": "ì´ë¯¸ ì¸ì¦ ì™„ë£Œëœ ì´ë©”ì¼ì…ë‹ˆë‹¤.",
            }

        # JWT í† í° ìƒì„± (30ë¶„ ìœ íš¨)
        token = create_email_verification_token(email)

        # ì¸ì¦ ì „ìš© í˜ì´ì§€ URL ìƒì„± (ëŒ€ì‹œë³´ë“œ ëŒ€ì‹  ì „ìš© í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸)
        base_url = get_base_url()
        verify_url = f"{base_url}/api/alerts/confirm-email?token={token}&email={email}"

        # EmailSender ì§ì ‘ ì‚¬ìš©
        from src.tools.notifications.email_sender import EmailSender

        email_sender = EmailSender()

        if not email_sender.is_enabled():
            raise HTTPException(status_code=503, detail="ì´ë©”ì¼ ì„œë¹„ìŠ¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # ì¸ì¦ ì´ë©”ì¼ ë°œì†¡
        result = await email_sender.send_verification_email(
            recipient=email, verify_url=verify_url, token=token
        )

        if result.success:
            logging.info(
                f"Verification email sent to {email} (JWT, expires in {EMAIL_VERIFICATION_EXPIRES_MINUTES}min)"
            )
            return {
                "success": True,
                "message": "ì¸ì¦ ì´ë©”ì¼ì´ ë°œì†¡ë˜ì—ˆìŠµë‹ˆë‹¤. (30ë¶„ ë‚´ ì¸ì¦í•´ì£¼ì„¸ìš”)",
            }
        else:
            raise HTTPException(status_code=500, detail=f"ì´ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨: {result.message}")

    except ValueError as e:
        # JWT_SECRET_KEY ë¯¸ì„¤ì • ì—ëŸ¬
        logging.error(f"JWT configuration error: {e}")
        raise HTTPException(status_code=500, detail=str(e)) from e
    except HTTPException:
        raise
    except Exception as e:
        logging.error(f"Send verification email error: {e}")
        raise HTTPException(status_code=500, detail=str(e)) from e


@app.post("/api/alerts/verify-email")
@limiter.limit("10/minute")  # ë¶„ë‹¹ 10íšŒ ì œí•œ (brute force ë°©ì§€)
async def verify_email_token_endpoint(request: Request):
    """
    ì´ë©”ì¼ ì¸ì¦ í† í° ê²€ì¦ (JWT ë°©ì‹)

    ë³´ì•ˆ: Rate Limitìœ¼ë¡œ brute force ë°©ì§€ (ë¶„ë‹¹ 10íšŒ)
    ì‚¬ìš©ìê°€ ì´ë©”ì¼ì˜ ì¸ì¦ ë²„íŠ¼ì„ í´ë¦­í•˜ë©´
    JWT í† í°ì„ ê²€ì¦í•˜ê³  ì´ë©”ì¼ ì¸ì¦ ìƒíƒœë¥¼ StateManagerì— ì˜êµ¬ ì €ì¥í•©ë‹ˆë‹¤.

    JWT í† í°ì€ statelessì´ë¯€ë¡œ ì„œë²„ ì¬ì‹œì‘ê³¼ ë¬´ê´€í•˜ê²Œ ê²€ì¦ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    """
    try:
        body = await request.json()
        token = body.get("token", "")
        email = body.get("email", "").strip()

        if not token or not email:
            raise HTTPException(status_code=400, detail="í† í°ê³¼ ì´ë©”ì¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")

        # JWT í† í° ê²€ì¦
        result = verify_jwt_email_token(token)

        if not result["valid"]:
            raise HTTPException(status_code=400, detail=result["error"])

        # í† í°ì˜ ì´ë©”ì¼ê³¼ ìš”ì²­ ì´ë©”ì¼ ì¼ì¹˜ í™•ì¸
        token_email = result["email"]
        if token_email != email:
            raise HTTPException(status_code=400, detail="ì´ë©”ì¼ì´ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

        # StateManagerì— ì¸ì¦ ì™„ë£Œ ìƒíƒœ ì˜êµ¬ ì €ì¥
        try:
            state_manager = get_state_manager()

            # ê¸°ì¡´ êµ¬ë… ì •ë³´ í™•ì¸
            existing = state_manager.get_subscription(email)

            if existing:
                # ê¸°ì¡´ êµ¬ë…ì´ ìˆìœ¼ë©´ verified ìƒíƒœ ì—…ë°ì´íŠ¸ + í™œì„±í™”
                existing.verified = True
                existing.verified_at = datetime.now()
                existing.consent = True
                existing.consent_date = datetime.now()
                existing.active = True
                state_manager._save_subscriptions()
            else:
                # ìƒˆ êµ¬ë… ë“±ë¡ (verified=Trueë¡œ ìƒì„±)
                state_manager.register_email(
                    email=email,
                    consent=True,
                    alert_types=["rank_change", "important_insight", "daily_summary"],
                )
                # verified ìƒíƒœ ì¶”ê°€ ì„¤ì •
                subscription = state_manager.get_subscription(email)
                if subscription:
                    subscription.verified = True
                    subscription.verified_at = datetime.now()
                    state_manager._save_subscriptions()

            logging.info(f"Email verified and saved to StateManager: {email}")
        except Exception as e:
            logging.warning(f"Failed to save verification status: {e}")

        return {"verified": True, "email": email, "message": "ì´ë©”ì¼ ì¸ì¦ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!"}

    except HTTPException:
        raise
    except Exception as e:
        logging.error(f"Verify email error: {e}")
        raise HTTPException(status_code=500, detail=str(e)) from e


@app.get("/api/alerts/confirm-email")
async def confirm_email_page(token: str, email: str):
    """
    ì´ë©”ì¼ ì¸ì¦ í™•ì¸ í˜ì´ì§€ (GET ìš”ì²­ìœ¼ë¡œ ì ‘ê·¼)

    ì‚¬ìš©ìê°€ ì´ë©”ì¼ì˜ ì¸ì¦ ë§í¬ë¥¼ í´ë¦­í•˜ë©´ ì´ í˜ì´ì§€ê°€ í‘œì‹œë©ë‹ˆë‹¤.
    í† í°ì„ ê²€ì¦í•˜ê³  ì¸ì¦ ì™„ë£Œ ìƒíƒœë¥¼ ì €ì¥í•œ í›„, ì°½ì„ ë‹«ì•„ë„ ë˜ëŠ” ì•ˆë‚´ í˜ì´ì§€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì›ë˜ ëŒ€ì‹œë³´ë“œ íƒ­ì€ í´ë§ìœ¼ë¡œ ì¸ì¦ ì™„ë£Œë¥¼ ê°ì§€í•˜ì—¬ ìë™ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì´ë™í•©ë‹ˆë‹¤.
    """
    from fastapi.responses import HTMLResponse

    # JWT í† í° ê²€ì¦
    result = verify_jwt_email_token(token)

    if not result["valid"]:
        error_html = f"""
        <!DOCTYPE html>
        <html lang="ko">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>ì¸ì¦ ì‹¤íŒ¨ - AMORE Pacific</title>
            <style>
                * {{ margin: 0; padding: 0; box-sizing: border-box; }}
                body {{
                    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
                    background: linear-gradient(135deg, #001C58 0%, #1F5795 100%);
                    min-height: 100vh;
                    display: flex;
                    align-items: center;
                    justify-content: center;
                    padding: 20px;
                }}
                .card {{
                    background: white;
                    border-radius: 20px;
                    padding: 48px;
                    max-width: 420px;
                    width: 100%;
                    text-align: center;
                    box-shadow: 0 20px 60px rgba(0,0,0,0.3);
                }}
                .icon {{
                    width: 80px;
                    height: 80px;
                    background: #fee2e2;
                    border-radius: 50%;
                    display: flex;
                    align-items: center;
                    justify-content: center;
                    margin: 0 auto 24px;
                }}
                .icon svg {{ width: 40px; height: 40px; color: #ef4444; }}
                h1 {{ color: #001C58; font-size: 24px; margin-bottom: 12px; }}
                p {{ color: #64748b; font-size: 15px; line-height: 1.6; }}
                .error-msg {{ color: #ef4444; font-size: 13px; margin-top: 16px; padding: 12px; background: #fef2f2; border-radius: 8px; }}
            </style>
        </head>
        <body>
            <div class="card">
                <div class="icon">
                    <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12"/>
                    </svg>
                </div>
                <h1>ì¸ì¦ ì‹¤íŒ¨</h1>
                <p>ì´ë©”ì¼ ì¸ì¦ ë§í¬ê°€ ë§Œë£Œë˜ì—ˆê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.</p>
                <div class="error-msg">{result.get('error', 'í† í°ì´ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.')}</div>
                <p style="margin-top: 20px; font-size: 13px;">ëŒ€ì‹œë³´ë“œì—ì„œ ë‹¤ì‹œ ì¸ì¦ì„ ìš”ì²­í•´ì£¼ì„¸ìš”.</p>
            </div>
        </body>
        </html>
        """
        return HTMLResponse(content=error_html, status_code=400)

    # í† í°ì˜ ì´ë©”ì¼ê³¼ ìš”ì²­ ì´ë©”ì¼ ì¼ì¹˜ í™•ì¸
    token_email = result["email"]
    if token_email != email:
        return HTMLResponse(content="ì´ë©”ì¼ì´ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.", status_code=400)

    # StateManagerì— ì¸ì¦ ì™„ë£Œ ìƒíƒœ ì €ì¥
    try:
        state_manager = get_state_manager()
        existing = state_manager.get_subscription(email)

        if existing:
            existing.verified = True
            existing.verified_at = datetime.now()
            existing.consent = True
            existing.consent_date = datetime.now()
            existing.active = True
            state_manager._save_subscriptions()
        else:
            state_manager.register_email(
                email=email,
                consent=True,
                alert_types=["rank_change", "important_insight", "daily_summary"],
            )
            subscription = state_manager.get_subscription(email)
            if subscription:
                subscription.verified = True
                subscription.verified_at = datetime.now()
                state_manager._save_subscriptions()

        logging.info(f"Email verified via confirm page: {email}")
    except Exception as e:
        logging.warning(f"Failed to save verification status: {e}")

    # ì¸ì¦ ì„±ê³µ í˜ì´ì§€ ë°˜í™˜
    success_html = f"""
    <!DOCTYPE html>
    <html lang="ko">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>ì¸ì¦ ì™„ë£Œ - AMORE Pacific</title>
        <style>
            * {{ margin: 0; padding: 0; box-sizing: border-box; }}
            body {{
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
                background: linear-gradient(135deg, #001C58 0%, #1F5795 100%);
                min-height: 100vh;
                display: flex;
                align-items: center;
                justify-content: center;
                padding: 20px;
            }}
            .card {{
                background: white;
                border-radius: 20px;
                padding: 48px;
                max-width: 420px;
                width: 100%;
                text-align: center;
                box-shadow: 0 20px 60px rgba(0,0,0,0.3);
            }}
            .icon {{
                width: 80px;
                height: 80px;
                background: #d1fae5;
                border-radius: 50%;
                display: flex;
                align-items: center;
                justify-content: center;
                margin: 0 auto 24px;
                animation: pulse 2s infinite;
            }}
            @keyframes pulse {{
                0%, 100% {{ transform: scale(1); }}
                50% {{ transform: scale(1.05); }}
            }}
            .icon svg {{ width: 40px; height: 40px; color: #10b981; }}
            h1 {{ color: #001C58; font-size: 24px; margin-bottom: 12px; }}
            p {{ color: #64748b; font-size: 15px; line-height: 1.6; }}
            .email {{
                color: #1F5795;
                font-weight: 600;
                background: #f0f9ff;
                padding: 8px 16px;
                border-radius: 8px;
                display: inline-block;
                margin: 16px 0;
            }}
            .hint {{
                margin-top: 24px;
                padding: 16px;
                background: #f8fafc;
                border-radius: 12px;
                font-size: 13px;
                color: #475569;
            }}
            .close-btn {{
                margin-top: 24px;
                padding: 14px 32px;
                background: #001C58;
                color: white;
                border: none;
                border-radius: 10px;
                font-size: 15px;
                font-weight: 600;
                cursor: pointer;
                transition: background 0.2s;
            }}
            .close-btn:hover {{ background: #1F5795; }}
        </style>
    </head>
    <body>
        <div class="card">
            <div class="icon">
                <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 13l4 4L19 7"/>
                </svg>
            </div>
            <h1>ì´ë©”ì¼ ì¸ì¦ ì™„ë£Œ!</h1>
            <div class="email">{email}</div>
            <p>ì´ë©”ì¼ ì£¼ì†Œê°€ ì„±ê³µì ìœ¼ë¡œ ì¸ì¦ë˜ì—ˆìŠµë‹ˆë‹¤.</p>
            <div class="hint">
                ì´ ì°½ì€ ë‹«ì•„ë„ ë©ë‹ˆë‹¤.<br>
                ì›ë˜ ëŒ€ì‹œë³´ë“œ í™”ë©´ì—ì„œ ìë™ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì´ë™í•©ë‹ˆë‹¤.
            </div>
            <button class="close-btn" onclick="window.close()">ì´ ì°½ ë‹«ê¸°</button>
        </div>
    </body>
    </html>
    """
    return HTMLResponse(content=success_html)


@app.get("/api/alerts/verification-status")
async def get_verification_status(email: str):
    """
    ì´ë©”ì¼ ì¸ì¦ ìƒíƒœ í™•ì¸ (StateManager ê¸°ë°˜)

    JWT ë°©ì‹ìœ¼ë¡œ ë³€ê²½ë˜ì–´ ì¸ì¦ ì™„ë£Œ ìƒíƒœëŠ” StateManagerì— ì˜êµ¬ ì €ì¥ë©ë‹ˆë‹¤.
    """
    try:
        state_manager = get_state_manager()
        subscription = state_manager.get_subscription(email)

        if subscription:
            return {
                "verified": subscription.verified,
                "status": "verified" if subscription.verified else "pending",
                "verified_at": subscription.verified_at.isoformat()
                if subscription.verified_at
                else None,
            }

        return {"verified": False, "status": "not_found"}

    except Exception as e:
        logging.error(f"Get verification status error: {e}")
        return {"verified": False, "status": "error", "error": str(e)}


# ============= Insight Email API =============


@app.post("/api/alerts/send-insight-report")
async def send_insight_report_email(request: Request):
    """
    ì¸ì‚¬ì´íŠ¸ ë¦¬í¬íŠ¸ ì´ë©”ì¼ ë°œì†¡ (ìˆ˜ë™)

    ëŒ€ì‹œë³´ë“œì—ì„œ 'ì´ë©”ì¼ë¡œ ë³´ë‚´ê¸°' ë²„íŠ¼ í´ë¦­ ì‹œ í˜¸ì¶œë©ë‹ˆë‹¤.
    í˜„ì¬ ì¸ì‚¬ì´íŠ¸ì™€ KPI ë°ì´í„°ë¥¼ ì´ë©”ì¼ë¡œ ë°œì†¡í•©ë‹ˆë‹¤.

    StateManager ê¸°ë°˜ ì¸ì¦ ìƒíƒœ í™•ì¸ (JWT ë°©ì‹ ë³€ê²½ì— ë”°ë¥¸ ì—…ë°ì´íŠ¸)
    """
    try:
        body = await request.json()
        recipient_email = body.get("email", "").strip()

        if not recipient_email:
            raise HTTPException(status_code=400, detail="ì´ë©”ì¼ ì£¼ì†Œê°€ í•„ìš”í•©ë‹ˆë‹¤.")

        # StateManagerì—ì„œ ì´ë©”ì¼ ì¸ì¦ ìƒíƒœ í™•ì¸
        state_manager = get_state_manager()
        subscription = state_manager.get_subscription(recipient_email)

        if not subscription or not subscription.verified:
            raise HTTPException(
                status_code=403, detail="ì´ë©”ì¼ ì¸ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤. ë¨¼ì € ì´ë©”ì¼ì„ ì¸ì¦í•´ì£¼ì„¸ìš”."
            )

        # EmailSender ì´ˆê¸°í™”
        from src.tools.notifications.email_sender import EmailSender

        email_sender = EmailSender()

        if not email_sender.is_enabled():
            raise HTTPException(status_code=503, detail="ì´ë©”ì¼ ì„œë¹„ìŠ¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # í˜„ì¬ ëŒ€ì‹œë³´ë“œ ë°ì´í„° ë¡œë“œ
        dashboard_data = load_dashboard_data()
        if not dashboard_data:
            raise HTTPException(status_code=404, detail="ëŒ€ì‹œë³´ë“œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        # KPI ê³„ì‚°
        products = dashboard_data.get("products", [])
        laneige_products = [p for p in products if p.get("brand") == "LANEIGE"]
        avg_rank = (
            sum(p.get("rank", 100) for p in laneige_products) / len(laneige_products)
            if laneige_products
            else 0
        )

        # SoS ê³„ì‚° (Top 100 ê¸°ì¤€)
        top100 = products[:100]
        laneige_in_top100 = len([p for p in top100 if p.get("brand") == "LANEIGE"])
        sos = (laneige_in_top100 / len(top100) * 100) if top100 else 0

        # HHI ê³„ì‚°
        brand_counts = {}
        for p in top100:
            brand = p.get("brand", "Unknown")
            brand_counts[brand] = brand_counts.get(brand, 0) + 1
        hhi = (
            sum((count / len(top100) * 100) ** 2 for count in brand_counts.values())
            if top100
            else 0
        )

        # ì¸ì‚¬ì´íŠ¸ ê°€ì ¸ì˜¤ê¸° (ìºì‹œëœ ê²ƒ ë˜ëŠ” ìƒˆë¡œ ìƒì„±)
        insight_content = dashboard_data.get("latest_insight", "")
        if not insight_content:
            insight_content = (
                "<p>í˜„ì¬ ìƒì„±ëœ ì¸ì‚¬ì´íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ëŒ€ì‹œë³´ë“œì—ì„œ ì¸ì‚¬ì´íŠ¸ë¥¼ ë¨¼ì € ìƒì„±í•´ì£¼ì„¸ìš”.</p>"
            )
        else:
            # ë§ˆí¬ë‹¤ìš´ì„ HTMLë¡œ ê°„ë‹¨ ë³€í™˜
            insight_content = insight_content.replace("\n\n", "</p><p>").replace("\n", "<br>")
            insight_content = f"<p>{insight_content}</p>"

        # Top 10 ì œí’ˆ ë°ì´í„°
        top10_products = []
        for i, p in enumerate(products[:10]):
            top10_products.append(
                {
                    "rank": i + 1,
                    "name": p.get("title", "N/A"),
                    "brand": p.get("brand", "Unknown"),
                    "change": p.get("rank_change", 0),
                }
            )

        # ë¸Œëœë“œë³„ ë³€ë™
        brand_changes = []
        for brand in ["LANEIGE", "e.l.f.", "Maybelline", "Summer Fridays", "COSRX"]:
            brand_products = [p for p in products if p.get("brand") == brand]
            if brand_products:
                avg_change = sum(p.get("rank_change", 0) for p in brand_products) / len(
                    brand_products
                )
                if avg_change > 0:
                    brand_changes.append(
                        {
                            "brand": brand,
                            "change_text": f"í‰ê·  â–²{avg_change:.1f} ìƒìŠ¹",
                            "color": "#28a745",
                        }
                    )
                elif avg_change < 0:
                    brand_changes.append(
                        {
                            "brand": brand,
                            "change_text": f"í‰ê·  â–¼{abs(avg_change):.1f} í•˜ë½",
                            "color": "#dc3545",
                        }
                    )

        # ë¦¬í¬íŠ¸ ë‚ ì§œ
        report_date = datetime.now().strftime("%Yë…„ %mì›” %dì¼")

        # ëŒ€ì‹œë³´ë“œ URL (Railway ìë™ ê°ì§€)
        dashboard_url = get_base_url() + "/dashboard"

        # ì´ë©”ì¼ ë°œì†¡
        result = await email_sender.send_insight_report(
            recipients=[recipient_email],
            report_date=report_date,
            avg_rank=avg_rank,
            sos=sos,
            hhi=hhi,
            insight_content=insight_content,
            top10_products=top10_products,
            brand_changes=brand_changes,
            dashboard_url=dashboard_url,
        )

        if result.success:
            logging.info(f"Insight report sent to {recipient_email}")
            return {
                "success": True,
                "message": f"ì¸ì‚¬ì´íŠ¸ ë¦¬í¬íŠ¸ê°€ {recipient_email}ë¡œ ë°œì†¡ë˜ì—ˆìŠµë‹ˆë‹¤.",
                "sent_to": result.sent_to,
            }
        else:
            raise HTTPException(status_code=500, detail=f"ì´ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨: {result.message}")

    except HTTPException:
        raise
    except Exception as e:
        logging.error(f"Send insight report error: {e}")
        raise HTTPException(status_code=500, detail=str(e)) from e


# ============= ì„œë²„ ì‹¤í–‰ =============

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8001)
