"""
Dashboard API Server
====================
ëŒ€ì‹œë³´ë“œìš© FastAPI ë°±ì—”ë“œ ì„œë²„ (ë©”ì¸ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸)

## í•µì‹¬ ê¸°ëŠ¥
- ì±—ë´‡ API (ChatGPT + RAG + Ontology ì—°ë™)
- DOCX ì¸ì‚¬ì´íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±
- ëŒ€í™” ë©”ëª¨ë¦¬ ì§€ì› (ì„¸ì…˜ë³„ TTL ê¸°ë°˜)
- Audit Trail ë¡œê¹…

## ì•„í‚¤í…ì²˜ íë¦„
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           FastAPI Server                                â”‚
â”‚   dashboard_api.py (PORT 8001)                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  /api/chat â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º HybridChatbotAgent â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º LLM (GPT-4.1)  â”‚
â”‚                                   â”‚                                     â”‚
â”‚                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚                         â–¼                   â–¼                          â”‚
â”‚                  KnowledgeGraph      DocumentRetriever                  â”‚
â”‚                  (ì˜¨í†¨ë¡œì§€)          (RAG ê°€ì´ë“œë¼ì¸)                   â”‚
â”‚                                                                         â”‚
â”‚  /api/crawl/start â”€â”€â”€â”€â–º UnifiedBrain â”€â”€â”€â”€â–º AmazonScraper               â”‚
â”‚                              â”‚              (Playwright)                â”‚
â”‚                              â–¼                                          â”‚
â”‚                        MetricCalculator                                 â”‚
â”‚                              â”‚                                          â”‚
â”‚                              â–¼                                          â”‚
â”‚                       SheetsWriter / SQLite                             â”‚
â”‚                                                                         â”‚
â”‚  /api/data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º dashboard_data.json (ìºì‹œëœ ë°ì´í„°)            â”‚
â”‚                                                                         â”‚
â”‚  /dashboard â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º amore_unified_dashboard_v4.html                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ì£¼ìš” ì—”ë“œí¬ì¸íŠ¸
- GET  /           : í—¬ìŠ¤ì²´í¬
- GET  /api/data   : ëŒ€ì‹œë³´ë“œ ë°ì´í„° JSON
- POST /api/chat   : ì±—ë´‡ v1 (RAG)
- POST /api/v2/chat: ì±—ë´‡ v2 (Unified Brain)
- POST /api/v3/chat: ì±—ë´‡ v3 (Simple Chat)
- POST /api/crawl/start: í¬ë¡¤ë§ ì‹œì‘ (API Key í•„ìš”)
- GET  /dashboard  : ëŒ€ì‹œë³´ë“œ UI

## í™˜ê²½ ë³€ìˆ˜
- OPENAI_API_KEY: OpenAI API í‚¤ (í•„ìˆ˜)
- API_KEY: ë³´í˜¸ëœ ì—”ë“œí¬ì¸íŠ¸ìš© ì¸ì¦í‚¤
- AUTO_START_SCHEDULER: ì„œë²„ ì‹œì‘ ì‹œ ìŠ¤ì¼€ì¤„ëŸ¬ ìë™ ì‹œì‘ (default: true)
"""

import asyncio
import json
import logging
import os
from collections import defaultdict
from datetime import datetime
from io import BytesIO
from pathlib import Path
from typing import Any

from docx import Document
from docx.enum.table import WD_TABLE_ALIGNMENT
from docx.enum.text import WD_ALIGN_PARAGRAPH
from docx.shared import Pt
from dotenv import load_dotenv
from fastapi import Depends, FastAPI, HTTPException, Request, Security
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse, HTMLResponse, StreamingResponse
from fastapi.security import APIKeyHeader
from fastapi.staticfiles import StaticFiles
from litellm import acompletion
from pydantic import BaseModel, Field

# Rate Limiting
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.errors import RateLimitExceeded
from slowapi.util import get_remote_address
from starlette.middleware.base import BaseHTTPMiddleware

# Export Routes (DOCX, Excel ë¹„ë™ê¸° ë‚´ë³´ë‚´ê¸°)
from src.api.routes.export import router as export_router

# External Signal Routes
from src.api.routes.signals import router as signals_router

# Ontology ìŠ¤í‚¤ë§ˆ
# í†µí•© ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° (deprecated - use UnifiedBrain instead)
# Level 4 Brain (LLM-First Autonomous Agent)
from src.core.brain import BrainMode, get_brain, get_initialized_brain
from src.core.crawl_manager import get_crawl_manager
from src.rag.retriever import DocumentRetriever

# RAG ì‹œìŠ¤í…œ ì—°ë™
from src.rag.router import QueryType, RAGRouter

logger = logging.getLogger(__name__)

# Market Intelligence Engine
from src.tools.market_intelligence import MarketIntelligenceEngine

# SQLite Storage
from src.tools.sqlite_storage import get_sqlite_storage

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# Rate Limiter ì„¤ì • (IP ê¸°ë°˜)
limiter = Limiter(key_func=get_remote_address)

app = FastAPI(
    title="AMORE Dashboard API",
    description="LANEIGE Amazon ëŒ€ì‹œë³´ë“œ ë°±ì—”ë“œ API (RAG + Ontology í†µí•©)",
    version="2.0.0",
)

# Rate Limit ì´ˆê³¼ ì‹œ ì—ëŸ¬ í•¸ë“¤ëŸ¬
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)


# ê¸€ë¡œë²Œ ì˜ˆì™¸ í•¸ë“¤ëŸ¬ - ì—ëŸ¬ ë°œìƒ ì‹œ Telegram ì•Œë¦¼
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    """ëª¨ë“  ì˜ˆì™¸ë¥¼ ì¡ì•„ì„œ Telegram ì•Œë¦¼ ì „ì†¡"""

    error_detail = f"{type(exc).__name__}: {str(exc)[:200]}"
    endpoint = f"{request.method} {request.url.path}"

    # ë¡œê¹…
    logger.error(f"Unhandled exception at {endpoint}: {error_detail}")

    # Telegram ì•Œë¦¼ (ë¹„ë™ê¸°, ì‹¤íŒ¨í•´ë„ ë¬´ì‹œ)
    try:
        from src.tools.telegram_bot import notify_error

        asyncio.create_task(notify_error(exc, context=f"API: {endpoint}"))
    except Exception:
        pass  # Telegram ì•Œë¦¼ ì‹¤íŒ¨ëŠ” ë¬´ì‹œ

    # í´ë¼ì´ì–¸íŠ¸ì—ê²ŒëŠ” ì¼ë°˜ ì—ëŸ¬ ì‘ë‹µ
    from fastapi.responses import JSONResponse

    return JSONResponse(
        status_code=500,
        content={"error": "Internal server error", "detail": error_detail},
    )


# ============================================================================
# URL í—¬í¼ í•¨ìˆ˜ (Railway í™˜ê²½ ìë™ ê°ì§€)
# ============================================================================
def get_base_url() -> str:
    """
    ë°°í¬ í™˜ê²½ì— ë§ëŠ” Base URL ë°˜í™˜

    ìš°ì„ ìˆœìœ„:
    1. DASHBOARD_URL í™˜ê²½ë³€ìˆ˜ (ëª…ì‹œì  ì„¤ì •)
    2. RAILWAY_PUBLIC_DOMAIN (Railway ìë™ ì œê³µ)
    3. localhost:8001 (ë¡œì»¬ ê°œë°œ)
    """
    # 1. ëª…ì‹œì  ì„¤ì • ìš°ì„ 
    if dashboard_url := os.getenv("DASHBOARD_URL"):
        return dashboard_url.rstrip("/")

    # 2. Railway í™˜ê²½ ìë™ ê°ì§€
    if railway_domain := os.getenv("RAILWAY_PUBLIC_DOMAIN"):
        return f"https://{railway_domain}"

    # 3. ë¡œì»¬ ê°œë°œ í™˜ê²½
    port = os.getenv("PORT", "8001")
    return f"http://localhost:{port}"


# CORS ì„¤ì • (í™˜ê²½ë³€ìˆ˜ë¡œ í—ˆìš© ë„ë©”ì¸ ì„¤ì • ê°€ëŠ¥)
ALLOWED_ORIGINS = os.getenv("ALLOWED_ORIGINS", "http://localhost:8001,http://127.0.0.1:8001").split(
    ","
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["GET", "POST", "DELETE", "OPTIONS"],
    allow_headers=["Content-Type", "X-API-Key", "Authorization"],
)


# Security Headers Middleware
class SecurityHeadersMiddleware(BaseHTTPMiddleware):
    """ë³´ì•ˆ í—¤ë” ì¶”ê°€ ë¯¸ë“¤ì›¨ì–´"""

    async def dispatch(self, request, call_next):
        response = await call_next(request)
        response.headers["X-Content-Type-Options"] = "nosniff"
        response.headers["X-Frame-Options"] = "SAMEORIGIN"  # iframe ì„ë² ë”©ì€ ê°™ì€ ë„ë©”ì¸ë§Œ í—ˆìš©
        response.headers["X-XSS-Protection"] = "1; mode=block"
        return response


app.add_middleware(SecurityHeadersMiddleware)

# External Signals Router ë“±ë¡
app.include_router(signals_router)

# Export Router ë“±ë¡ (ë¹„ë™ê¸° DOCX/Excel ë‚´ë³´ë‚´ê¸°)
app.include_router(export_router)

# Telegram Admin Bot Router (ê´€ë¦¬ì ì „ìš©)
try:
    from src.tools.telegram_bot import telegram_router

    app.include_router(telegram_router)
    logger.info("Telegram Admin Bot router enabled")
except ImportError as e:
    logger.warning(f"Telegram Bot not available: {e}")

# Static Files ì„œë¹™ (í°íŠ¸, ì´ë¯¸ì§€ ë“±)
# Arita í°íŠ¸ íŒŒì¼: /fonts/AritaDotumKR-Medium.ttf ë“±ìœ¼ë¡œ ì ‘ê·¼ ê°€ëŠ¥
STATIC_DIR = Path(__file__).parent / "static"
if STATIC_DIR.exists():
    app.mount("/static", StaticFiles(directory=str(STATIC_DIR)), name="static")
    # í¸ì˜ë¥¼ ìœ„í•´ /fonts ê²½ë¡œë„ ë³„ë„ ë§ˆìš´íŠ¸
    FONTS_DIR = STATIC_DIR / "fonts"
    if FONTS_DIR.exists():
        app.mount("/fonts", StaticFiles(directory=str(FONTS_DIR)), name="fonts")

# Dashboard í´ë” ì„œë¹™
DASHBOARD_DIR = Path(__file__).parent / "dashboard"
if DASHBOARD_DIR.exists():
    app.mount("/dashboard", StaticFiles(directory=str(DASHBOARD_DIR), html=True), name="dashboard")

# ============= ì„œë²„ ì‹œì‘ ì‹œ ìë™ ìŠ¤ì¼€ì¤„ëŸ¬ =============

# Railway ë°°í¬ ì‹œ healthcheck íƒ€ì„ì•„ì›ƒ ë°©ì§€: ê¸°ë³¸ê°’ false
# ë¡œì»¬ ê°œë°œ ì‹œ AUTO_START_SCHEDULER=true ë¡œ ì„¤ì •í•˜ë©´ ìŠ¤ì¼€ì¤„ëŸ¬ ìë™ ì‹œì‘
AUTO_START_SCHEDULER = os.getenv("AUTO_START_SCHEDULER", "false").lower() == "true"


@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ìë™ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ ë° ì¦‰ì‹œ í¬ë¡¤ë§ ì²´í¬

    âš ï¸ ì¤‘ìš”: í¬ë¡¤ë§ì€ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰í•˜ì—¬ healthcheck íƒ€ì„ì•„ì›ƒ ë°©ì§€
    """
    # 1. í¬ë¡¤ë§ í•„ìš” ì—¬ë¶€ ì²´í¬ í›„ ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ (ë¹„ë¸”ë¡œí‚¹)
    try:
        crawl_manager = await get_crawl_manager()
        if crawl_manager.needs_crawl():
            logging.info(
                f"ì„œë²„ ì‹œì‘: ì˜¤ëŠ˜({crawl_manager.get_kst_today()}) ë°ì´í„° ì—†ìŒ â†’ í¬ë¡¤ë§ ë°±ê·¸ë¼ìš´ë“œ ì‹œì‘"
            )
            # âš ï¸ await ëŒ€ì‹  create_taskë¡œ ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ (healthcheck ë¸”ë¡œí‚¹ ë°©ì§€)
            asyncio.create_task(crawl_manager.start_crawl())
        else:
            logging.info(
                f"ì„œë²„ ì‹œì‘: ì˜¤ëŠ˜ ë°ì´í„° ìˆìŒ ë˜ëŠ” í¬ë¡¤ë§ ì¤‘ (data_date={crawl_manager.get_data_date()})"
            )
    except Exception as e:
        logging.error(f"ì„œë²„ ì‹œì‘ í¬ë¡¤ë§ ì²´í¬ ì‹¤íŒ¨: {e}")

    # 2. ììœ¨ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ (ë§¤ì¼ 06:00 ì •ê¸° í¬ë¡¤ë§ìš©)
    if AUTO_START_SCHEDULER:
        try:
            brain = await get_initialized_brain()
            await brain.start_scheduler()
            logging.info("ììœ¨ ìŠ¤ì¼€ì¤„ëŸ¬ ìë™ ì‹œì‘ ì™„ë£Œ (ë§¤ì¼ í•œêµ­ì‹œê°„ 06:00 í¬ë¡¤ë§)")
        except Exception as e:
            logging.error(f"ììœ¨ ìŠ¤ì¼€ì¤„ëŸ¬ ìë™ ì‹œì‘ ì‹¤íŒ¨: {e}")

    # 3. Export Job Queue Worker ì‹œì‘ (ë¹„ë™ê¸° ë‚´ë³´ë‚´ê¸°ìš©)
    try:
        from src.tools.export_handlers import register_all_handlers
        from src.tools.job_queue import get_job_queue

        queue = get_job_queue()
        await queue.initialize()
        register_all_handlers(queue)
        await queue.start_worker()
        logging.info("Export Job Queue Worker ì‹œì‘ ì™„ë£Œ")
    except Exception as e:
        logging.error(f"Export Job Queue Worker ì‹œì‘ ì‹¤íŒ¨: {e}")

    # 4. Telegram Admin Bot ì•Œë¦¼ (ì„œë²„ ì‹œì‘)
    try:
        from src.tools.telegram_bot import get_bot

        bot = get_bot()
        if bot.is_enabled():
            await bot.send_alert("ğŸš€ ì„œë²„ ì‹œì‘ë¨", level="info")
            logging.info("Telegram Admin Bot í™œì„±í™”ë¨")
    except Exception as e:
        logging.debug(f"Telegram Bot ì•Œë¦¼ ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")


# ë°ì´í„° ê²½ë¡œ
DATA_PATH = "./data/dashboard_data.json"
DOCS_PATH = "./"  # MD íŒŒì¼ë“¤ì´ ë£¨íŠ¸ì— ìˆìŒ
AUDIT_LOG_DIR = "./logs"

# ============= API Key ì¸ì¦ ì„¤ì • =============

# API_KEY: í™˜ê²½ë³€ìˆ˜ í•„ìˆ˜ - ê¸°ë³¸ê°’ ì—†ìŒ (ë³´ì•ˆìƒ í•˜ë“œì½”ë”© ê¸ˆì§€)
API_KEY = os.getenv("API_KEY")
if not API_KEY:
    logging.warning(
        "âš ï¸ API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë³´í˜¸ëœ ì—”ë“œí¬ì¸íŠ¸ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    )
api_key_header = APIKeyHeader(name="X-API-Key", auto_error=False)


async def verify_api_key(api_key: str = Security(api_key_header)):
    """
    API Key ê²€ì¦ (ë¯¼ê°í•œ ì—”ë“œí¬ì¸íŠ¸ìš©)

    ì‚¬ìš©ë²•: ì—”ë“œí¬ì¸íŠ¸ì— dependencies=[Depends(verify_api_key)] ì¶”ê°€
    """
    import hmac

    if api_key is None:
        raise HTTPException(
            status_code=401, detail="API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤. í—¤ë”ì— X-API-Keyë¥¼ ì¶”ê°€í•˜ì„¸ìš”."
        )
    # íƒ€ì´ë° ê³µê²© ë°©ì–´: hmac.compare_digest ì‚¬ìš©
    if not API_KEY or not hmac.compare_digest(api_key.encode(), API_KEY.encode()):
        raise HTTPException(status_code=403, detail="ìœ íš¨í•˜ì§€ ì•Šì€ API Keyì…ë‹ˆë‹¤.")
    return api_key


# ============= Audit Trail Logger ì„¤ì • =============


def setup_audit_logger():
    """Audit Trail ë¡œê±° ì„¤ì •"""
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    Path(AUDIT_LOG_DIR).mkdir(parents=True, exist_ok=True)

    # ì˜¤ëŠ˜ ë‚ ì§œ ê¸°ë°˜ ë¡œê·¸ íŒŒì¼
    today = datetime.now().strftime("%Y-%m-%d")
    log_file = Path(AUDIT_LOG_DIR) / f"chatbot_audit_{today}.log"

    # ë¡œê±° ìƒì„±
    audit_logger = logging.getLogger("audit_trail")
    audit_logger.setLevel(logging.INFO)

    # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±° (ì¤‘ë³µ ë°©ì§€)
    audit_logger.handlers.clear()

    # íŒŒì¼ í•¸ë“¤ëŸ¬ ì¶”ê°€
    file_handler = logging.FileHandler(log_file, encoding="utf-8")
    file_handler.setLevel(logging.INFO)

    # í¬ë§· ì„¤ì •
    formatter = logging.Formatter("%(asctime)s | %(message)s", datefmt="%Y-%m-%d %H:%M:%S")
    file_handler.setFormatter(formatter)
    audit_logger.addHandler(file_handler)

    return audit_logger


audit_logger = setup_audit_logger()


def log_chat_interaction(
    session_id: str,
    user_query: str,
    ai_response: str,
    query_type: str,
    confidence: float,
    entities: dict,
    sources: list[str],
    response_time_ms: float,
):
    """ì±—ë´‡ ëŒ€í™” Audit Trail ê¸°ë¡"""
    audit_entry = {
        "session_id": session_id,
        "timestamp": datetime.now().isoformat(),
        "user_query": user_query,
        "ai_response": ai_response[:500] + "..." if len(ai_response) > 500 else ai_response,
        "query_type": query_type,
        "confidence": round(confidence, 4),
        "entities": entities,
        "sources": sources,
        "response_time_ms": round(response_time_ms, 2),
    }

    # JSON í˜•ì‹ìœ¼ë¡œ ë¡œê·¸ ê¸°ë¡
    audit_logger.info(json.dumps(audit_entry, ensure_ascii=False))


# ============= Global Instances =============

# RAG ì‹œìŠ¤í…œ
rag_router = RAGRouter()
doc_retriever = DocumentRetriever(DOCS_PATH)

# ì„¸ì…˜ë³„ ëŒ€í™” ë©”ëª¨ë¦¬ (TTL ê¸°ë°˜ ìë™ ì •ë¦¬)
conversation_memory: dict[str, list[dict[str, str]]] = defaultdict(list)
session_last_activity: dict[str, datetime] = {}  # ì„¸ì…˜ë³„ ë§ˆì§€ë§‰ í™œë™ ì‹œê°„
MAX_MEMORY_TURNS = 10
SESSION_TTL_HOURS = 1  # ì„¸ì…˜ ë§Œë£Œ ì‹œê°„ (1ì‹œê°„)
MAX_SESSIONS = 1000  # ìµœëŒ€ ì„¸ì…˜ ìˆ˜


def cleanup_expired_sessions() -> int:
    """ë§Œë£Œëœ ì„¸ì…˜ ì •ë¦¬ (TTL ê¸°ë°˜)"""
    now = datetime.now()
    expired = [
        sid
        for sid, last_time in session_last_activity.items()
        if (now - last_time).total_seconds() > SESSION_TTL_HOURS * 3600
    ]
    for sid in expired:
        if sid in conversation_memory:
            del conversation_memory[sid]
        if sid in session_last_activity:
            del session_last_activity[sid]
    return len(expired)


# í†µí•© ì‹œìŠ¤í…œì€ UnifiedBrain (get_brain())ìœ¼ë¡œ ê´€ë¦¬ë¨

# Market Intelligence Engine ì‹±ê¸€í†¤
_market_intelligence_engine: MarketIntelligenceEngine | None = None


async def get_market_intelligence() -> MarketIntelligenceEngine:
    """Market Intelligence Engine ì‹±ê¸€í†¤ ë°˜í™˜"""
    global _market_intelligence_engine
    if _market_intelligence_engine is None:
        _market_intelligence_engine = MarketIntelligenceEngine()
        await _market_intelligence_engine.initialize()
    return _market_intelligence_engine


# ============= Pydantic Models =============


class ChatRequest(BaseModel):
    """ì±—ë´‡ ìš”ì²­"""

    message: str = Field(..., max_length=10000, description="ìµœëŒ€ 10,000ì")
    session_id: str | None = Field(default="default", max_length=100)
    context: dict | None = None


class ChatResponse(BaseModel):
    """ì±—ë´‡ ì‘ë‹µ"""

    response: str
    query_type: str
    confidence: float
    sources: list[str]
    suggestions: list[str]
    entities: dict[str, Any]


class ExportRequest(BaseModel):
    """ë‚´ë³´ë‚´ê¸° ìš”ì²­"""

    start_date: str | None = None
    end_date: str | None = None
    include_strategy: bool = True


class MarketIntelligenceStatusResponse(BaseModel):
    """Market Intelligence ìƒíƒœ ì‘ë‹µ"""

    initialized: bool
    layers_collected: list[int]
    last_collection: str | None = None
    stats: dict[str, Any]


class LayerDataResponse(BaseModel):
    """ë ˆì´ì–´ ë°ì´í„° ì‘ë‹µ"""

    layer: int
    layer_name: str
    collected_at: str
    data: dict[str, Any]
    sources: list[dict[str, Any]]


# ============= Helper Functions =============


def load_dashboard_data() -> dict[str, Any]:
    """ëŒ€ì‹œë³´ë“œ ë°ì´í„° ë¡œë“œ"""
    try:
        with open(DATA_PATH, encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        return {}


def get_conversation_history(session_id: str, limit: int = 5) -> str:
    """ëŒ€í™” ê¸°ë¡ ì¡°íšŒ (ë¬¸ìì—´ í˜•íƒœ)"""
    history = conversation_memory.get(session_id, [])[-limit:]
    if not history:
        return ""

    lines = []
    for turn in history:
        role = "ì‚¬ìš©ì" if turn["role"] == "user" else "AI"
        content = turn["content"][:150] + "..." if len(turn["content"]) > 150 else turn["content"]
        lines.append(f"[{role}]: {content}")

    return "\n".join(lines)


def add_to_memory(session_id: str, role: str, content: str) -> None:
    """ëŒ€í™” ë©”ëª¨ë¦¬ì— ì¶”ê°€"""
    now = datetime.now()

    # ì£¼ê¸°ì  ì •ë¦¬ (ë§¤ 100ë²ˆì§¸ í˜¸ì¶œë§ˆë‹¤ ë˜ëŠ” ì„¸ì…˜ ìˆ˜ ì´ˆê³¼ ì‹œ)
    if len(session_last_activity) > MAX_SESSIONS or len(session_last_activity) % 100 == 0:
        cleanup_expired_sessions()

    # ë§ˆì§€ë§‰ í™œë™ ì‹œê°„ ì—…ë°ì´íŠ¸
    session_last_activity[session_id] = now

    conversation_memory[session_id].append(
        {"role": role, "content": content, "timestamp": now.isoformat()}
    )
    # ìµœëŒ€ ê°œìˆ˜ ìœ ì§€
    if len(conversation_memory[session_id]) > MAX_MEMORY_TURNS * 2:
        conversation_memory[session_id] = conversation_memory[session_id][-MAX_MEMORY_TURNS * 2 :]


def build_data_context(data: dict, query_type: QueryType, entities: dict) -> str:
    """
    ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (Ontology ê¸°ë°˜)

    ì§ˆë¬¸ ìœ í˜•ê³¼ ì¶”ì¶œëœ ì—”í‹°í‹°ì— ë”°ë¼ í•„ìš”í•œ ë°ì´í„°ë§Œ ì„ íƒ
    """
    if not data:
        return "í˜„ì¬ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

    context_parts = []

    # ë©”íƒ€ë°ì´í„° (í•­ìƒ í¬í•¨)
    metadata = data.get("metadata", {})
    context_parts.append(f"""[ë°ì´í„° í˜„í™©]
- ê¸°ì¤€ì¼: {metadata.get('data_date', 'N/A')}
- ì´ ì œí’ˆ ìˆ˜: {metadata.get('total_products', 0)}ê°œ
- LANEIGE ì œí’ˆ ìˆ˜: {metadata.get('laneige_products', 0)}ê°œ""")

    # ì§ˆë¬¸ ìœ í˜•ë³„ ë°ì´í„° ì„ íƒ
    brand_kpis = data.get("brand", {}).get("kpis", {})

    # ì‹œì¥/ë¸Œëœë“œ ì§€í‘œ (DEFINITION, INTERPRETATION, ANALYSIS)
    if query_type in [
        QueryType.DEFINITION,
        QueryType.INTERPRETATION,
        QueryType.ANALYSIS,
        QueryType.COMBINATION,
    ]:
        if brand_kpis:
            context_parts.append(f"""
[LANEIGE ë¸Œëœë“œ KPI] (Ontology: BrandMetrics)
- SoS (Share of Shelf): {brand_kpis.get('sos', 0)}% {brand_kpis.get('sos_delta', '')}
- Top 10 ì œí’ˆ ìˆ˜: {brand_kpis.get('top10_count', 0)}ê°œ
- í‰ê·  ìˆœìœ„: {brand_kpis.get('avg_rank', 0)}ìœ„
- HHI (ì‹œì¥ ì§‘ì¤‘ë„): {brand_kpis.get('hhi', 0)}""")

    # ê²½ìŸì‚¬ ì •ë³´ (ANALYSIS, DATA_QUERYì—ì„œ ê²½ìŸì‚¬ ì–¸ê¸‰ ì‹œ)
    competitors = data.get("brand", {}).get("competitors", [])
    brands_mentioned = entities.get("brands", [])

    if query_type == QueryType.ANALYSIS or any(
        b for b in brands_mentioned if b.lower() != "laneige"
    ):
        if competitors:
            top_comps = competitors[:5]
            comp_lines = [
                f"  - {c['brand']}: SoS {c['sos']}%, í‰ê·  ìˆœìœ„ {c['avg_rank']}ìœ„, ì œí’ˆ {c['product_count']}ê°œ"
                for c in top_comps
            ]
            context_parts.append("[ê²½ìŸì‚¬ í˜„í™©]\n" + "\n".join(comp_lines))

    # ì œí’ˆ ì •ë³´ (DATA_QUERY, íŠ¹ì • ì œí’ˆ ì–¸ê¸‰ ì‹œ)
    products = data.get("products", {})
    products_mentioned = entities.get("products", [])

    if query_type == QueryType.DATA_QUERY or products_mentioned:
        if products:
            prod_lines = []
            for _asin, p in list(products.items())[:5]:
                prod_lines.append(f"""  - {p['name'][:40]}
    ìˆœìœ„: #{p['rank']} ({p['rank_delta']}), í‰ì : {p['rating']}, ë³€ë™ì„±: {p.get('volatility_status', 'N/A')}""")
            context_parts.append(
                "[LANEIGE ì œí’ˆ í˜„í™©] (Ontology: ProductMetrics)\n" + "\n".join(prod_lines)
            )

    # ì¹´í…Œê³ ë¦¬ ì •ë³´
    categories = data.get("categories", {})
    categories_mentioned = entities.get("categories", [])

    if categories_mentioned or query_type in [QueryType.ANALYSIS, QueryType.INTERPRETATION]:
        if categories:
            cat_lines = []
            for _cat_id, cat in categories.items():
                cat_lines.append(
                    f"  - {cat['name']}: SoS {cat['sos']}%, ìµœê³  ìˆœìœ„ #{cat['best_rank']}, CPI {cat.get('cpi', 100)}"
                )
            context_parts.append(
                "[ì¹´í…Œê³ ë¦¬ í˜„í™©] (Ontology: MarketMetrics)\n" + "\n".join(cat_lines)
            )

    # ì•¡ì…˜ ì•„ì´í…œ (ì „ëµ ì§ˆë¬¸)
    if query_type == QueryType.ANALYSIS:
        action_items = data.get("home", {}).get("action_items", [])
        if action_items:
            action_lines = [
                f"  - [{a['priority']}] {a['product_name']}: {a['signal']} â†’ {a['action_tag']}"
                for a in action_items[:4]
            ]
            context_parts.append("[í˜„ì¬ ì•¡ì…˜ ì•„ì´í…œ]\n" + "\n".join(action_lines))

    return "\n\n".join(context_parts)


async def get_rag_context(query: str, query_type: QueryType) -> tuple[str, list[str]]:
    """
    RAG ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰

    Returns:
        (ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´, ì¶œì²˜ ëª©ë¡)
    """
    # DocumentRetriever ì´ˆê¸°í™” (ì²˜ìŒ í˜¸ì¶œ ì‹œ)
    if not doc_retriever._initialized:
        await doc_retriever.initialize()

    # ì§ˆë¬¸ ìœ í˜•ì— ë§ëŠ” ë¬¸ì„œ ê²€ìƒ‰
    target_doc = rag_router.get_target_document(query_type)

    # ê²€ìƒ‰ ì‹¤í–‰
    results = await doc_retriever.search(query, top_k=3, doc_filter=target_doc)

    if not results:
        return "", []

    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context_parts = []
    sources = []

    for result in results:
        metadata = result.get("metadata", {})
        content = result.get("content", "")
        title = metadata.get("title", "Unknown")
        doc_id = metadata.get("doc_id", "")

        context_parts.append(f"[{title}]\n{content}")

        # ì¶œì²˜ ì¶”ê°€
        doc_name_map = {
            "strategic_indicators": "Strategic Indicators Definition",
            "metric_interpretation": "Metric Interpretation Guide",
            "indicator_combination": "Indicator Combination Playbook",
            "home_insight_rules": "Home Page Insight Rules",
        }
        if doc_id in doc_name_map and doc_name_map[doc_id] not in sources:
            sources.append(doc_name_map[doc_id])

    return "\n\n---\n\n".join(context_parts), sources


def generate_dynamic_suggestions(
    query_type: QueryType, entities: dict, response: str, user_query: str = ""
) -> list[str]:
    """
    ë™ì  í›„ì† ì§ˆë¬¸ ì œì•ˆ (v2 - ê°œì„  ë²„ì „)

    ì‘ë‹µ ë‚´ìš©, ì—”í‹°í‹°, ì¿¼ë¦¬ ìœ í˜•ì„ ì¢…í•©í•˜ì—¬ ë§ì¶¤í˜• ì œì•ˆ ìƒì„±

    ìš°ì„ ìˆœìœ„:
    1. ì‘ë‹µ í‚¤ì›Œë“œ ê¸°ë°˜ (response ë¶„ì„)
    2. ì—”í‹°í‹° ê¸°ë°˜ (ë¸Œëœë“œ, ì¹´í…Œê³ ë¦¬, ì§€í‘œ í™œìš©)
    3. ì¿¼ë¦¬ ìœ í˜• ê¸°ë°˜ (í´ë°±)

    Args:
        query_type: ì§ˆë¬¸ ìœ í˜•
        entities: ì¶”ì¶œëœ ì—”í‹°í‹°
        response: AI ì‘ë‹µ ë‚´ìš©
        user_query: ì›ë³¸ ì‚¬ìš©ì ì§ˆë¬¸

    Returns:
        3ê°œì˜ í›„ì† ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
    """
    suggestions = []

    # ì—”í‹°í‹° ì¶”ì¶œ
    brands = entities.get("brands", [])
    indicators = entities.get("indicators", [])
    categories = entities.get("categories", [])

    # 1ìˆœìœ„: ì‘ë‹µ í‚¤ì›Œë“œ ê¸°ë°˜ ì œì•ˆ
    if response:
        keyword_suggestions = _extract_response_keywords(response)
        suggestions.extend(keyword_suggestions)

    # 2ìˆœìœ„: ì—”í‹°í‹° ê¸°ë°˜ ì œì•ˆ
    if len(suggestions) < 3:
        entity_suggestions = _generate_entity_suggestions(brands, categories, indicators)
        suggestions.extend(entity_suggestions)

    # 3ìˆœìœ„: ì¿¼ë¦¬ ìœ í˜• ê¸°ë°˜ ì œì•ˆ (í´ë°±)
    if len(suggestions) < 3:
        type_suggestions = _generate_type_suggestions(query_type, brands, indicators)
        suggestions.extend(type_suggestions)

    # ì¤‘ë³µ ì œê±° ë° ìƒìœ„ 3ê°œ
    unique = list(dict.fromkeys(suggestions))
    return unique[:3]


def _extract_response_keywords(response: str) -> list[str]:
    """ì‘ë‹µì—ì„œ í›„ì† ì§ˆë¬¸ ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    import re

    keywords = []

    # íŒ¨í„´ ë§¤ì¹­ - ì‘ë‹µ ë‚´ìš©ì— ë”°ë¼ ê´€ë ¨ í›„ì† ì§ˆë¬¸ ìƒì„±
    patterns = {
        r"ìˆœìœ„.{0,10}(í•˜ë½|ê¸‰ë½|ë–¨ì–´)": "ìˆœìœ„ í•˜ë½ ì›ì¸ì„ ë¶„ì„í•´ì£¼ì„¸ìš”",
        r"ìˆœìœ„.{0,10}(ìƒìŠ¹|ê¸‰ë“±|ì˜¬ë¼)": "ìƒìŠ¹ ìš”ì¸ì„ ìƒì„¸ ë¶„ì„í•´ì£¼ì„¸ìš”",
        r"ê²½ìŸì‚¬|ê²½ìŸ ë¸Œëœë“œ|competitor": "ê²½ìŸì‚¬ ìƒì„¸ ë¹„êµë¥¼ í•´ì£¼ì„¸ìš”",
        r"ê°€ê²©.{0,10}(ì¸ìƒ|ì¸í•˜|ë³€ë™)": "ê°€ê²© ì „ëµì„ ë¶„ì„í•´ì£¼ì„¸ìš”",
        r"ë¦¬ë·°|í‰ì |rating": "ì†Œë¹„ì í”¼ë“œë°±ì„ ìƒì„¸ ë¶„ì„í•´ì£¼ì„¸ìš”",
        r"íŠ¸ë Œë“œ|ìœ í–‰|trend": "íŠ¸ë Œë“œ ìƒì„¸ ë¶„ì„ì„ í•´ì£¼ì„¸ìš”",
        r"ì„±ì¥.{0,5}(ê¸°íšŒ|ê°€ëŠ¥|potential)": "ì„±ì¥ ì „ëµì„ ì œì•ˆí•´ì£¼ì„¸ìš”",
        r"ìœ„í—˜|ë¦¬ìŠ¤í¬|ìœ„í˜‘|risk": "ë¦¬ìŠ¤í¬ ëŒ€ì‘ ì „ëµì€?",
        r"SoS|ì ìœ ìœ¨|share": "ì ìœ ìœ¨ ê°œì„  ì „ëµì€?",
        r"Top.{0,3}(10|5)|ìƒìœ„": "Top 10 ì§„ì… ì „ëµì€?",
    }

    for pattern, suggestion in patterns.items():
        if re.search(pattern, response, re.IGNORECASE):
            keywords.append(suggestion)
            if len(keywords) >= 2:  # ìµœëŒ€ 2ê°œ
                break

    return keywords


def _generate_entity_suggestions(
    brands: list[str], categories: list[str], indicators: list[str]
) -> list[str]:
    """ì—”í‹°í‹° ê¸°ë°˜ ë™ì  ì œì•ˆ ìƒì„±"""
    suggestions = []

    # ë¸Œëœë“œ ê¸°ë°˜
    if brands:
        brand = brands[0]
        suggestions.append(f"{brand} ê²½ìŸì‚¬ ë¹„êµ ë¶„ì„")
        if len(brands) > 1:
            suggestions.append(f"{brands[0]} vs {brands[1]} ë¹„êµ")

    # ì¹´í…Œê³ ë¦¬ ê¸°ë°˜
    if categories:
        cat = categories[0]
        suggestions.append(f"{cat} ì‹œì¥ íŠ¸ë Œë“œ ë¶„ì„")

    # ì§€í‘œ ê¸°ë°˜
    if indicators:
        ind = indicators[0].upper()
        suggestions.append(f"{ind} ê°œì„  ì „ëµ")

    return suggestions


def _generate_type_suggestions(
    query_type: QueryType, brands: list[str], indicators: list[str]
) -> list[str]:
    """ì¿¼ë¦¬ ìœ í˜• ê¸°ë°˜ í´ë°± ì œì•ˆ"""
    suggestions = []

    if query_type == QueryType.DEFINITION:
        if indicators:
            ind = indicators[0].upper()
            suggestions.append(f"{ind}ê°€ ë†’ìœ¼ë©´ ì–´ë–¤ ì˜ë¯¸ì¸ê°€ìš”?")
        suggestions.extend(["ê´€ë ¨ëœ ë‹¤ë¥¸ ì§€í‘œëŠ”?", "ì‹¤ì œ ë°ì´í„°ì— ì ìš©í•´ì£¼ì„¸ìš”"])

    elif query_type == QueryType.INTERPRETATION:
        suggestions.extend(
            ["í˜„ì¬ LANEIGE ìˆ˜ì¹˜ ë¶„ì„", "ê²½ìŸì‚¬ì™€ ë¹„êµí•´ì£¼ì„¸ìš”", "ê°œì„  ì•¡ì…˜ ì•„ì´í…œì€?"]
        )

    elif query_type == QueryType.DATA_QUERY:
        suggestions.extend(["ì´ ìˆ˜ì¹˜ê°€ ì¢‹ì€ ê±´ê°€ìš”?", "ìµœê·¼ 7ì¼ ì¶”ì´ ë¶„ì„", "ê²½ìŸì‚¬ ëŒ€ë¹„ í˜„í™©"])

    elif query_type == QueryType.ANALYSIS:
        suggestions.extend(["ê°€ì¥ ì‹œê¸‰í•œ ì•¡ì…˜ì€?", "Top 10 ì§„ì… ì „ëµ", "ë¦¬ìŠ¤í¬ ìš”ì¸ ë¶„ì„"])

    elif query_type == QueryType.COMBINATION:
        suggestions.extend(["ë‹¤ë¥¸ ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„", "í˜„ì¬ í•´ë‹¹ ìƒí™© ì¡´ì¬ ì—¬ë¶€"])

    else:
        # ê¸°ë³¸ ì œì•ˆ
        suggestions = ["SoS(ì ìœ ìœ¨) ì„¤ëª…í•´ì£¼ì„¸ìš”", "LANEIGE í˜„ì¬ ìˆœìœ„ëŠ”?", "ì „ëµì  ê¶Œê³ ì‚¬í•­"]

    return suggestions


# ============= API Endpoints =============


@app.get("/")
async def root():
    """í—¬ìŠ¤ ì²´í¬"""
    return {
        "status": "ok",
        "message": "AMORE Dashboard API v2.0 (RAG + Ontology)",
        "features": ["chatbot", "rag", "ontology", "memory", "docx_export"],
    }


@app.get("/api/data")
async def get_data():
    """ëŒ€ì‹œë³´ë“œ ë°ì´í„° ì¡°íšŒ"""
    data = load_dashboard_data()
    if not data:
        raise HTTPException(status_code=404, detail="Dashboard data not found")
    return data


@app.post("/api/chat", response_model=ChatResponse, dependencies=[Depends(verify_api_key)])
@limiter.limit("10/minute")  # ë¶„ë‹¹ 10íšŒ ì œí•œ (ë³´ì•ˆ ê°•í™”)
async def chat(request: Request, body: ChatRequest):
    """
    ChatGPT + RAG + Ontology í†µí•© ì±—ë´‡ API

    1. ì§ˆë¬¸ ë¶„ì„ (RAGRouter)
    2. ì—”í‹°í‹° ì¶”ì¶œ (Ontology ê¸°ë°˜)
    3. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ (RAG)
    4. ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    5. ëŒ€í™” ê¸°ë¡ ì°¸ì¡°
    6. LLM ì‘ë‹µ ìƒì„±
    7. Audit Trail ë¡œê¹…
    """
    import time

    start_time = time.time()

    message = body.message.strip()
    session_id = body.session_id or "default"

    if not message:
        raise HTTPException(status_code=400, detail="Message is required")

    # 1. ì§ˆë¬¸ ë¶„ë¥˜ (RAGRouter ì‚¬ìš©)
    route_result = rag_router.route(message)
    query_type = route_result["query_type"]
    confidence = route_result["confidence"]

    # 2. ì—”í‹°í‹° ì¶”ì¶œ (Ontology ê¸°ë°˜)
    entities = rag_router.extract_entities(message)

    # 3. ëª…í™•í™” í•„ìš” ì—¬ë¶€ í™•ì¸
    clarification = rag_router.needs_clarification(route_result, entities)
    if clarification and confidence < 0.5:
        # ëª…í™•í™” ìš”ì²­
        add_to_memory(session_id, "user", message)
        add_to_memory(session_id, "assistant", clarification)

        return ChatResponse(
            response=clarification,
            query_type=query_type.value if hasattr(query_type, "value") else str(query_type),
            confidence=confidence,
            sources=[],
            suggestions=[
                "ì˜ˆ, ì „ì²´ ë¸Œëœë“œ ë¶„ì„í•´ì£¼ì„¸ìš”",
                "LANEIGEë§Œ ë¶„ì„í•´ì£¼ì„¸ìš”",
                "Lip Care ì¹´í…Œê³ ë¦¬ë§Œ",
            ],
            entities=entities,
        )

    # 4. RAG ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
    rag_context, sources = await get_rag_context(message, query_type)

    # 5. ë°ì´í„° ë¡œë“œ ë° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    data = load_dashboard_data()
    data_context = build_data_context(data, query_type, entities)

    # 6. ëŒ€í™” ê¸°ë¡ ì¡°íšŒ
    conversation_history = get_conversation_history(session_id)

    # 7. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    system_prompt = """ë‹¹ì‹ ì€ AMORE Pacificì˜ LANEIGE ë¸Œëœë“œ Amazon ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì—­í• :
- Amazon US ë² ìŠ¤íŠ¸ì…€ëŸ¬ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì¸ì‚¬ì´íŠ¸ ì œê³µ
- LANEIGE ë¸Œëœë“œì˜ ì‹œì¥ í¬ì§€ì…˜ ë¶„ì„
- ê²½ìŸì‚¬ ëŒ€ë¹„ ì „ëµì  ê¶Œê³  ì œê³µ
- ì§€í‘œ ì •ì˜ ë° í•´ì„ ê°€ì´ë“œ ì œê³µ

Ontology ì—”í‹°í‹° ì´í•´:
- Brand: ë¸Œëœë“œ ì •ë³´ (LANEIGE, ê²½ìŸì‚¬ ë“±)
- Product: ì œí’ˆ ì •ë³´ (ASIN, ìˆœìœ„, í‰ì , ê°€ê²© ë“±)
- Category: ì¹´í…Œê³ ë¦¬ (Lip Care, Skin Care ë“±)
- BrandMetrics: SoS, í‰ê· ìˆœìœ„, ì œí’ˆìˆ˜ ë“±
- ProductMetrics: ìˆœìœ„ë³€ë™ì„±, ì—°ì†ì²´ë¥˜ì¼, í‰ì ì¶”ì„¸ ë“±
- MarketMetrics: HHI(ì‹œì¥ì§‘ì¤‘ë„), êµì²´ìœ¨ ë“±

ì‘ë‹µ ê°€ì´ë“œë¼ì¸:
1. ë°ì´í„°ì— ê¸°ë°˜í•œ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ ì¸ìš©
2. RAG ë¬¸ì„œì˜ ì •ì˜/í•´ì„ ê¸°ì¤€ í™œìš©
3. ì´ì „ ëŒ€í™” ë§¥ë½ ê³ ë ¤
4. ê°„ê²°í•˜ê³  ì•¡ì…˜ ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸ ì œê³µ
5. ë¶ˆí™•ì‹¤í•œ ê²½ìš° ëª…í™•íˆ ë°í ê²ƒ
6. ë‹¨ì •ì  í‘œí˜„ í”¼í•˜ê¸°
7. í•œêµ­ì–´ë¡œ ì‘ë‹µ

ì§ˆë¬¸ ìœ í˜•ë³„ ì‘ë‹µ ìŠ¤íƒ€ì¼:
- ì •ì˜(DEFINITION): ì§€í‘œì˜ ì •ì˜, ì‚°ì¶œì‹, ì˜ë¯¸ë¥¼ ì„¤ëª…
- í•´ì„(INTERPRETATION): ìˆ˜ì¹˜ì˜ ì˜ë¯¸, ì¢‹ê³  ë‚˜ì¨ì˜ ê¸°ì¤€ ì„¤ëª…
- ì¡°í•©(COMBINATION): ì—¬ëŸ¬ ì§€í‘œë¥¼ í•¨ê»˜ í•´ì„, ì‹œë‚˜ë¦¬ì˜¤ë³„ ì•¡ì…˜ ì œì•ˆ
- ë°ì´í„°ì¡°íšŒ(DATA_QUERY): í˜„ì¬ ìˆ˜ì¹˜ì™€ ë³€ë™ í˜„í™© ì•ˆë‚´
- ë¶„ì„(ANALYSIS): ì¢…í•© ë¶„ì„ê³¼ ì „ëµì  ê¶Œê³  ì œê³µ
"""

    # 8. ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    user_prompt = f"""## ì‚¬ìš©ì ì§ˆë¬¸
{message}

## ì§ˆë¬¸ ìœ í˜•
{query_type.value if hasattr(query_type, 'value') else str(query_type)} (ì‹ ë¢°ë„: {confidence:.1%})

## ì¶”ì¶œëœ ì—”í‹°í‹°
- ë¸Œëœë“œ: {', '.join(entities.get('brands', [])) or 'ì—†ìŒ'}
- ì¹´í…Œê³ ë¦¬: {', '.join(entities.get('categories', [])) or 'ì—†ìŒ'}
- ì§€í‘œ: {', '.join(entities.get('indicators', [])) or 'ì—†ìŒ'}
- ê¸°ê°„: {entities.get('time_range') or 'ì—†ìŒ'}

## RAG ì°¸ì¡° ë¬¸ì„œ
{rag_context if rag_context else 'ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ'}

## í˜„ì¬ ë°ì´í„°
{data_context}

## ì´ì „ ëŒ€í™”
{conversation_history if conversation_history else 'ì´ì „ ëŒ€í™” ì—†ìŒ'}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
- ì§ˆë¬¸ ìœ í˜•ì— ë§ëŠ” ì‘ë‹µ ìŠ¤íƒ€ì¼ì„ ì‚¬ìš©í•˜ì„¸ìš”.
- RAG ë¬¸ì„œì— ê´€ë ¨ ì •ì˜/í•´ì„ì´ ìˆìœ¼ë©´ ì¸ìš©í•˜ì„¸ìš”.
- ì´ì „ ëŒ€í™” ë§¥ë½ì´ ìˆìœ¼ë©´ ê³ ë ¤í•˜ì„¸ìš”.
"""

    try:
        response = await acompletion(
            model="gpt-4.1-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.3,
            max_tokens=1000,
        )

        answer = response.choices[0].message.content

        # 9. ëŒ€í™” ë©”ëª¨ë¦¬ì— ì €ì¥
        add_to_memory(session_id, "user", message)
        add_to_memory(session_id, "assistant", answer)

        # 10. ë™ì  í›„ì† ì§ˆë¬¸ ì œì•ˆ (v2 - ê°œì„  ë²„ì „)
        suggestions = generate_dynamic_suggestions(query_type, entities, answer, message)

        # 11. Audit Trail ë¡œê¹…
        response_time_ms = (time.time() - start_time) * 1000
        log_chat_interaction(
            session_id=session_id,
            user_query=message,
            ai_response=answer,
            query_type=query_type.value if hasattr(query_type, "value") else str(query_type),
            confidence=confidence,
            entities=entities,
            sources=sources,
            response_time_ms=response_time_ms,
        )

        return ChatResponse(
            response=answer,
            query_type=query_type.value if hasattr(query_type, "value") else str(query_type),
            confidence=confidence,
            sources=sources,
            suggestions=suggestions,
            entities=entities,
        )

    except Exception as e:
        logger.error(f"LLM Error: {e}")

        # Fallback ì‘ë‹µ
        fallback = route_result.get("fallback_message") or rag_router.get_fallback_response(
            "unknown"
        )

        # ë°ì´í„° ê¸°ë°˜ ê¸°ë³¸ ì‘ë‹µ ì¶”ê°€
        if data and query_type == QueryType.DATA_QUERY:
            brand_kpis = data.get("brand", {}).get("kpis", {})
            fallback = f"""í˜„ì¬ LANEIGE í˜„í™©:
- SoS: {brand_kpis.get('sos', 0)}%
- Top 10 ì œí’ˆ: {brand_kpis.get('top10_count', 0)}ê°œ
- í‰ê·  ìˆœìœ„: {brand_kpis.get('avg_rank', 0)}ìœ„

(ìƒì„¸ ë¶„ì„ì„ ìœ„í•´ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”)"""

        # Fallback ì‘ë‹µë„ Audit Trail ê¸°ë¡
        response_time_ms = (time.time() - start_time) * 1000
        log_chat_interaction(
            session_id=session_id,
            user_query=message,
            ai_response=f"[ERROR] {str(e)[:100]} | Fallback: {fallback[:200]}",
            query_type=query_type.value if hasattr(query_type, "value") else str(query_type),
            confidence=0.0,
            entities=entities,
            sources=["fallback"],
            response_time_ms=response_time_ms,
        )

        return ChatResponse(
            response=fallback,
            query_type=query_type.value if hasattr(query_type, "value") else str(query_type),
            confidence=0.0,
            sources=[],
            suggestions=["ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”", "SoSê°€ ë­”ê°€ìš”?", "í˜„ì¬ ìˆœìœ„ ì•Œë ¤ì£¼ì„¸ìš”"],
            entities=entities,
        )


@app.delete("/api/chat/memory/{session_id}")
async def clear_memory(session_id: str):
    """ì„¸ì…˜ ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"""
    if session_id in conversation_memory:
        del conversation_memory[session_id]
    return {"status": "ok", "message": f"Session {session_id} memory cleared"}


# ============= Simple Chat API (v3 - ë‹¨ìˆœí™”) =============

from src.core.simple_chat import get_chat_service


class SimpleChatRequest(BaseModel):
    """Simple Chat ìš”ì²­"""

    message: str
    session_id: str | None = "default"


class SimpleChatResponse(BaseModel):
    """Simple Chat ì‘ë‹µ"""

    text: str
    suggestions: list[str]
    tools_used: list[str]
    sources: list[dict[str, Any]] = []  # AI ì¶œì²˜ ì •ë³´ ì¶”ê°€
    data_date: str
    processing_time_ms: float


@app.post("/api/v3/chat", response_model=SimpleChatResponse, dependencies=[Depends(verify_api_key)])
@limiter.limit("10/minute")  # ë¶„ë‹¹ 10íšŒ ì œí•œ (ë³´ì•ˆ ê°•í™”)
async def chat_v3(request: Request, body: SimpleChatRequest):
    """
    Simple LLM Chat API (v3)

    ë‹¨ìˆœí™”ëœ êµ¬ì¡°:
    - LLMì´ ëª¨ë“  íŒë‹¨ ë‹´ë‹¹
    - Function Callingìœ¼ë¡œ ë„êµ¬ ì‚¬ìš©
    - ë¶ˆí•„ìš”í•œ ë ˆì´ì–´ ì œê±°
    """
    message = body.message.strip()
    session_id = body.session_id or "default"

    if not message:
        raise HTTPException(status_code=400, detail="Message is required")

    # í¬ë¡¤ë§ ìƒíƒœ ì²´í¬
    crawl_manager = await get_crawl_manager()
    crawl_notification = None
    crawl_started = False

    if crawl_manager.needs_crawl():
        crawl_started = await crawl_manager.start_crawl()

    if crawl_manager.should_notify(session_id):
        crawl_notification = crawl_manager.get_notification_message()
        crawl_manager.mark_notified(session_id)

    # Simple Chat Serviceë¡œ ì²˜ë¦¬
    chat_service = get_chat_service()
    result = await chat_service.chat(message, session_id)

    # í¬ë¡¤ë§ ì•Œë¦¼ ì¶”ê°€
    response_text = result["text"]
    if crawl_notification:
        response_text = f"{crawl_notification}\n\n---\n\n{response_text}"
    elif crawl_started:
        data_date = crawl_manager.get_data_date() or "ì—†ìŒ"
        response_text = (
            f"ğŸ“¡ **ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì˜¤ëŠ˜ ë°ì´í„° ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤.**\n"
            f"í˜„ì¬ ë°ì´í„°: {data_date}\n"
            f"ìˆ˜ì§‘ì´ ì™„ë£Œë˜ë©´ ì•Œë ¤ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n\n---\n\n{response_text}"
        )

    return SimpleChatResponse(
        text=response_text,
        suggestions=result.get("suggestions", []),
        tools_used=result.get("tools_used", []),
        sources=result.get("sources", []),  # AI ì¶œì²˜ ì •ë³´ ì „ë‹¬
        data_date=result.get("data_date", "N/A"),
        processing_time_ms=result.get("processing_time_ms", 0),
    )


@app.post("/api/v3/chat/stream", dependencies=[Depends(verify_api_key)])
@limiter.limit("10/minute")  # ë¶„ë‹¹ 10íšŒ ì œí•œ (ë³´ì•ˆ ê°•í™”)
async def chat_v3_stream(request: Request, body: SimpleChatRequest):
    """
    Simple LLM Chat API with SSE Streaming (v3)

    SSE í˜•ì‹ìœ¼ë¡œ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤.

    ì´ë²¤íŠ¸ íƒ€ì…:
    - text: ì‘ë‹µ í…ìŠ¤íŠ¸ ì²­í¬
    - tool_call: ë„êµ¬ í˜¸ì¶œ ì •ë³´
    - done: ì™„ë£Œ (í›„ì† ì§ˆë¬¸ ë“± ë©”íƒ€ë°ì´í„° í¬í•¨)
    - error: ì˜¤ë¥˜ ë°œìƒ
    """
    message = body.message.strip()
    session_id = body.session_id or "default"

    if not message:
        raise HTTPException(status_code=400, detail="Message is required")

    chat_service = get_chat_service()

    async def generate():
        """SSE ì´ë²¤íŠ¸ ìƒì„±ê¸°"""
        try:
            async for chunk in chat_service.chat_stream(message, session_id):
                event_type = chunk.get("type", "text")
                content = chunk.get("content", "")

                # SSE í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                if event_type == "text":
                    data = json.dumps({"type": "text", "content": content}, ensure_ascii=False)
                elif event_type == "tool_call":
                    data = json.dumps({"type": "tool_call", "content": content}, ensure_ascii=False)
                elif event_type == "done":
                    data = json.dumps({"type": "done", "content": content}, ensure_ascii=False)
                elif event_type == "error":
                    data = json.dumps({"type": "error", "content": content}, ensure_ascii=False)
                else:
                    data = json.dumps(chunk, ensure_ascii=False)

                yield f"data: {data}\n\n"

        except Exception as e:
            logger.error(f"SSE stream error: {e}")
            error_data = json.dumps({"type": "error", "content": str(e)}, ensure_ascii=False)
            yield f"data: {error_data}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",  # nginx ë²„í¼ë§ ë¹„í™œì„±í™”
        },
    )


# ============= LLM Orchestrator API (v2 - ê¸°ì¡´, deprecated) =============


class OrchestratorChatRequest(BaseModel):
    """LLM Orchestrator ì±—ë´‡ ìš”ì²­"""

    message: str
    session_id: str | None = "default"
    skip_cache: bool = False


class OrchestratorChatResponse(BaseModel):
    """LLM Orchestrator ì±—ë´‡ ì‘ë‹µ"""

    text: str
    query_type: str
    confidence_level: str
    confidence_score: float
    sources: list[str]
    entities: dict[str, Any]
    tools_called: list[str]
    suggestions: list[str]
    is_fallback: bool
    is_clarification: bool
    processing_time_ms: float


@app.post(
    "/api/v2/chat", response_model=OrchestratorChatResponse, dependencies=[Depends(verify_api_key)]
)
@limiter.limit("10/minute")  # ë¶„ë‹¹ 10íšŒ ì œí•œ (ë³´ì•ˆ ê°•í™”)
async def chat_v2(request: Request, body: OrchestratorChatRequest):
    """
    í†µí•© ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ê¸°ë°˜ ì±—ë´‡ API (v2)

    ë™ì‘ íë¦„:
    1. ì§ˆë¬¸ ìˆ˜ì‹ 
    2. ì‹œìŠ¤í…œ ìƒíƒœ ì ê²€ (ë°ì´í„° ì‹ ì„ ë„, ì‚¬ìš© ê°€ëŠ¥ ì—ì´ì „íŠ¸)
    3. LLMì´ ìƒí™© íŒë‹¨ â†’ ì—ì´ì „íŠ¸ ì„ íƒ
    4. ì—ì´ì „íŠ¸ ì‹¤í–‰ (ì—ëŸ¬ ì‹œ ì „ëµì— ë”°ë¼ ì²˜ë¦¬)
    5. ì‘ë‹µ ìƒì„±

    ì—ëŸ¬ ì „ëµ:
    - RETRY: ì¬ì‹œë„ (ìµœëŒ€ 2íšŒ)
    - FALLBACK: ìºì‹œ ë°ì´í„° ì‚¬ìš©
    - SKIP: ê±´ë„ˆë›°ê³  ê³„ì†
    - ABORT: ì¤‘ë‹¨ + ì‚¬ìš©ì ì•Œë¦¼
    """
    import time

    start_time = time.time()

    message = body.message.strip()
    session_id = body.session_id or "default"

    if not message:
        raise HTTPException(status_code=400, detail="Message is required")

    # === í¬ë¡¤ë§ ìƒíƒœ ì²´í¬ ===
    crawl_manager = await get_crawl_manager()
    crawl_notification = None
    crawl_started = False

    # ì˜¤ëŠ˜ ë°ì´í„°ê°€ ì—†ê³ , í¬ë¡¤ë§ ì¤‘ì´ ì•„ë‹ˆë©´ ì‹œì‘
    if crawl_manager.needs_crawl():
        crawl_started = await crawl_manager.start_crawl()
        if crawl_started:
            logging.info("Background crawl started for today's data")

    # í¬ë¡¤ë§ ì™„ë£Œ ì•Œë¦¼ ì²´í¬ (ì´ ì„¸ì…˜ì—ì„œ ì•„ì§ ì•ˆ ì•Œë ¸ìœ¼ë©´)
    if crawl_manager.should_notify(session_id):
        crawl_notification = crawl_manager.get_notification_message()
        crawl_manager.mark_notified(session_id)

    try:
        # UnifiedBrainìœ¼ë¡œ ì²˜ë¦¬
        brain = get_brain()

        # í˜„ì¬ ë©”íŠ¸ë¦­ ë°ì´í„° ë¡œë“œ
        data = load_dashboard_data()
        current_metrics = data if data else None

        # ì²˜ë¦¬
        response = await brain.process_query(
            query=message,
            session_id=session_id,
            current_metrics=current_metrics,
            skip_cache=body.skip_cache,
        )

        # ì‘ë‹µ ë³€í™˜ (UnifiedBrain response ì²˜ë¦¬)
        response_dict = response.to_dict() if hasattr(response, "to_dict") else response

        # ì‘ë‹µ í…ìŠ¤íŠ¸ êµ¬ì„±
        response_text = response_dict.get("text", response_dict.get("content", ""))

        # í¬ë¡¤ë§ ì•Œë¦¼ ì¶”ê°€
        if crawl_notification:
            response_text = f"{crawl_notification}\n\n---\n\n{response_text}"
        elif crawl_started:
            # í¬ë¡¤ë§ ì‹œì‘ ì•Œë¦¼
            data_date = crawl_manager.get_data_date() or "ì—†ìŒ"
            response_text = (
                f"ğŸ“¡ **ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì˜¤ëŠ˜ ë°ì´í„° ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤.**\n"
                f"í˜„ì¬ ë°ì´í„°: {data_date}\n"
                f"ìˆ˜ì§‘ì´ ì™„ë£Œë˜ë©´ ì•Œë ¤ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n\n---\n\n{response_text}"
            )

        # ì‘ë‹µ ë³€í™˜
        return OrchestratorChatResponse(
            text=response_text,
            query_type=response_dict.get("query_type", "unknown"),
            confidence_level=response_dict.get("confidence_level", "medium"),
            confidence_score=response_dict.get(
                "confidence_score", response_dict.get("confidence", 0.5)
            ),
            sources=response_dict.get("sources", []),
            entities=response_dict.get("entities", {}),
            tools_called=response_dict.get("tools_called", response_dict.get("tools_used", [])),
            suggestions=response_dict.get("suggestions", []),
            is_fallback=response_dict.get("is_fallback", False),
            is_clarification=response_dict.get("is_clarification", False),
            processing_time_ms=response_dict.get("processing_time_ms", 0),
        )

    except Exception as e:
        logging.error(f"Orchestrator error: {e}")
        return OrchestratorChatResponse(
            text=f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            query_type="error",
            confidence_level="unknown",
            confidence_score=0.0,
            sources=[],
            entities={},
            tools_called=[],
            suggestions=["ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”"],
            is_fallback=True,
            is_clarification=False,
            processing_time_ms=(time.time() - start_time) * 1000,
        )


@app.get("/api/v2/stats")
async def get_orchestrator_stats():
    """UnifiedBrain í†µê³„ ì¡°íšŒ"""
    brain = get_brain()
    return brain.get_stats() if hasattr(brain, "get_stats") else {"status": "ok"}


@app.get("/api/v2/state")
async def get_orchestrator_state():
    """UnifiedBrain ìƒíƒœ ì¡°íšŒ"""
    brain = get_brain()
    return {
        "summary": brain.get_state_summary() if hasattr(brain, "get_state_summary") else {},
        "state": brain.state.to_dict()
        if hasattr(brain, "state") and hasattr(brain.state, "to_dict")
        else {},
    }


@app.get("/api/v2/errors")
async def get_orchestrator_errors():
    """UnifiedBrain ìµœê·¼ ì—ëŸ¬ ì¡°íšŒ"""
    brain = get_brain()
    return {
        "recent_errors": brain.get_recent_errors(limit=20)
        if hasattr(brain, "get_recent_errors")
        else [],
        "stats": brain.get_stats() if hasattr(brain, "get_stats") else {},
    }


@app.post("/api/v2/reset-errors")
async def reset_orchestrator_errors():
    """ì‹¤íŒ¨í•œ ì—ì´ì „íŠ¸ ëª©ë¡ ì´ˆê¸°í™”"""
    brain = get_brain()
    if hasattr(brain, "reset_failed_agents"):
        brain.reset_failed_agents()
    return {"status": "ok", "message": "Failed agents list cleared"}


@app.get("/api/crawl/status")
async def get_crawl_status():
    """
    í¬ë¡¤ë§ ìƒíƒœ ì¡°íšŒ

    Returns:
        - status: idle/running/completed/failed
        - date: í¬ë¡¤ë§ ëŒ€ìƒ ë‚ ì§œ
        - progress: ì§„í–‰ë¥  (0-100)
        - data_date: í˜„ì¬ ë°ì´í„° ë‚ ì§œ
        - needs_crawl: í¬ë¡¤ë§ í•„ìš” ì—¬ë¶€
    """
    crawl_manager = await get_crawl_manager()
    return {
        **crawl_manager.state.to_dict(),
        "data_date": crawl_manager.get_data_date(),
        "needs_crawl": crawl_manager.needs_crawl(),
        "is_today_available": crawl_manager.is_today_data_available(),
        "status_message": crawl_manager.get_status_message(),
    }


@app.post("/api/crawl/start", dependencies=[Depends(verify_api_key)])
async def start_crawl():
    """
    ìˆ˜ë™ìœ¼ë¡œ í¬ë¡¤ë§ ì‹œì‘ (API Key í•„ìš”)

    Returns:
        - started: í¬ë¡¤ë§ ì‹œì‘ ì—¬ë¶€
        - message: ìƒíƒœ ë©”ì‹œì§€
    """
    crawl_manager = await get_crawl_manager()

    if crawl_manager.is_crawling():
        return {
            "started": False,
            "message": "í¬ë¡¤ë§ì´ ì´ë¯¸ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤.",
            "status": crawl_manager.state.to_dict(),
        }

    if crawl_manager.is_today_data_available():
        return {
            "started": False,
            "message": "ì˜¤ëŠ˜ ë°ì´í„°ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.",
            "status": crawl_manager.state.to_dict(),
        }

    started = await crawl_manager.start_crawl()
    return {
        "started": started,
        "message": "í¬ë¡¤ë§ì„ ì‹œì‘í–ˆìŠµë‹ˆë‹¤." if started else "í¬ë¡¤ë§ ì‹œì‘ ì‹¤íŒ¨",
        "status": crawl_manager.state.to_dict(),
    }


# ============= Historical Data API =============

from datetime import UTC, timedelta

from src.tools.sheets_writer import SheetsWriter

# SheetsWriter ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_sheets_writer: SheetsWriter | None = None


def get_sheets_writer() -> SheetsWriter:
    """SheetsWriter ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _sheets_writer
    if _sheets_writer is None:
        _sheets_writer = SheetsWriter()
    return _sheets_writer


@app.get("/api/historical")
async def get_historical_data(
    start_date: str, end_date: str, category_id: str | None = None, brand: str | None = "LANEIGE"
):
    """
    íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„° ì¡°íšŒ (SQLite ìš°ì„ , Google Sheets fallback)

    Args:
        start_date: ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)
        end_date: ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)
        category_id: ì¹´í…Œê³ ë¦¬ í•„í„° (ì„ íƒ)
        brand: ë¸Œëœë“œ í•„í„° (ê¸°ë³¸ê°’: LANEIGE)

    Returns:
        - data: ë‚ ì§œë³„ ì§€í‘œ ë°ì´í„°
        - sos_history: SoS ì¶”ì´ ë°ì´í„°
        - raw_data: ìˆœìœ„ ì¶”ì´ ë°ì´í„°
    """
    try:
        records = []
        data_source = None

        # 1ì°¨: SQLiteì—ì„œ ì¡°íšŒ (ë¹ ë¦„)
        try:
            sqlite = get_sqlite_storage()
            await sqlite.initialize()
            records = await sqlite.get_raw_data(
                start_date=start_date,
                end_date=end_date,
                category_id=category_id,
                limit=50000,  # ì¶©ë¶„íˆ í° limit
            )
            if records:
                data_source = "sqlite"
                logging.info(
                    f"Historical: loaded {len(records)} records from SQLite ({start_date} ~ {end_date})"
                )
        except Exception as sqlite_err:
            logging.warning(f"Historical: SQLite ì¡°íšŒ ì‹¤íŒ¨: {sqlite_err}")

        # 2ì°¨: SQLite ì‹¤íŒ¨/ë¹ˆ ê²°ê³¼ ì‹œ Google Sheets fallback
        if not records:
            try:
                sheets_writer = get_sheets_writer()
                if not sheets_writer._initialized:
                    await sheets_writer.initialize()
                records = await sheets_writer.get_raw_data(
                    start_date=start_date, end_date=end_date, category_id=category_id
                )
                if records:
                    data_source = "sheets"
                    logging.info(
                        f"Historical: loaded {len(records)} records from Sheets ({start_date} ~ {end_date})"
                    )
            except Exception as sheets_err:
                logging.warning(f"Historical: Google Sheets ì¡°íšŒ ì‹¤íŒ¨: {sheets_err}")

        if not records:
            # ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ë°ì´í„° ì—†ìŒ - ë¡œì»¬ JSON íŒŒì¼ì—ì„œ ì‹œë„
            return await _get_historical_from_local(start_date, end_date, brand)

        # ë‚ ì§œ ë²”ìœ„ ê³„ì‚°
        start_dt = datetime.strptime(start_date, "%Y-%m-%d")
        end_dt = datetime.strptime(end_date, "%Y-%m-%d")
        days = (end_dt - start_dt).days + 1

        # ë‚ ì§œë³„ ë°ì´í„° ì§‘ê³„ (íŠ¹ì • ë¸Œëœë“œ í•„í„°ë§)
        daily_data = {}
        brand_lower = brand.lower() if brand else ""
        for record in records:
            snapshot_date = record.get("snapshot_date", "")
            if not snapshot_date or snapshot_date < start_date or snapshot_date > end_date:
                continue

            # íŠ¹ì • ë¸Œëœë“œ í•„í„°ë§ (SoS ì¶”ì´ ê³„ì‚°ìš©)
            record_brand = record.get("brand", "")
            if brand_lower and record_brand.lower() != brand_lower:
                continue

            if snapshot_date not in daily_data:
                daily_data[snapshot_date] = {
                    "date": snapshot_date,
                    "products": [],
                    "total_count": 0,
                    "top10_count": 0,
                }

            rank = int(record.get("rank", 0)) if record.get("rank") else 0
            daily_data[snapshot_date]["products"].append(
                {
                    "asin": record.get("asin", ""),
                    "product_name": record.get("product_name", ""),
                    "brand": record_brand,
                    "rank": rank,
                    "price": record.get("price", ""),
                    "rating": record.get("rating", ""),
                }
            )
            daily_data[snapshot_date]["total_count"] += 1
            if rank <= 10:
                daily_data[snapshot_date]["top10_count"] += 1

        # SoS ì¶”ì´ ê³„ì‚° (Top 100 ê¸°ì¤€, í•´ë‹¹ ë¸Œëœë“œ ê¸°ì¤€)
        sos_history = []
        raw_data = []
        for date_str in sorted(daily_data.keys()):
            day_data = daily_data[date_str]
            products = day_data["products"]

            # SoS = (ë¸Œëœë“œ ì œí’ˆ ìˆ˜ / 100) * 100
            sos = round(len(products) / 100 * 100, 1) if products else 0
            sos_history.append(
                {
                    "date": date_str,
                    "sos": sos,
                    "product_count": len(products),
                    "top10_count": day_data["top10_count"],
                }
            )

            # í‰ê·  ìˆœìœ„ (ìˆëŠ” ê²½ìš°)
            if products:
                avg_rank = round(sum(p["rank"] for p in products) / len(products), 1)
                raw_data.append(
                    {
                        "date": date_str,
                        "rank": avg_rank,
                        "best_rank": min(p["rank"] for p in products),
                        "worst_rank": max(p["rank"] for p in products),
                    }
                )

        # available_dates ê³„ì‚°
        available_dates = sorted(daily_data.keys())

        # brand_metrics ê³„ì‚° (ì „ì²´ ê¸°ê°„ í†µí•© - ëª¨ë“  ë¸Œëœë“œ í¬í•¨)
        brand_metrics = await _calculate_brand_metrics_for_period(records, daily_data, brand)

        # rank_history ìƒì„± (Product View ì°¨íŠ¸ìš©)
        # í˜•ì‹: { "2026-01-14": { "products": [{ "name": "...", "rank": 5, "price": 21.5 }, ...] } }
        rank_history = {}
        for record in records:
            snapshot_date = record.get("snapshot_date", "")
            if not snapshot_date or snapshot_date < start_date or snapshot_date > end_date:
                continue

            if snapshot_date not in rank_history:
                rank_history[snapshot_date] = {"products": []}

            rank = int(record.get("rank", 0)) if record.get("rank") else 0
            price_val = record.get("price", 0)
            try:
                price = float(str(price_val).replace("$", "").replace(",", "")) if price_val else 0
            except (ValueError, TypeError):
                price = 0

            rank_history[snapshot_date]["products"].append(
                {
                    "name": record.get("product_name", ""),
                    "product_name": record.get("product_name", ""),
                    "brand": record.get("brand", ""),
                    "asin": record.get("asin", ""),
                    "rank": rank,
                    "price": price,
                    "rating": record.get("rating", ""),
                    "discount_percent": record.get("discount_percent", 0),
                }
            )

        # ì „ì²´ ë°ì´í„°ì˜ ì‚¬ìš© ê°€ëŠ¥í•œ ë‚ ì§œ ë²”ìœ„ ì¡°íšŒ (SQLiteì—ì„œ)
        available_date_range = {"min": None, "max": None}
        try:
            sqlite = get_sqlite_storage()
            stats = sqlite.get_stats()
            if "date_range" in stats:
                available_date_range = stats["date_range"]
        except Exception:
            pass

        return {
            "success": True,
            "available_dates": available_dates,
            "available_date_range": available_date_range,
            "data_source": data_source,
            "brand_metrics": brand_metrics,
            "rank_history": rank_history,
            "data": {
                "sos_history": sos_history,
                "raw_data": raw_data,
                "daily_data": list(daily_data.values()),
                "period": {"start": start_date, "end": end_date, "days": days},
                "brand": brand,
            },
        }

    except Exception as e:
        logging.error(f"Historical data error: {e}")
        # í´ë°±: ë¡œì»¬ ë°ì´í„°ì—ì„œ ì‹œë„
        return await _get_historical_from_local(start_date, end_date, brand)


async def _calculate_brand_metrics_for_period(
    records: list[dict], daily_data: dict, target_brand: str
) -> list[dict]:
    """
    ê¸°ê°„ ë‚´ ëª¨ë“  ë¸Œëœë“œì˜ ë©”íŠ¸ë¦­ ê³„ì‚° (SoS Ã— Avg Rank ì°¨íŠ¸ìš©)

    Note:
        ê¸°ê°„ ì¡°íšŒ ì‹œ ë™ì¼ ASINì´ ì—¬ëŸ¬ ë‚ ì§œì— ì¤‘ë³µ ë“±ì¥í•˜ë¯€ë¡œ,
        ASIN ê¸°ì¤€ ìœ ë‹ˆí¬ ì¹´ìš´íŠ¸ë¥¼ ì ìš©í•˜ì—¬ ì •í™•í•œ ì œí’ˆ ìˆ˜ ê³„ì‚°

    Returns:
        ë¸Œëœë“œë³„ SoS, í‰ê·  ìˆœìœ„, ì œí’ˆ ìˆ˜ ë“±
    """
    # ì „ì²´ ì œí’ˆ ë°ì´í„° ì§‘ê³„ (ëª¨ë“  ë¸Œëœë“œ)
    brand_data = {}
    brand_unique_asins: dict[str, set] = {}  # ASIN ì¤‘ë³µ ì œê±°ìš©

    for record in records:
        brand_name = record.get("brand", "Unknown")
        asin = record.get("asin", "")
        rank = int(record.get("rank", 0)) if record.get("rank") else 0

        # Unknown ë¸Œëœë“œ ë° ë¹ˆ ë¸Œëœë“œ ì œì™¸ (ëŒ€ì‹œë³´ë“œì—ì„œ ì˜ë¯¸ ì—†ìŒ)
        if not brand_name or brand_name.lower() == "unknown" or rank == 0:
            continue

        if brand_name not in brand_data:
            brand_data[brand_name] = {"brand": brand_name, "ranks": [], "product_count": 0}
            brand_unique_asins[brand_name] = set()

        # ìˆœìœ„ëŠ” ëª¨ë“  ë ˆì½”ë“œì—ì„œ ìˆ˜ì§‘ (í‰ê·  ê³„ì‚°ìš©)
        brand_data[brand_name]["ranks"].append(rank)

        # ì œí’ˆ ìˆ˜ëŠ” ASIN ê¸°ì¤€ ìœ ë‹ˆí¬ ì¹´ìš´íŠ¸ (ì¤‘ë³µ ì œê±°)
        if asin and asin not in brand_unique_asins[brand_name]:
            brand_unique_asins[brand_name].add(asin)
            brand_data[brand_name]["product_count"] += 1
        elif not asin:
            # ASINì´ ì—†ëŠ” ê²½ìš° ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì¹´ìš´íŠ¸ (í´ë°±)
            brand_data[brand_name]["product_count"] += 1

    # ì´ ìœ ë‹ˆí¬ ì œí’ˆ ìˆ˜ (ëª¨ë“  ë¸Œëœë“œ - Unknown ì œì™¸ í›„)
    total_products = sum(b["product_count"] for b in brand_data.values())

    # ë©”íŠ¸ë¦­ ê³„ì‚°
    brand_metrics = []
    for brand_name, data in brand_data.items():
        if not data["ranks"]:
            continue

        sos = round(data["product_count"] / max(total_products, 100) * 100, 2)
        avg_rank = round(sum(data["ranks"]) / len(data["ranks"]), 1)

        # ë²„ë¸” í¬ê¸°: ì œí’ˆ ìˆ˜ ê¸°ë°˜ (ìµœì†Œ 5, ìµœëŒ€ 25)
        bubble_size = max(5, min(25, data["product_count"] * 2))

        is_laneige = target_brand.upper() in brand_name.upper()

        brand_metrics.append(
            {
                "brand": brand_name,
                "sos": sos,
                "avg_rank": avg_rank,
                "product_count": data["product_count"],
                "bubble_size": bubble_size,
                "is_laneige": is_laneige,
            }
        )

    # SoS ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    brand_metrics.sort(key=lambda x: x["sos"], reverse=True)

    # ìƒìœ„ 10ê°œ ì¶”ì¶œ
    top_10 = brand_metrics[:10]

    # LANEIGEê°€ top_10ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ (brand_dataì— ìˆëŠ”ì§€ê°€ ì•„ë‹ˆë¼ top_10ì— ìˆëŠ”ì§€!)
    laneige_in_top10 = any(b.get("is_laneige") for b in top_10)

    # LANEIGEê°€ top_10ì— ì—†ìœ¼ë©´ ì¶”ê°€ (ë°ì´í„°ê°€ ì¡´ì¬í•  ê²½ìš°)
    if not laneige_in_top10 and target_brand:
        # brand_dataì—ì„œ LANEIGE ì°¾ê¸° (ëŒ€ì†Œë¬¸ì ë³€í˜• ëª¨ë‘ ì‹œë„)
        laneige_data = None
        for key in [
            target_brand,
            target_brand.upper(),
            target_brand.lower(),
            target_brand.capitalize(),
        ]:
            if key in brand_data:
                laneige_data = brand_data[key]
                break

        if laneige_data and laneige_data["ranks"]:
            sos = round(laneige_data["product_count"] / max(total_products, 100) * 100, 2)
            avg_rank = round(sum(laneige_data["ranks"]) / len(laneige_data["ranks"]), 1)
            bubble_size = max(5, min(25, laneige_data["product_count"] * 2))
            top_10.append(
                {
                    "brand": target_brand,
                    "sos": sos,
                    "avg_rank": avg_rank,
                    "product_count": laneige_data["product_count"],
                    "bubble_size": bubble_size,
                    "is_laneige": True,
                }
            )
            # ë‹¤ì‹œ ì •ë ¬ í›„ ìƒìœ„ 11ê°œ ìœ ì§€ (LANEIGE í¬í•¨ ë³´ì¥)
            top_10.sort(key=lambda x: x["sos"], reverse=True)

    # Summer Fridays íŠ¹ë³„ ì²˜ë¦¬ (ê³ ê° ìš”ì²­ tracked competitor)
    # top_10ì— ì—†ìœ¼ë©´ ê°•ì œ ì¶”ê°€
    TRACKED_COMPETITORS = ["Summer Fridays"]
    for tracked_brand in TRACKED_COMPETITORS:
        tracked_in_top = any(b.get("brand") == tracked_brand for b in top_10)
        if not tracked_in_top and tracked_brand in brand_data:
            tracked_data = brand_data[tracked_brand]
            if tracked_data["ranks"]:
                sos = round(tracked_data["product_count"] / max(total_products, 100) * 100, 2)
                avg_rank = round(sum(tracked_data["ranks"]) / len(tracked_data["ranks"]), 1)
                bubble_size = max(5, min(25, tracked_data["product_count"] * 2))
                top_10.append(
                    {
                        "brand": tracked_brand,
                        "sos": sos,
                        "avg_rank": avg_rank,
                        "product_count": tracked_data["product_count"],
                        "bubble_size": bubble_size,
                        "is_laneige": False,
                        "is_tracked": True,  # tracked competitor í‘œì‹œ
                    }
                )
        # ë°ì´í„°ê°€ ì—†ì–´ë„ placeholder ì¶”ê°€ (UIì—ì„œ "-" ëŒ€ì‹  "ë°ì´í„° ì—†ìŒ" í‘œì‹œ ê°€ëŠ¥)
        elif not tracked_in_top:
            top_10.append(
                {
                    "brand": tracked_brand,
                    "sos": 0,
                    "avg_rank": None,
                    "product_count": 0,
                    "bubble_size": 5,
                    "is_laneige": False,
                    "is_tracked": True,
                    "no_data": True,  # í•´ë‹¹ ê¸°ê°„ ë°ì´í„° ì—†ìŒ í‘œì‹œ
                }
            )

    # ìµœì¢… ì •ë ¬ (SoS ë‚´ë¦¼ì°¨ìˆœ, trackedëŠ” í•˜ë‹¨ì— ìœ ì§€)
    top_10.sort(key=lambda x: (not x.get("is_tracked", False), x["sos"]), reverse=True)

    return top_10


def _get_brand_metrics_from_dashboard(dashboard_data: dict | None, target_brand: str) -> list[dict]:
    """
    ëŒ€ì‹œë³´ë“œ ë°ì´í„°ì—ì„œ ë¸Œëœë“œ ë©”íŠ¸ë¦­ ì¶”ì¶œ (ë¡œì»¬ í´ë°±ìš©)
    """
    if not dashboard_data:
        return []

    # ëŒ€ì‹œë³´ë“œì˜ brand_matrix ë°ì´í„° ì‚¬ìš©
    brand_matrix = dashboard_data.get("charts", {}).get("brand_matrix", [])
    if brand_matrix:
        return brand_matrix

    # ê²½ìŸì‚¬ ë°ì´í„°ì—ì„œ ìƒì„±
    competitors = dashboard_data.get("brand", {}).get("competitors", [])
    if not competitors:
        return []

    brand_metrics = []
    for comp in competitors:
        brand_metrics.append(
            {
                "brand": comp.get("brand", "Unknown"),
                "sos": comp.get("sos", 0),
                "avg_rank": comp.get("avg_rank", 50),
                "product_count": comp.get("product_count", 0),
                "bubble_size": max(5, min(25, comp.get("product_count", 0) * 2)),
                "is_laneige": target_brand.upper() in comp.get("brand", "").upper(),
            }
        )

    return brand_metrics


async def _get_historical_from_local(
    start_date: str, end_date: str, brand: str = "LANEIGE"
) -> dict[str, Any]:
    """
    ë¡œì»¬ JSON íŒŒì¼ì—ì„œ íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„° ì¡°íšŒ (í´ë°±)

    data/ í´ë”ì˜ ë‚ ì§œë³„ JSON íŒŒì¼ì´ë‚˜ dashboard_data.jsonì˜ íˆìŠ¤í† ë¦¬ ë°ì´í„° í™œìš©
    """
    try:
        # ë©”ì¸ ëŒ€ì‹œë³´ë“œ ë°ì´í„° ë¡œë“œ
        data = load_dashboard_data()
        sos_history = []
        raw_data = []

        # 1. ëŒ€ì‹œë³´ë“œ ë°ì´í„°ì—ì„œ í˜„ì¬ SoS/ìˆœìœ„ ì •ë³´ ì¶”ì¶œ
        if data:
            brand_kpis = data.get("brand", {}).get("kpis", {})
            current_sos = brand_kpis.get("sos", 0)
            data_date = data.get("metadata", {}).get(
                "data_date", datetime.now().strftime("%Y-%m-%d")
            )

            # í˜„ì¬ ë‚ ì§œê°€ ìš”ì²­ ë²”ìœ„ì— í¬í•¨ë˜ë©´ ì¶”ê°€
            if start_date <= data_date <= end_date:
                sos_history.append(
                    {
                        "date": data_date,
                        "sos": current_sos,
                        "product_count": brand_kpis.get("product_count", 0),
                        "top10_count": brand_kpis.get("top10_count", 0),
                    }
                )

                avg_rank = brand_kpis.get("avg_rank", 0)
                if avg_rank:
                    raw_data.append(
                        {
                            "date": data_date,
                            "rank": avg_rank,
                            "best_rank": brand_kpis.get("best_rank", avg_rank),
                            "worst_rank": brand_kpis.get("worst_rank", avg_rank),
                        }
                    )

        # 2. latest_crawl_result.jsonì—ì„œ ë°ì´í„° ì¶”ì¶œ
        latest_crawl_path = Path("./data/latest_crawl_result.json")
        if latest_crawl_path.exists():
            try:
                with open(latest_crawl_path, encoding="utf-8") as f:
                    crawl_data = json.load(f)

                # ëª¨ë“  ì¹´í…Œê³ ë¦¬ì—ì„œ ë¸Œëœë“œ ì œí’ˆ ì°¾ê¸°
                brand_products = []
                crawl_date = None

                for _cat_id, cat_data in crawl_data.get("categories", {}).items():
                    for product in cat_data.get("products", []):
                        product_brand = product.get("brand", "")
                        product_name = product.get("product_name", "")

                        # ë¸Œëœë“œ ë§¤ì¹­ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ, ë¶€ë¶„ ë§¤ì¹­)
                        if (
                            brand.upper() in product_brand.upper()
                            or brand.upper() in product_name.upper()
                        ):
                            brand_products.append(product)
                            if not crawl_date:
                                crawl_date = product.get("snapshot_date")

                if brand_products and crawl_date and start_date <= crawl_date <= end_date:
                    # ì¤‘ë³µ ì œê±° í™•ì¸
                    if not any(h["date"] == crawl_date for h in sos_history):
                        # ì¹´í…Œê³ ë¦¬ë³„ ì´ ì œí’ˆ ìˆ˜ (Top 100 ê¸°ì¤€)
                        total_products = sum(
                            len(cat.get("products", []))
                            for cat in crawl_data.get("categories", {}).values()
                        )

                        sos = round(len(brand_products) / max(total_products, 100) * 100, 2)
                        avg_rank = round(
                            sum(p.get("rank", 0) for p in brand_products) / len(brand_products), 1
                        )

                        sos_history.append(
                            {
                                "date": crawl_date,
                                "sos": sos,
                                "product_count": len(brand_products),
                                "top10_count": sum(
                                    1 for p in brand_products if p.get("rank", 100) <= 10
                                ),
                            }
                        )
                        raw_data.append(
                            {
                                "date": crawl_date,
                                "rank": avg_rank,
                                "best_rank": min(p.get("rank", 100) for p in brand_products),
                                "worst_rank": max(p.get("rank", 100) for p in brand_products),
                            }
                        )

            except (json.JSONDecodeError, ValueError) as e:
                logging.warning(f"Failed to parse latest_crawl_result.json: {e}")

        # 3. raw_products í´ë”ì—ì„œ ë‚ ì§œë³„ ë°ì´í„° ê²€ìƒ‰ (ê¸°ì¡´ ë¡œì§)
        raw_data_dir = Path("./data/raw_products")
        if raw_data_dir.exists():
            for json_file in raw_data_dir.glob("*.json"):
                try:
                    file_date = json_file.stem  # íŒŒì¼ëª…ì´ YYYY-MM-DD í˜•ì‹ì´ë¼ê³  ê°€ì •
                    if start_date <= file_date <= end_date:
                        with open(json_file, encoding="utf-8") as f:
                            daily_raw = json.load(f)

                        # ë¸Œëœë“œ ì œí’ˆë§Œ í•„í„°ë§
                        brand_products = [
                            p
                            for p in daily_raw
                            if brand.upper() in p.get("brand", "").upper()
                            or brand.upper() in p.get("product_name", "").upper()
                        ]

                        if brand_products:
                            sos = round(len(brand_products) / 100 * 100, 1)
                            avg_rank = round(
                                sum(p.get("rank", 0) for p in brand_products) / len(brand_products),
                                1,
                            )

                            # ì¤‘ë³µ ì œê±°
                            if not any(h["date"] == file_date for h in sos_history):
                                sos_history.append(
                                    {
                                        "date": file_date,
                                        "sos": sos,
                                        "product_count": len(brand_products),
                                        "top10_count": sum(
                                            1 for p in brand_products if p.get("rank", 100) <= 10
                                        ),
                                    }
                                )
                                raw_data.append(
                                    {
                                        "date": file_date,
                                        "rank": avg_rank,
                                        "best_rank": min(
                                            p.get("rank", 100) for p in brand_products
                                        ),
                                        "worst_rank": max(
                                            p.get("rank", 100) for p in brand_products
                                        ),
                                    }
                                )
                except (json.JSONDecodeError, ValueError):
                    continue

        # ë‚ ì§œìˆœ ì •ë ¬
        sos_history.sort(key=lambda x: x["date"])
        raw_data.sort(key=lambda x: x["date"])

        # available_dates ê³„ì‚°
        available_dates = [h["date"] for h in sos_history]

        # brand_metrics ê³„ì‚° (í˜„ì¬ ëŒ€ì‹œë³´ë“œ ë°ì´í„°ì—ì„œ)
        brand_metrics = _get_brand_metrics_from_dashboard(data, brand)

        if not sos_history:
            return {
                "success": False,
                "error": "No historical data found for the specified period",
                "available_dates": [],
                "brand_metrics": [],
                "data": None,
            }

        return {
            "success": True,
            "available_dates": available_dates,
            "brand_metrics": brand_metrics,
            "data": {
                "sos_history": sos_history,
                "raw_data": raw_data,
                "period": {"start": start_date, "end": end_date},
                "brand": brand,
                "source": "local",
            },
        }

    except Exception as e:
        logging.error(f"Local historical data error: {e}")
        return {
            "success": False,
            "error": str(e),
            "available_dates": [],
            "brand_metrics": [],
            "data": None,
        }


@app.post("/api/export/docx")
async def export_docx(request: ExportRequest):
    """
    ì¸ì‚¬ì´íŠ¸ ë¦¬í¬íŠ¸ DOCX ìƒì„± ë° ë‹¤ìš´ë¡œë“œ
    """
    data = load_dashboard_data()
    if not data:
        raise HTTPException(status_code=404, detail="Dashboard data not found")

    # DOCX ë¬¸ì„œ ìƒì„±
    doc = Document()

    # ìŠ¤íƒ€ì¼ ì„¤ì •
    style = doc.styles["Normal"]
    font = style.font
    font.name = "Arial"
    font.size = Pt(11)

    # ===== í‘œì§€ =====
    title = doc.add_heading("AMORE INSIGHT Report", 0)
    title.alignment = WD_ALIGN_PARAGRAPH.CENTER

    subtitle = doc.add_paragraph("LANEIGE Amazon US ë¶„ì„ ë¦¬í¬íŠ¸")
    subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER

    # ë‚ ì§œ
    metadata = data.get("metadata", {})
    date_para = doc.add_paragraph()
    date_para.alignment = WD_ALIGN_PARAGRAPH.CENTER
    date_para.add_run(
        f"ë¶„ì„ ê¸°ì¤€ì¼: {metadata.get('data_date', datetime.now().strftime('%Y-%m-%d'))}"
    )
    date_para.add_run(f"\nìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M')}")

    doc.add_page_break()

    # ===== 1. Executive Summary =====
    doc.add_heading("1. Executive Summary", level=1)

    brand_kpis = data.get("brand", {}).get("kpis", {})
    home_status = data.get("home", {}).get("status", {})

    summary_text = f"""
LANEIGE ë¸Œëœë“œëŠ” Amazon US ì‹œì¥ì—ì„œ {home_status.get('exposure', 'N/A')} ìƒíƒœì…ë‹ˆë‹¤.

â€¢ Share of Shelf (SoS): {brand_kpis.get('sos', 0)}%
â€¢ Top 10 ì§„ì… ì œí’ˆ: {brand_kpis.get('top10_count', 0)}ê°œ
â€¢ í‰ê·  ìˆœìœ„: {brand_kpis.get('avg_rank', 0)}ìœ„
â€¢ ì‹œì¥ ì§‘ì¤‘ë„ (HHI): {brand_kpis.get('hhi', 0)}

í˜„ì¬ ì‹œì¥ í¬ì§€ì…˜: {home_status.get('position', 'N/A')}
ì£¼ì˜ í•„ìš” ì œí’ˆ: {home_status.get('warning_count', 0)}ê°œ
"""
    doc.add_paragraph(summary_text)

    # ===== 2. ì œí’ˆë³„ í˜„í™© =====
    doc.add_heading("2. LANEIGE ì œí’ˆ í˜„í™©", level=1)

    products = data.get("products", {})
    if products:
        # í…Œì´ë¸” ìƒì„±
        table = doc.add_table(rows=1, cols=5)
        table.style = "Table Grid"
        table.alignment = WD_TABLE_ALIGNMENT.CENTER

        # í—¤ë”
        header_cells = table.rows[0].cells
        headers = ["ì œí’ˆëª…", "ìˆœìœ„", "ë³€ë™", "í‰ì ", "ë³€ë™ì„±"]
        for i, header in enumerate(headers):
            header_cells[i].text = header
            header_cells[i].paragraphs[0].runs[0].bold = True

        # ë°ì´í„° í–‰
        for _asin, product in products.items():
            row = table.add_row().cells
            row[0].text = product.get("name", "")[:40]
            row[1].text = f"#{product.get('rank', 'N/A')}"
            row[2].text = product.get("rank_delta", "-")
            row[3].text = str(product.get("rating", "-"))
            row[4].text = product.get("volatility_status", "-")

    doc.add_paragraph()

    # ===== 3. ê²½ìŸì‚¬ ë¶„ì„ =====
    doc.add_heading("3. ê²½ìŸì‚¬ ë¶„ì„", level=1)

    competitors = data.get("brand", {}).get("competitors", [])
    if competitors:
        table = doc.add_table(rows=1, cols=4)
        table.style = "Table Grid"

        header_cells = table.rows[0].cells
        headers = ["ë¸Œëœë“œ", "SoS (%)", "í‰ê·  ìˆœìœ„", "ì œí’ˆ ìˆ˜"]
        for i, header in enumerate(headers):
            header_cells[i].text = header
            header_cells[i].paragraphs[0].runs[0].bold = True

        for comp in competitors[:10]:
            row = table.add_row().cells
            row[0].text = comp.get("brand", "")
            row[1].text = str(comp.get("sos", 0))
            row[2].text = str(comp.get("avg_rank", "-"))
            row[3].text = str(comp.get("product_count", 0))

    doc.add_paragraph()

    # ===== 4. ì•¡ì…˜ ì•„ì´í…œ =====
    doc.add_heading("4. ì•¡ì…˜ ì•„ì´í…œ", level=1)

    action_items = data.get("home", {}).get("action_items", [])
    if action_items:
        for item in action_items:
            priority_marker = "ğŸ”´" if item.get("priority") == "P1" else "ğŸŸ "
            para = doc.add_paragraph()
            para.add_run(f"{priority_marker} [{item.get('priority')}] ").bold = True
            para.add_run(f"{item.get('product_name', '')}\n")
            para.add_run(f"   ì‹ í˜¸: {item.get('signal', '')}\n")
            para.add_run(f"   ê¶Œì¥ ì•¡ì…˜: {item.get('action_tag', '')}")
    else:
        doc.add_paragraph("í˜„ì¬ íŠ¹ë³„í•œ ì•¡ì…˜ ì•„ì´í…œì´ ì—†ìŠµë‹ˆë‹¤.")

    # ===== 5. ì „ëµì  ê¶Œê³ ì‚¬í•­ =====
    if request.include_strategy:
        doc.add_heading("5. ì „ëµì  ê¶Œê³ ì‚¬í•­", level=1)

        # ChatGPTë¡œ ì „ëµ ìƒì„± (RAG ì»¨í…ìŠ¤íŠ¸ í™œìš©)
        try:
            # RAGì—ì„œ ì „ëµ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
            strategy_context, _ = await get_rag_context("ì „ëµ ì•¡ì…˜ ê¶Œê³ ", QueryType.ANALYSIS)

            strategy_prompt = f"""ë‹¤ìŒ ë°ì´í„°ì™€ ê°€ì´ë“œë¼ì¸ì„ ë°”íƒ•ìœ¼ë¡œ LANEIGE ë¸Œëœë“œì˜ ì „ëµì  ê¶Œê³ ì‚¬í•­ 3ê°€ì§€ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

ë°ì´í„°:
- SoS: {brand_kpis.get('sos', 0)}%
- Top 10 ì œí’ˆ: {brand_kpis.get('top10_count', 0)}ê°œ
- í‰ê·  ìˆœìœ„: {brand_kpis.get('avg_rank', 0)}ìœ„
- ì£¼ìš” ê²½ìŸì‚¬: {', '.join([c['brand'] for c in competitors[:3]])}

ì°¸ê³  ê°€ì´ë“œë¼ì¸:
{strategy_context if strategy_context else 'ê¸°ë³¸ ì „ëµ ê¸°ì¤€ ì ìš©'}

ê° ê¶Œê³ ì‚¬í•­ì€ 1-2ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
"""
            response = await acompletion(
                model="gpt-4.1-mini",
                messages=[
                    {
                        "role": "system",
                        "content": "ë‹¹ì‹ ì€ ë·°í‹° ì´ì»¤ë¨¸ìŠ¤ ì „ë¬¸ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤. ê°„ê²°í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì „ëµì„ ì œì•ˆí•©ë‹ˆë‹¤.",
                    },
                    {"role": "user", "content": strategy_prompt},
                ],
                temperature=0.3,
                max_tokens=500,
            )

            strategy_text = response.choices[0].message.content
            doc.add_paragraph(strategy_text)

        except Exception:
            # í´ë°± ì „ëµ
            doc.add_paragraph("""
1. Top 10 ìœ ì§€ ì „ëµ: í˜„ì¬ ìƒìœ„ê¶Œ ì œí’ˆì˜ ë¦¬ë·° ê´€ë¦¬ ë° ì¬ê³  í™•ë³´ë¥¼ í†µí•œ í¬ì§€ì…˜ ìœ ì§€

2. ê²½ìŸì‚¬ ëª¨ë‹ˆí„°ë§: e.l.f., Maybelline ë“± ì£¼ìš” ê²½ìŸì‚¬ì˜ ê°€ê²© ë° í”„ë¡œëª¨ì…˜ ë™í–¥ íŒŒì•…

3. ì‹ ê·œ ì§„ì… ê¸°íšŒ: Lip Care ì¹´í…Œê³ ë¦¬ ì™¸ Face Powder, Toner ë“± í™•ì¥ ê°€ëŠ¥ì„± ê²€í† 
""")

    # ===== í‘¸í„° =====
    doc.add_paragraph()
    footer = doc.add_paragraph()
    footer.alignment = WD_ALIGN_PARAGRAPH.CENTER
    footer.add_run("Â© 2025 AMORE Pacific - Confidential").italic = True

    # BytesIOë¡œ ì €ì¥
    buffer = BytesIO()
    doc.save(buffer)
    buffer.seek(0)

    # íŒŒì¼ëª… ìƒì„±
    filename = f"AMORE_Insight_Report_{datetime.now().strftime('%Y%m%d_%H%M')}.docx"

    return StreamingResponse(
        buffer,
        media_type="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
        headers={"Content-Disposition": f"attachment; filename={filename}"},
    )


class AnalystReportRequest(BaseModel):
    """ì• ë„ë¦¬ìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìš”ì²­"""

    start_date: str  # Required: YYYY-MM-DD
    end_date: str  # Required: YYYY-MM-DD
    include_charts: bool = True
    include_external_signals: bool = True


@app.post("/api/export/excel")
async def export_excel(request: Request):
    """
    ì—‘ì…€ ë°ì´í„° ë‚´ë³´ë‚´ê¸° (JSON íŒŒì¼ â†’ Excel)

    ë°ì´í„° ì†ŒìŠ¤:
    - Railway: /data/latest_crawl_result.json (Volume)
    - Local: ./data/latest_crawl_result.json
    """

    import pandas as pd

    try:
        # Parse request body
        body = await request.json()
        start_date = body.get("start_date")
        end_date = body.get("end_date")
        _include_metrics = body.get("include_metrics", True)  # reserved for future use

        # ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì •
        data_dir = Path("./data")

        # ========================================
        # 1ì°¨: SQLiteì—ì„œ ê¸°ê°„ë³„ ë°ì´í„° ì¡°íšŒ (ê°€ì¥ ë¹ ë¦„)
        # ========================================
        all_records = []
        data_source = None

        if start_date and end_date:
            # 1-1. SQLite ì‹œë„
            try:
                from src.tools.sqlite_storage import get_sqlite_storage

                sqlite = get_sqlite_storage()
                await sqlite.initialize()

                # limitì„ í¬ê²Œ ì„¤ì • (5ê°œ ì¹´í…Œê³ ë¦¬ Ã— 100ê°œ Ã— ê¸°ê°„ì¼ìˆ˜)
                start_dt = datetime.strptime(start_date, "%Y-%m-%d")
                end_dt = datetime.strptime(end_date, "%Y-%m-%d")
                days = (end_dt - start_dt).days + 1
                max_records = 500 * days  # ì¶©ë¶„í•œ ì—¬ìœ 

                records = await sqlite.get_raw_data(
                    start_date=start_date, end_date=end_date, limit=max_records
                )

                if records:
                    all_records = records
                    data_source = "sqlite"
                    logging.info(
                        f"Excel export: loaded {len(all_records)} records from SQLite ({start_date} ~ {end_date})"
                    )

            except Exception as sqlite_err:
                logging.warning(f"Excel export: SQLite ì¡°íšŒ ì‹¤íŒ¨: {sqlite_err}")

            # 1-2. SQLite ì‹¤íŒ¨ ì‹œ Google Sheets ì‹œë„
            if not all_records:
                try:
                    sheets_writer = get_sheets_writer()
                    if not sheets_writer._initialized:
                        await sheets_writer.initialize()

                    records = await sheets_writer.get_raw_data(days=days)

                    if records:
                        for record in records:
                            snapshot_date = record.get("snapshot_date", "")
                            if snapshot_date and start_date <= snapshot_date <= end_date:
                                all_records.append(record)

                        if all_records:
                            data_source = "sheets"
                            logging.info(
                                f"Excel export: loaded {len(all_records)} records from Google Sheets ({start_date} ~ {end_date})"
                            )

                except Exception as sheets_err:
                    logging.warning(f"Excel export: Google Sheets ì¡°íšŒ ì‹¤íŒ¨: {sheets_err}")

        # ========================================
        # 2ì°¨: ë¡œì»¬ JSON íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ (í´ë°±)
        # ========================================
        crawl_data = None
        json_path = None

        if not data_source:
            possible_paths = [
                data_dir / "latest_crawl_result.json",
                data_dir / "dashboard_data.json",
            ]

            for path in possible_paths:
                if path.exists():
                    json_path = path
                    break

            if json_path is None:
                raise HTTPException(
                    status_code=404, detail="í¬ë¡¤ë§ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € í¬ë¡¤ë§ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”."
                )

            with open(json_path, encoding="utf-8") as f:
                crawl_data = json.load(f)

            logging.info(f"Excel export: loaded data from {json_path}")

        # ë°ì´í„° ì†ŒìŠ¤ ìœ í˜• íŒë‹¨
        # 1. data_source: SQLite ë˜ëŠ” Google Sheetsì—ì„œ ê¸°ê°„ë³„ ë°ì´í„° ë¡œë“œë¨
        # 2. is_crawl_data: latest_crawl_result.jsonì˜ raw ë°ì´í„°
        # 3. is_dashboard_data: dashboard_data.jsonì˜ ì§‘ê³„ ë°ì´í„°
        is_crawl_data = False
        is_dashboard_data = False

        if crawl_data:
            if "categories" in crawl_data:
                first_cat = next(iter(crawl_data["categories"].values()), {})
                is_crawl_data = isinstance(first_cat, dict) and (
                    "rank_records" in first_cat or "products" in first_cat
                )
            is_dashboard_data = "metadata" in crawl_data and "brand" in crawl_data

        # ì¶œë ¥ ê²½ë¡œ (Railway í™˜ê²½ ê³ ë ¤)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = data_dir / "exports"
        output_dir.mkdir(parents=True, exist_ok=True)
        output_path = output_dir / f"AMORE_Data_{timestamp}.xlsx"

        sheets_created = []
        total_rows = 0

        # ì¹´í…Œê³ ë¦¬ ë§¤í•‘
        categories_info = {
            "beauty": "Beauty & Personal Care",
            "skin_care": "Skin Care",
            "lip_care": "Lip Care",
            "lip_makeup": "Lip Makeup",
            "face_powder": "Face Powder",
        }

        with pd.ExcelWriter(str(output_path), engine="openpyxl") as writer:
            # Google Sheets RawDataì™€ ë™ì¼í•œ ì»¬ëŸ¼ ìˆœì„œ
            RAWDATA_COLUMNS = [
                "snapshot_date",
                "category_id",
                "rank",
                "asin",
                "product_name",
                "brand",
                "price",
                "list_price",
                "discount_percent",
                "rating",
                "reviews_count",
                "badge",
                "coupon_text",
                "is_subscribe_save",
                "promo_badges",
                "product_url",
            ]

            # ========================================
            # Case 1: SQLite/Google Sheetsì—ì„œ ê¸°ê°„ë³„ ë°ì´í„° ë¡œë“œë¨
            # ========================================
            if data_source and all_records:
                source_name = "SQLite" if data_source == "sqlite" else "Google Sheets"
                logging.info(
                    f"Excel export: using {source_name} data ({len(all_records)} records, {start_date} ~ {end_date})"
                )

                df_all = pd.DataFrame(all_records)

                if not df_all.empty:
                    # 1. RawData ì‹œíŠ¸ - ì „ì²´ ë°ì´í„°
                    available_cols = [c for c in RAWDATA_COLUMNS if c in df_all.columns]
                    df_raw = df_all[available_cols].copy()
                    df_raw = df_raw.sort_values(["snapshot_date", "category_id", "rank"])
                    df_raw.to_excel(writer, sheet_name="RawData", index=False)
                    sheets_created.append("RawData")
                    total_rows += len(df_raw)

                    # 2. ë‚ ì§œë³„ ìš”ì•½ ì‹œíŠ¸
                    if "snapshot_date" in df_all.columns:
                        date_summary = []
                        for date in sorted(df_all["snapshot_date"].unique()):
                            df_date = df_all[df_all["snapshot_date"] == date]
                            laneige_count = (
                                len(df_date[df_date["brand"].str.upper() == "LANEIGE"])
                                if "brand" in df_date.columns
                                else 0
                            )
                            date_summary.append(
                                {
                                    "ë‚ ì§œ": date,
                                    "ì´ ì œí’ˆ ìˆ˜": len(df_date),
                                    "LANEIGE ì œí’ˆ ìˆ˜": laneige_count,
                                    "LANEIGE SoS (%)": round(laneige_count / len(df_date) * 100, 1)
                                    if len(df_date) > 0
                                    else 0,
                                }
                            )
                        if date_summary:
                            df_summary = pd.DataFrame(date_summary)
                            df_summary.to_excel(writer, sheet_name="Daily Summary", index=False)
                            sheets_created.append("Daily Summary")
                            total_rows += len(df_summary)

                    # 3. ì¹´í…Œê³ ë¦¬ë³„ ì‹œíŠ¸
                    if "category_id" in df_all.columns:
                        for cat_id in df_all["category_id"].unique():
                            df_cat = df_all[df_all["category_id"] == cat_id].copy()
                            if df_cat.empty:
                                continue

                            display_cols = [
                                "snapshot_date",
                                "rank",
                                "asin",
                                "product_name",
                                "brand",
                                "price",
                                "rating",
                                "reviews_count",
                                "badge",
                            ]
                            available_display = [c for c in display_cols if c in df_cat.columns]
                            df_display = df_cat[available_display].sort_values(
                                ["snapshot_date", "rank"]
                            )

                            sheet_name = categories_info.get(cat_id, cat_id)[:31]
                            df_display.to_excel(writer, sheet_name=sheet_name, index=False)
                            sheets_created.append(sheet_name)
                            total_rows += len(df_display)

                    # 4. LANEIGE ì œí’ˆ ì „ìš© ì‹œíŠ¸
                    if "brand" in df_all.columns:
                        df_laneige = df_all[df_all["brand"].str.upper() == "LANEIGE"].copy()
                        if not df_laneige.empty:
                            laneige_cols = [
                                "snapshot_date",
                                "category_id",
                                "rank",
                                "asin",
                                "product_name",
                                "price",
                                "rating",
                                "reviews_count",
                                "badge",
                            ]
                            available_laneige = [c for c in laneige_cols if c in df_laneige.columns]
                            df_laneige = df_laneige[available_laneige].sort_values(
                                ["snapshot_date", "category_id", "rank"]
                            )
                            df_laneige.to_excel(writer, sheet_name="LANEIGE Products", index=False)
                            sheets_created.append("LANEIGE Products")
                            total_rows += len(df_laneige)

            # ========================================
            # Case 2: ëŒ€ì‹œë³´ë“œ ë°ì´í„° í˜•ì‹ (ì§‘ê³„ ë°ì´í„°ë§Œ)
            # ========================================
            elif is_dashboard_data and not is_crawl_data:
                logging.info("Excel export: using dashboard_data.json (aggregated data only)")

                # 1. Overview ì‹œíŠ¸
                metadata = crawl_data.get("metadata", {})
                data_source = crawl_data.get("data_source", {})
                overview_data = [
                    {"í•­ëª©": "ë°ì´í„° ë‚ ì§œ", "ê°’": metadata.get("data_date", "N/A")},
                    {"í•­ëª©": "ìƒì„± ì‹œê°", "ê°’": metadata.get("generated_at", "N/A")},
                    {"í•­ëª©": "ì´ ì œí’ˆ ìˆ˜", "ê°’": metadata.get("total_products", 0)},
                    {"í•­ëª©": "LANEIGE ì œí’ˆ ìˆ˜", "ê°’": metadata.get("laneige_products", 0)},
                    {"í•­ëª©": "í”Œë«í¼", "ê°’": data_source.get("platform", "Amazon US")},
                ]
                df_overview = pd.DataFrame(overview_data)
                df_overview.to_excel(writer, sheet_name="Overview", index=False)
                sheets_created.append("Overview")
                total_rows += len(df_overview)

                # 2. Brand KPIs ì‹œíŠ¸
                brand_kpis = crawl_data.get("brand", {}).get("kpis", {})
                if brand_kpis:
                    kpi_data = [
                        {"KPI": "SoS (Share of Shelf)", "ê°’": f"{brand_kpis.get('sos', 0)}%"},
                        {"KPI": "SoS ë³€í™”", "ê°’": brand_kpis.get("sos_delta", "N/A")},
                        {"KPI": "Top 10 ì œí’ˆ ìˆ˜", "ê°’": brand_kpis.get("top10_count", 0)},
                        {"KPI": "í‰ê·  ìˆœìœ„", "ê°’": brand_kpis.get("avg_rank", 0)},
                        {"KPI": "HHI (ì‹œì¥ ì§‘ì¤‘ë„)", "ê°’": brand_kpis.get("hhi", 0)},
                    ]
                    df_kpis = pd.DataFrame(kpi_data)
                    df_kpis.to_excel(writer, sheet_name="LANEIGE KPIs", index=False)
                    sheets_created.append("LANEIGE KPIs")
                    total_rows += len(df_kpis)

                # 3. Competitors ì‹œíŠ¸
                competitors = crawl_data.get("brand", {}).get("competitors", [])
                if competitors:
                    df_comp = pd.DataFrame(competitors)
                    column_mapping = {
                        "brand": "Brand",
                        "sos": "SoS (%)",
                        "avg_rank": "Avg Rank",
                        "product_count": "Product Count",
                        "avg_price": "Avg Price ($)",
                    }
                    existing_cols = {
                        k: v for k, v in column_mapping.items() if k in df_comp.columns
                    }
                    df_comp = df_comp.rename(columns=existing_cols)
                    df_comp.to_excel(writer, sheet_name="Competitors", index=False)
                    sheets_created.append("Competitors")
                    total_rows += len(df_comp)

                # 4. Action Items ì‹œíŠ¸
                action_items = crawl_data.get("home", {}).get("action_items", [])
                if action_items:
                    df_actions = pd.DataFrame(action_items)
                    df_actions.to_excel(writer, sheet_name="Action Items", index=False)
                    sheets_created.append("Action Items")
                    total_rows += len(df_actions)

                # 5. Category View ì‹œíŠ¸
                category_data = crawl_data.get("category", {})
                if category_data:
                    for cat_id, cat_info in category_data.items():
                        top_products = cat_info.get("top_products", [])
                        if top_products:
                            df_cat = pd.DataFrame(top_products)
                            sheet_name = categories_info.get(cat_id, cat_id)[:31]
                            df_cat.to_excel(writer, sheet_name=sheet_name, index=False)
                            sheets_created.append(sheet_name)
                            total_rows += len(df_cat)

            # ========================================
            # Case 3: ë¡œì»¬ í¬ë¡¤ë§ ì›ë³¸ ë°ì´í„°
            # ========================================
            else:
                logging.info("Excel export: using latest_crawl_result.json (raw crawl data)")

                # ì „ì²´ RawData ìˆ˜ì§‘ (ì¹´í…Œê³ ë¦¬ë³„ rank_records)
                all_records = []
                for cat_id, cat_data in crawl_data.get("categories", {}).items():
                    records = cat_data.get("rank_records", cat_data.get("products", []))
                    for record in records:
                        # category_id ì¶”ê°€ (ì—†ëŠ” ê²½ìš°)
                        if "category_id" not in record:
                            record["category_id"] = cat_id
                        all_records.append(record)

                if not all_records:
                    logging.warning("Excel export: no rank_records found in crawl data")

                if all_records:
                    df_all = pd.DataFrame(all_records)

                    # ë‚ ì§œ í•„í„° ì ìš© (ì„ íƒ ê¸°ê°„)
                    if start_date and "snapshot_date" in df_all.columns:
                        df_all = df_all[df_all["snapshot_date"] >= start_date]
                    if end_date and "snapshot_date" in df_all.columns:
                        df_all = df_all[df_all["snapshot_date"] <= end_date]

                    if not df_all.empty:
                        # 1. RawData ì‹œíŠ¸ - Google Sheetsì™€ ë™ì¼í•œ ì „ì²´ ë°ì´í„°
                        available_cols = [c for c in RAWDATA_COLUMNS if c in df_all.columns]
                        df_raw = df_all[available_cols].copy()
                        df_raw = df_raw.sort_values(["category_id", "rank"])
                        df_raw.to_excel(writer, sheet_name="RawData", index=False)
                        sheets_created.append("RawData")
                        total_rows += len(df_raw)

                        # 2. ì¹´í…Œê³ ë¦¬ë³„ ì‹œíŠ¸ (ìš”ì•½ ë³´ê¸°ìš©)
                        for cat_id in df_all["category_id"].unique():
                            df_cat = df_all[df_all["category_id"] == cat_id].copy()
                            if df_cat.empty:
                                continue

                            # í•µì‹¬ ì»¬ëŸ¼ë§Œ ì„ íƒí•˜ì—¬ ê°€ë…ì„± í–¥ìƒ
                            display_cols = [
                                "rank",
                                "asin",
                                "product_name",
                                "brand",
                                "price",
                                "rating",
                                "reviews_count",
                                "badge",
                            ]
                            available_display = [c for c in display_cols if c in df_cat.columns]
                            df_display = df_cat[available_display].sort_values("rank")

                            # ì‹œíŠ¸ ì´ë¦„ (31ì ì œí•œ)
                            sheet_name = categories_info.get(cat_id, cat_id)[:31]
                            df_display.to_excel(writer, sheet_name=sheet_name, index=False)
                            sheets_created.append(sheet_name)
                            total_rows += len(df_display)

                        # 3. LANEIGE ì œí’ˆ ì „ìš© ì‹œíŠ¸
                        df_laneige = df_all[df_all["brand"].str.upper() == "LANEIGE"].copy()
                        if not df_laneige.empty:
                            laneige_cols = [
                                "snapshot_date",
                                "category_id",
                                "rank",
                                "asin",
                                "product_name",
                                "price",
                                "rating",
                                "reviews_count",
                                "badge",
                            ]
                            available_laneige = [c for c in laneige_cols if c in df_laneige.columns]
                            df_laneige = df_laneige[available_laneige].sort_values(
                                ["category_id", "rank"]
                            )
                            df_laneige.to_excel(writer, sheet_name="LANEIGE Products", index=False)
                            sheets_created.append("LANEIGE Products")
                            total_rows += len(df_laneige)

                        # 4. Summary ì‹œíŠ¸ - ë¸Œëœë“œë³„ ì§‘ê³„
                        if "brand" in df_all.columns:
                            agg_dict = {"asin": "count"}
                            if "rank" in df_all.columns:
                                agg_dict["rank"] = "mean"
                            if "price" in df_all.columns:
                                agg_dict["price"] = "mean"
                            if "rating" in df_all.columns:
                                agg_dict["rating"] = "mean"

                            summary = df_all.groupby("brand").agg(agg_dict).reset_index()
                            col_names = ["Brand", "Product Count"]
                            if "rank" in agg_dict:
                                col_names.append("Avg Rank")
                            if "price" in agg_dict:
                                col_names.append("Avg Price")
                            if "rating" in agg_dict:
                                col_names.append("Avg Rating")
                            summary.columns = col_names

                            summary = summary.sort_values("Product Count", ascending=False).head(30)
                            for col in ["Avg Rank", "Avg Price", "Avg Rating"]:
                                if col in summary.columns:
                                    summary[col] = summary[col].round(2)

                            summary.to_excel(writer, sheet_name="Summary", index=False)
                            sheets_created.append("Summary")
                            total_rows += len(summary)

            # 4. ì‹œíŠ¸ê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ì•ˆë‚´ ì‹œíŠ¸ ìƒì„±
            if not sheets_created:
                data_source_info = (
                    "SQLite"
                    if data_source == "sqlite"
                    else (
                        "Google Sheets"
                        if data_source == "sheets"
                        else (str(json_path) if json_path else "N/A")
                    )
                )
                no_data_info = [
                    {"í•­ëª©": "ìš”ì²­ ê¸°ê°„", "ê°’": f"{start_date or 'N/A'} ~ {end_date or 'N/A'}"},
                    {"í•­ëª©": "ê²°ê³¼", "ê°’": "í•´ë‹¹ ê¸°ê°„ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"},
                    {"í•­ëª©": "ë°ì´í„° ì†ŒìŠ¤", "ê°’": data_source_info},
                    {"í•­ëª©": "ì•ˆë‚´", "ê°’": "í¬ë¡¤ë§ ì‹¤í–‰ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”"},
                ]
                df_no_data = pd.DataFrame(no_data_info)
                df_no_data.to_excel(writer, sheet_name="No Data", index=False)
                sheets_created.append("No Data")

        logging.info(f"Excel exported: {output_path} ({total_rows} rows, sheets: {sheets_created})")

        # íŒŒì¼ ë°˜í™˜
        return FileResponse(
            path=str(output_path),
            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            headers={"Content-Disposition": f"attachment; filename={output_path.name}"},
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Excel export error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Excel ë‚´ë³´ë‚´ê¸° ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤") from e


# ============= Competitor Comparison API =============


@app.get("/api/competitors")
async def get_competitor_data(brand: str | None = None):
    """
    ê²½ìŸì‚¬ ì¶”ì  ë°ì´í„° ì¡°íšŒ

    Args:
        brand: ë¸Œëœë“œ í•„í„° (ì˜ˆ: "Summer Fridays")

    Returns:
        ê²½ìŸì‚¬ ì œí’ˆ ëª©ë¡ ë° LANEIGE ë¹„êµ ë°ì´í„°
    """
    try:
        import json
        from pathlib import Path

        from src.tools.sqlite_storage import get_sqlite_storage

        result = {"competitors": {}, "laneige_products": {}, "comparison": []}

        # 1. SQLiteì—ì„œ ê²½ìŸì‚¬ ë°ì´í„° ì¡°íšŒ ì‹œë„
        try:
            sqlite = get_sqlite_storage()
            await sqlite.initialize()
            comp_products = await sqlite.get_competitor_products(brand=brand)

            if comp_products:
                # ë¸Œëœë“œë³„ë¡œ ê·¸ë£¹í™”
                for p in comp_products:
                    brand_name = p.get("brand", "Unknown")
                    if brand_name not in result["competitors"]:
                        result["competitors"][brand_name] = {
                            "brand": brand_name,
                            "products": [],
                            "product_count": 0,
                            "avg_price": 0,
                            "avg_rating": 0,
                        }
                    result["competitors"][brand_name]["products"].append(p)

                # ë¸Œëœë“œë³„ ì§‘ê³„
                for _brand_name, brand_data in result["competitors"].items():
                    products = brand_data["products"]
                    brand_data["product_count"] = len(products)
                    prices = [p["price"] for p in products if p.get("price")]
                    ratings = [p["rating"] for p in products if p.get("rating")]
                    brand_data["avg_price"] = round(sum(prices) / len(prices), 2) if prices else 0
                    brand_data["avg_rating"] = (
                        round(sum(ratings) / len(ratings), 1) if ratings else 0
                    )

        except Exception as sqlite_err:
            logging.warning(f"SQLite competitor query failed: {sqlite_err}")

        # 2. JSON íŒŒì¼ì—ì„œ í´ë°±
        if not result["competitors"]:
            json_path = Path("./data/competitor_products.json")
            if json_path.exists():
                with open(json_path, encoding="utf-8") as f:
                    json_data = json.load(f)
                    for p in json_data.get("products", []):
                        brand_name = p.get("brand", "Unknown")
                        if brand and brand_name != brand:
                            continue
                        if brand_name not in result["competitors"]:
                            result["competitors"][brand_name] = {
                                "brand": brand_name,
                                "products": [],
                                "product_count": 0,
                            }
                        result["competitors"][brand_name]["products"].append(p)

        # 3. LANEIGE ì œí’ˆ ë°ì´í„° ë¡œë“œ (ìµœì‹  í¬ë¡¤ë§ ë°ì´í„°ì—ì„œ)
        data = load_dashboard_data()
        if data:
            # ì¹´í…Œê³ ë¦¬ë³„ LANEIGE ì œí’ˆ ì¶”ì¶œ
            for cat_id, cat_data in data.get("category", {}).items():
                for product in cat_data.get("top_products", []):
                    if "laneige" in product.get("brand", "").lower():
                        product_type = _detect_product_type(product.get("product_name", ""))
                        if product_type not in result["laneige_products"]:
                            result["laneige_products"][product_type] = []
                        result["laneige_products"][product_type].append(
                            {**product, "category_id": cat_id, "product_type": product_type}
                        )

        # 4. ì œí’ˆ íƒ€ì…ë³„ ë¹„êµ ë°ì´í„° ìƒì„±
        for brand_name, brand_data in result["competitors"].items():
            for comp_product in brand_data["products"]:
                laneige_match = comp_product.get("laneige_competitor")
                product_type = comp_product.get("product_type", "")

                comparison_item = {
                    "competitor_brand": brand_name,
                    "competitor_product": comp_product.get("product_name", ""),
                    "competitor_price": comp_product.get("price"),
                    "competitor_rating": comp_product.get("rating"),
                    "competitor_reviews": comp_product.get("reviews_count"),
                    "product_type": product_type,
                    "laneige_product": laneige_match,
                    "laneige_price": None,
                    "laneige_rating": None,
                    "laneige_reviews": None,
                    "price_diff": None,
                    "rating_diff": None,
                }

                # LANEIGE ë§¤ì¹­ ì œí’ˆ ì°¾ê¸°
                if product_type in result["laneige_products"]:
                    for lp in result["laneige_products"][product_type]:
                        comparison_item["laneige_price"] = lp.get("price")
                        comparison_item["laneige_rating"] = lp.get("rating")
                        comparison_item["laneige_reviews"] = lp.get("reviews_count")

                        # ì°¨ì´ ê³„ì‚°
                        if comparison_item["competitor_price"] and comparison_item["laneige_price"]:
                            comparison_item["price_diff"] = round(
                                comparison_item["laneige_price"]
                                - comparison_item["competitor_price"],
                                2,
                            )
                        if (
                            comparison_item["competitor_rating"]
                            and comparison_item["laneige_rating"]
                        ):
                            comparison_item["rating_diff"] = round(
                                comparison_item["laneige_rating"]
                                - comparison_item["competitor_rating"],
                                1,
                            )
                        break

                result["comparison"].append(comparison_item)

        return result

    except Exception as e:
        logger.error(f"Competitor data error: {e}", exc_info=True)
        raise HTTPException(
            status_code=500, detail="ê²½ìŸì‚¬ ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤"
        ) from e


def _detect_product_type(product_name: str) -> str:
    """ì œí’ˆëª…ì—ì„œ ì œí’ˆ íƒ€ì… ì¶”ì¶œ"""
    name_lower = product_name.lower()

    if "lip sleeping" in name_lower or "lip mask" in name_lower:
        return "lip_balm"
    elif "lip glowy" in name_lower or "lip butter" in name_lower or "lip balm" in name_lower:
        return "lip_balm"
    elif "water sleeping" in name_lower or "sleeping mask" in name_lower:
        return "sleeping_mask"
    elif "water bank" in name_lower or "cream" in name_lower or "moisturizer" in name_lower:
        return "moisturizer"
    elif "toner" in name_lower or "cream skin" in name_lower:
        return "toner"
    elif "serum" in name_lower:
        return "serum"
    else:
        return "other"


@app.get("/api/competitors/brands")
async def get_tracked_brands():
    """ì¶”ì  ì¤‘ì¸ ê²½ìŸì‚¬ ë¸Œëœë“œ ëª©ë¡"""
    try:
        import json
        from pathlib import Path

        config_path = Path("./config/tracked_competitors.json")
        if not config_path.exists():
            return {"brands": []}

        with open(config_path, encoding="utf-8") as f:
            config = json.load(f)

        brands = []
        for brand_name, brand_config in config.get("competitors", {}).items():
            brands.append(
                {
                    "name": brand_name,
                    "tier": brand_config.get("tier", ""),
                    "product_count": len(brand_config.get("products", [])),
                }
            )

        return {"brands": brands}

    except Exception as e:
        logging.error(f"Get tracked brands error: {e}")
        return {"brands": []}


# ============= Alert Settings API =============

from src.core.state_manager import StateManager, get_state_manager

# ì‹±ê¸€í†¤ State Manager
_state_manager: StateManager | None = None


def get_app_state_manager() -> StateManager:
    """ì•± ë ˆë²¨ State Manager ë°˜í™˜"""
    global _state_manager
    if _state_manager is None:
        _state_manager = get_state_manager()
    return _state_manager


class AlertSettingsRequest(BaseModel):
    """ì•Œë¦¼ ì„¤ì • ìš”ì²­"""

    email: str
    consent: bool
    alert_types: list[str] = []


class AlertSettingsResponse(BaseModel):
    """ì•Œë¦¼ ì„¤ì • ì‘ë‹µ"""

    email: str
    consent: bool
    alert_types: list[str]
    consent_date: str | None = None


@app.get("/api/v3/alert-settings")
async def get_alert_settings():
    """
    í˜„ì¬ ì•Œë¦¼ ì„¤ì • ì¡°íšŒ

    ì°¸ê³ : í˜„ì¬ëŠ” ë‹¨ì¼ ì‚¬ìš©ì ì„¤ì •ë§Œ ì§€ì› (ì²« ë²ˆì§¸ ë“±ë¡ëœ ì´ë©”ì¼)
    """
    state_manager = get_app_state_manager()
    subscriptions = state_manager.get_all_subscriptions()

    if not subscriptions:
        return {"email": "", "consent": False, "alert_types": [], "consent_date": None}

    # ì²« ë²ˆì§¸ êµ¬ë… ë°˜í™˜
    email, sub = next(iter(subscriptions.items()))
    return {
        "email": email,
        "consent": sub.consent,
        "alert_types": sub.alert_types,
        "consent_date": sub.consent_date.isoformat() if sub.consent_date else None,
    }


@app.post("/api/v3/alert-settings", dependencies=[Depends(verify_api_key)])
@limiter.limit("5/minute")  # ë¶„ë‹¹ 5íšŒ ì œí•œ (ìŠ¤íŒ¸ ë°©ì§€)
async def save_alert_settings(request: Request, settings: AlertSettingsRequest):
    """
    ì•Œë¦¼ ì„¤ì • ì €ì¥

    ë³´ì•ˆ: API Key + Rate Limiting (IPë‹¹ ë¶„ë‹¹ 5íšŒ)
    ì¤‘ìš”: consentê°€ Trueì¼ ë•Œë§Œ ì´ë©”ì¼ ë“±ë¡
    """
    state_manager = get_app_state_manager()

    if not settings.email:
        raise HTTPException(status_code=400, detail="ì´ë©”ì¼ ì£¼ì†Œê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    if settings.consent:
        # ì´ë©”ì¼ ë“±ë¡ (ëª…ì‹œì  ë™ì˜)
        success = state_manager.register_email(
            email=settings.email, consent=True, alert_types=settings.alert_types
        )

        if not success:
            raise HTTPException(status_code=400, detail="ì´ë©”ì¼ ë“±ë¡ ì‹¤íŒ¨")

        return {"status": "ok", "message": "ì•Œë¦¼ ì„¤ì •ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤."}
    else:
        # ë™ì˜ ì—†ìœ¼ë©´ ì—…ë°ì´íŠ¸ë§Œ (ì•Œë¦¼ ìœ í˜• ë³€ê²½)
        success = state_manager.update_email_subscription(
            email=settings.email, alert_types=settings.alert_types
        )

        return {"status": "ok", "message": "ì„¤ì •ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤."}


@app.post("/api/v3/alert-settings/revoke", dependencies=[Depends(verify_api_key)])
@limiter.limit("5/minute")  # ë¶„ë‹¹ 5íšŒ ì œí•œ
async def revoke_alert_consent(request: Request):
    """
    ì•Œë¦¼ ë™ì˜ ì² íšŒ

    ë³´ì•ˆ: API Key + Rate Limiting
    ì²« ë²ˆì§¸ ë“±ë¡ëœ ì´ë©”ì¼ì˜ ë™ì˜ë¥¼ ì² íšŒí•©ë‹ˆë‹¤.
    """
    state_manager = get_app_state_manager()
    subscriptions = state_manager.get_all_subscriptions()

    if not subscriptions:
        return {"status": "ok", "message": "ì² íšŒí•  ë™ì˜ê°€ ì—†ìŠµë‹ˆë‹¤."}

    # ì²« ë²ˆì§¸ ì´ë©”ì¼ ì² íšŒ
    email = next(iter(subscriptions.keys()))
    state_manager.revoke_email_consent(email)

    return {"status": "ok", "message": "ë™ì˜ê°€ ì² íšŒë˜ì—ˆìŠµë‹ˆë‹¤."}


@app.get("/api/v3/alerts")
async def get_alerts(limit: int = 50, alert_type: str | None = None):
    """
    ì•Œë¦¼ ëª©ë¡ ì¡°íšŒ

    Args:
        limit: ìµœëŒ€ ê°œìˆ˜
        alert_type: í•„í„°í•  ì•Œë¦¼ ìœ í˜•
    """
    from src.agents.alert_agent import AlertAgent

    state_manager = get_app_state_manager()
    alert_agent = AlertAgent(state_manager)

    return {
        "alerts": alert_agent.get_alerts(limit=limit, alert_type=alert_type),
        "pending_count": alert_agent.get_pending_count(),
        "stats": alert_agent.get_stats(),
    }


# ============= ëŒ€ì‹œë³´ë“œ HTML ì„œë¹™ =============


@app.get("/dashboard")
async def serve_dashboard():
    """
    ëŒ€ì‹œë³´ë“œ HTML í˜ì´ì§€ ì„œë¹™ (API í‚¤ ìë™ ì£¼ì…)

    ì„œë²„ì˜ API_KEYë¥¼ HTMLì— ìë™ìœ¼ë¡œ ì£¼ì…í•˜ì—¬
    í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ë³„ë„ ì„¤ì • ì—†ì´ ì¸ì¦ëœ API í˜¸ì¶œ ê°€ëŠ¥
    """
    dashboard_path = Path("./dashboard/amore_unified_dashboard_v4.html")
    if not dashboard_path.exists():
        raise HTTPException(status_code=404, detail="Dashboard not found")

    # API_KEYê°€ ì„¤ì •ëœ ê²½ìš° HTMLì— ìë™ ì£¼ì…
    if API_KEY:
        html_content = dashboard_path.read_text(encoding="utf-8")
        # </head> ì§ì „ì— API í‚¤ ì„¤ì • ìŠ¤í¬ë¦½íŠ¸ ì‚½ì…
        api_key_script = f'<script>window.DASHBOARD_API_KEY = "{API_KEY}";</script>\n</head>'
        html_content = html_content.replace("</head>", api_key_script)
        return HTMLResponse(content=html_content, media_type="text/html")

    return FileResponse(dashboard_path, media_type="text/html")


@app.get("/api/health")
async def health_check():
    """ê¸°ë³¸ í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸ (Railway healthcheckìš©)"""
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}


@app.get("/api/health/deep")
async def deep_health_check():
    """
    Deep Health Check - ëª¨ë“  ì„œë¸Œì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸

    Returns:
        - database: SQLite ì—°ê²° ìƒíƒœ
        - knowledge_graph: KG ë¡œë“œ ìƒíƒœ ë° íŠ¸ë¦¬í”Œ ìˆ˜
        - llm: OpenAI API ì—°ê²° ìƒíƒœ
        - scheduler: ììœ¨ ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ
        - memory: ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        - disk: ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ (Railway Volume)
    """
    import os
    import sqlite3
    from pathlib import Path

    health = {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "checks": {},
        "warnings": [],
    }

    # 1. SQLite ì—°ê²° í™•ì¸
    try:
        db_path = (
            Path("/data/amore_data.db") if Path("/data").exists() else Path("data/amore_data.db")
        )
        if db_path.exists():
            conn = sqlite3.connect(str(db_path), timeout=5)
            cursor = conn.execute("SELECT COUNT(*) FROM raw_data")
            count = cursor.fetchone()[0]
            conn.close()
            health["checks"]["database"] = {
                "status": "healthy",
                "records": count,
                "path": str(db_path),
            }
        else:
            health["checks"]["database"] = {"status": "missing", "path": str(db_path)}
            health["warnings"].append("SQLite database not found")
    except Exception as e:
        health["checks"]["database"] = {"status": "unhealthy", "error": str(e)}
        health["status"] = "degraded"

    # 2. Knowledge Graph ìƒíƒœ
    try:
        from src.ontology.knowledge_graph import get_knowledge_graph

        kg = get_knowledge_graph()
        triple_count = len(kg.triples) if kg.triples else 0
        health["checks"]["knowledge_graph"] = {
            "status": "healthy" if triple_count > 0 else "empty",
            "triples": triple_count,
            "max_triples": kg.max_triples,
        }
        if triple_count == 0:
            health["warnings"].append("Knowledge Graph is empty")
    except Exception as e:
        health["checks"]["knowledge_graph"] = {"status": "unhealthy", "error": str(e)}
        health["status"] = "degraded"

    # 3. LLM API ì—°ê²° (OpenAI)
    try:
        api_key = os.getenv("OPENAI_API_KEY", "")
        if api_key and api_key.startswith("sk-"):
            health["checks"]["llm"] = {
                "status": "configured",
                "provider": "openai",
                "key_prefix": api_key[:10] + "...",
            }
        else:
            health["checks"]["llm"] = {"status": "not_configured"}
            health["warnings"].append("OPENAI_API_KEY not properly configured")
    except Exception as e:
        health["checks"]["llm"] = {"status": "error", "error": str(e)}

    # 4. ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ
    try:
        brain = await get_initialized_brain()
        scheduler_running = brain.scheduler.is_running if brain.scheduler else False
        health["checks"]["scheduler"] = {
            "status": "running" if scheduler_running else "stopped",
            "mode": brain.mode.value if brain.mode else "unknown",
        }
    except Exception as e:
        health["checks"]["scheduler"] = {"status": "error", "error": str(e)}

    # 5. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
    try:
        import psutil

        memory = psutil.virtual_memory()
        health["checks"]["memory"] = {
            "status": "healthy" if memory.percent < 90 else "warning",
            "used_percent": round(memory.percent, 1),
            "available_gb": round(memory.available / (1024**3), 2),
        }
        if memory.percent > 90:
            health["warnings"].append(f"High memory usage: {memory.percent}%")
            health["status"] = "degraded"
    except ImportError:
        health["checks"]["memory"] = {"status": "unknown", "note": "psutil not installed"}

    # 6. ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ (Railway Volume)
    try:
        import shutil

        data_path = Path("/data") if Path("/data").exists() else Path("data")
        if data_path.exists():
            total, used, free = shutil.disk_usage(data_path)
            used_percent = (used / total) * 100
            health["checks"]["disk"] = {
                "status": "healthy" if used_percent < 90 else "warning",
                "used_percent": round(used_percent, 1),
                "free_gb": round(free / (1024**3), 2),
                "path": str(data_path),
            }
            if used_percent > 90:
                health["warnings"].append(f"Low disk space: {100-used_percent:.1f}% free")
                health["status"] = "degraded"
    except Exception as e:
        health["checks"]["disk"] = {"status": "error", "error": str(e)}

    # ìµœì¢… ìƒíƒœ ê²°ì •
    unhealthy_checks = [k for k, v in health["checks"].items() if v.get("status") == "unhealthy"]
    if unhealthy_checks:
        health["status"] = "unhealthy"

    return health


# ============= Level 4 Brain API (v4) =============


class BrainChatRequest(BaseModel):
    """Brain ì±—ë´‡ ìš”ì²­"""

    message: str
    session_id: str | None = "default"
    skip_cache: bool = False


class BrainChatResponse(BaseModel):
    """Brain ì±—ë´‡ ì‘ë‹µ"""

    text: str
    confidence: float
    sources: list[str]
    reasoning: str | None = None
    tools_used: list[str]
    processing_time_ms: float
    from_cache: bool
    brain_mode: str


@app.post("/api/v4/chat", response_model=BrainChatResponse, dependencies=[Depends(verify_api_key)])
@limiter.limit("10/minute")  # ë¶„ë‹¹ 10íšŒ ì œí•œ (ë³´ì•ˆ ê°•í™”)
async def chat_v4(request: Request, body: BrainChatRequest):
    """
    Level 4 Brain ê¸°ë°˜ ì±—ë´‡ API (v4)

    LLM-First ì ‘ê·¼:
    - ëª¨ë“  íŒë‹¨ì„ LLMì´ ìˆ˜í–‰
    - ê·œì¹™ ê¸°ë°˜ ë¹ ë¥¸ ê²½ë¡œ ì—†ìŒ
    - RAG + KG í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
    - ììœ¨ ìŠ¤ì¼€ì¤„ëŸ¬ì™€ í†µí•©
    """
    import time

    start_time = time.time()

    message = body.message.strip()
    session_id = body.session_id or "default"

    if not message:
        raise HTTPException(status_code=400, detail="Message is required")

    try:
        # Brain ì¸ìŠ¤í„´ìŠ¤ íšë“
        brain = await get_initialized_brain()

        # í˜„ì¬ ë©”íŠ¸ë¦­ ë°ì´í„° ë¡œë“œ
        data = load_dashboard_data()
        current_metrics = data if data else None

        # Brainìœ¼ë¡œ ì²˜ë¦¬ (LLM-First)
        response = await brain.process_query(
            query=message,
            session_id=session_id,
            current_metrics=current_metrics,
            skip_cache=body.skip_cache,
        )

        processing_time = (time.time() - start_time) * 1000

        return BrainChatResponse(
            text=response.content,
            confidence=response.confidence,
            sources=response.sources,
            reasoning=response.reasoning,
            tools_used=response.tools_called if hasattr(response, "tools_called") else [],
            processing_time_ms=processing_time,
            from_cache=response.from_cache if hasattr(response, "from_cache") else False,
            brain_mode=brain.mode.value,
        )

    except Exception as e:
        logging.error(f"Brain error: {e}")
        return BrainChatResponse(
            text=f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            confidence=0.0,
            sources=[],
            reasoning=None,
            tools_used=[],
            processing_time_ms=(time.time() - start_time) * 1000,
            from_cache=False,
            brain_mode="error",
        )


@app.get("/api/v4/brain/status")
async def get_brain_status():
    """
    Brain ìƒíƒœ ì¡°íšŒ

    Returns:
        - mode: í˜„ì¬ Brain ëª¨ë“œ
        - scheduler: ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ
        - pending_tasks: ëŒ€ê¸° ì¤‘ íƒœìŠ¤í¬
        - stats: í†µê³„
    """
    try:
        brain = await get_initialized_brain()

        return {
            "mode": brain.mode.value,
            "scheduler_running": brain.scheduler.running if brain.scheduler else False,
            "pending_tasks": brain.scheduler.get_pending_count() if brain.scheduler else 0,
            "stats": brain.get_stats(),
            "initialized": True,
        }
    except Exception as e:
        return {
            "mode": "uninitialized",
            "scheduler_running": False,
            "pending_tasks": 0,
            "stats": {},
            "initialized": False,
            "error": str(e),
        }


@app.post("/api/v4/brain/scheduler/start", dependencies=[Depends(verify_api_key)])
async def start_brain_scheduler():
    """
    ììœ¨ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ (API Key í•„ìš”)

    - ì¼ì¼ í¬ë¡¤ë§ (09:00)
    - ì£¼ê¸°ì  ì•Œë¦¼ ì²´í¬ (30ë¶„)
    - ë°±ê·¸ë¼ìš´ë“œ ë¶„ì„
    """
    try:
        brain = await get_initialized_brain()

        if brain.scheduler and brain.scheduler.running:
            return {
                "started": False,
                "message": "ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.",
                "status": "running",
            }

        await brain.start_scheduler()

        return {"started": True, "message": "ììœ¨ ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.", "status": "running"}
    except Exception as e:
        return {"started": False, "message": f"ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ ì‹¤íŒ¨: {str(e)}", "status": "error"}


@app.post("/api/v4/brain/scheduler/stop", dependencies=[Depends(verify_api_key)])
async def stop_brain_scheduler():
    """ììœ¨ ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€ (API Key í•„ìš”)"""
    try:
        brain = await get_initialized_brain()

        if brain.scheduler:
            brain.scheduler.stop()

        return {"stopped": True, "message": "ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.", "status": "stopped"}
    except Exception as e:
        return {"stopped": False, "message": f"ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€ ì‹¤íŒ¨: {str(e)}", "status": "error"}


@app.post("/api/v4/brain/autonomous-cycle", dependencies=[Depends(verify_api_key)])
async def run_autonomous_cycle():
    """
    ììœ¨ ì‚¬ì´í´ ìˆ˜ë™ ì‹¤í–‰ (API Key í•„ìš”)

    1. ë°ì´í„° ì‹ ì„ ë„ í™•ì¸
    2. í•„ìš”ì‹œ í¬ë¡¤ë§
    3. ì§€í‘œ ê³„ì‚°
    4. ì•Œë¦¼ ì¡°ê±´ ì²´í¬
    5. ì¸ì‚¬ì´íŠ¸ ìƒì„±
    """
    try:
        brain = await get_initialized_brain()
        result = await brain.run_autonomous_cycle()

        return {"success": True, "result": result}
    except Exception as e:
        return {"success": False, "error": str(e)}


@app.post("/api/v4/brain/check-alerts")
async def check_brain_alerts():
    """
    ì•Œë¦¼ ì¡°ê±´ ìˆ˜ë™ ì²´í¬

    í˜„ì¬ ë©”íŠ¸ë¦­ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì•Œë¦¼ ì¡°ê±´ì„ ì²´í¬í•©ë‹ˆë‹¤.
    """
    try:
        brain = await get_initialized_brain()
        data = load_dashboard_data()

        if not data:
            return {"alerts": [], "message": "ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}

        alerts = await brain.check_alerts(data)

        return {"alerts": alerts, "count": len(alerts), "checked_at": datetime.now().isoformat()}
    except Exception as e:
        return {"alerts": [], "error": str(e)}


@app.get("/api/v4/brain/stats")
async def get_brain_stats():
    """Brain í†µê³„ ì¡°íšŒ"""
    try:
        brain = await get_initialized_brain()
        return brain.get_stats()
    except Exception as e:
        return {"error": str(e)}


@app.post("/api/v4/brain/mode", dependencies=[Depends(verify_api_key)])
async def set_brain_mode(mode: str):
    """
    Brain ëª¨ë“œ ë³€ê²½ (API Key í•„ìš”)

    Args:
        mode: reactive, proactive, autonomous
    """
    try:
        brain = await get_initialized_brain()

        mode_map = {
            "reactive": BrainMode.REACTIVE,
            "proactive": BrainMode.PROACTIVE,
            "autonomous": BrainMode.AUTONOMOUS,
        }

        if mode not in mode_map:
            raise HTTPException(
                status_code=400, detail=f"Invalid mode. Valid modes: {list(mode_map.keys())}"
            )

        brain.mode = mode_map[mode]

        return {"mode": brain.mode.value, "message": f"Brain ëª¨ë“œê°€ {mode}(ìœ¼)ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤."}
    except HTTPException:
        raise
    except Exception as e:
        return {"error": str(e)}


# ============= Amazon Deals API =============

from src.tools.deals_scraper import get_deals_scraper


class DealsRequest(BaseModel):
    """Deals í¬ë¡¤ë§ ìš”ì²­"""

    max_items: int = 50
    beauty_only: bool = True


class DealsResponse(BaseModel):
    """Deals ì‘ë‹µ"""

    success: bool
    count: int
    lightning_count: int
    competitor_count: int
    snapshot_datetime: str
    deals: list[dict[str, Any]]
    competitor_deals: list[dict[str, Any]]
    error: str | None = None


@app.get("/api/deals")
async def get_deals_data(brand: str | None = None, hours: int = 24, limit: int = 100):
    """
    ì €ì¥ëœ Deals ë°ì´í„° ì¡°íšŒ

    Args:
        brand: ë¸Œëœë“œ í•„í„° (ì„ íƒ)
        hours: ìµœê·¼ Nì‹œê°„ ë°ì´í„° (ê¸°ë³¸: 24ì‹œê°„)
        limit: ìµœëŒ€ ê°œìˆ˜

    Returns:
        - deals: ë”œ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        - summary: ìš”ì•½ í†µê³„
    """
    try:
        storage = get_sqlite_storage()
        await storage.initialize()

        # ê²½ìŸì‚¬ ë”œ ì¡°íšŒ
        deals = await storage.get_competitor_deals(brand=brand, hours=hours)

        # ìµœëŒ€ ê°œìˆ˜ ì œí•œ
        deals = deals[:limit] if len(deals) > limit else deals

        # ìš”ì•½ í†µê³„
        summary = await storage.get_deals_summary(days=7)

        return {
            "success": True,
            "deals": deals,
            "count": len(deals),
            "summary": summary,
            "filters": {"brand": brand, "hours": hours},
        }

    except Exception as e:
        logging.error(f"Deals data error: {e}")
        return {"success": False, "deals": [], "count": 0, "error": str(e)}


@app.get("/api/deals/summary")
async def get_deals_summary(days: int = 7):
    """
    Deals ìš”ì•½ í†µê³„

    Args:
        days: ë¶„ì„ ê¸°ê°„ (ì¼)

    Returns:
        - by_brand: ë¸Œëœë“œë³„ ë”œ í˜„í™©
        - by_date: ì¼ë³„ ì¶”ì´
    """
    try:
        storage = get_sqlite_storage()
        await storage.initialize()

        summary = await storage.get_deals_summary(days=days)

        return {"success": True, **summary}

    except Exception as e:
        logging.error(f"Deals summary error: {e}")
        return {"success": False, "error": str(e)}


@app.post("/api/deals/scrape", dependencies=[Depends(verify_api_key)])
async def scrape_deals(request: DealsRequest):
    """
    Amazon Deals í˜ì´ì§€ í¬ë¡¤ë§ (API Key í•„ìš”)

    ê²½ìŸì‚¬ í• ì¸ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤.

    Args:
        max_items: ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜
        beauty_only: ë·°í‹° ì¹´í…Œê³ ë¦¬ë§Œ í•„í„°ë§

    Returns:
        - deals: ìˆ˜ì§‘ëœ ë”œ ë°ì´í„°
        - competitor_deals: ê²½ìŸì‚¬ ë”œ
        - lightning_count: Lightning Deal ìˆ˜
    """
    try:
        scraper = await get_deals_scraper()

        # í¬ë¡¤ë§ ì‹¤í–‰
        result = await scraper.scrape_deals(
            max_items=request.max_items, beauty_only=request.beauty_only
        )

        if result["success"]:
            # SQLiteì— ì €ì¥
            storage = get_sqlite_storage()
            await storage.initialize()

            # ëª¨ë“  ë”œ ì €ì¥
            if result["deals"]:
                await storage.save_deals(result["deals"], is_competitor=False)

            # ê²½ìŸì‚¬ ë”œì€ is_competitor=Trueë¡œ ë³„ë„ ì €ì¥
            if result["competitor_deals"]:
                await storage.save_deals(result["competitor_deals"], is_competitor=True)

                # ì•Œë¦¼ ì„œë¹„ìŠ¤ë¡œ ì•Œë¦¼ ì²˜ë¦¬
                try:
                    alert_service = get_alert_service()
                    alerts = await alert_service.process_deals_for_alerts(
                        result["competitor_deals"]
                    )

                    # DBì— ì•Œë¦¼ ì €ì¥
                    for alert in alerts:
                        await storage.save_deal_alert(alert)

                    logging.info(
                        f"Processed {len(alerts)} alerts from {len(result['competitor_deals'])} competitor deals"
                    )
                except Exception as alert_err:
                    logging.error(f"Alert processing error: {alert_err}")
                    # ì•Œë¦¼ ì‹¤íŒ¨í•´ë„ í¬ë¡¤ë§ ê²°ê³¼ëŠ” ë°˜í™˜

            logging.info(
                f"Deals scraped: {result['count']} total, {len(result['competitor_deals'])} competitors"
            )

        return DealsResponse(
            success=result["success"],
            count=result["count"],
            lightning_count=result["lightning_count"],
            competitor_count=len(result["competitor_deals"]),
            snapshot_datetime=result["snapshot_datetime"],
            deals=result["deals"],
            competitor_deals=result["competitor_deals"],
            error=result.get("error"),
        )

    except Exception as e:
        logging.error(f"Deals scrape error: {e}")
        return DealsResponse(
            success=False,
            count=0,
            lightning_count=0,
            competitor_count=0,
            snapshot_datetime=datetime.now().isoformat(),
            deals=[],
            competitor_deals=[],
            error=str(e),
        )


@app.get("/api/deals/alerts")
async def get_deals_alerts(limit: int = 50, unsent_only: bool = False):
    """
    Deals ì•Œë¦¼ ëª©ë¡ ì¡°íšŒ

    Args:
        limit: ìµœëŒ€ ê°œìˆ˜
        unsent_only: ë¯¸ë°œì†¡ ì•Œë¦¼ë§Œ ì¡°íšŒ

    Returns:
        - alerts: ì•Œë¦¼ ëª©ë¡
        - count: ì´ ê°œìˆ˜
    """
    try:
        storage = get_sqlite_storage()
        await storage.initialize()

        if unsent_only:
            alerts = await storage.get_unsent_alerts(limit=limit)
        else:
            # ëª¨ë“  ì•Œë¦¼ ì¡°íšŒ (ìµœê·¼ 7ì¼)
            with storage.get_connection() as conn:
                cursor = conn.execute(
                    """
                    SELECT * FROM deals_alerts
                    ORDER BY alert_datetime DESC
                    LIMIT ?
                """,
                    (limit,),
                )
                alerts = [dict(row) for row in cursor.fetchall()]

        return {"success": True, "alerts": alerts, "count": len(alerts)}

    except Exception as e:
        logging.error(f"Deals alerts error: {e}")
        return {"success": False, "alerts": [], "count": 0, "error": str(e)}


@app.post("/api/deals/export")
async def export_deals_report(days: int = 7, format: str = "excel"):
    """
    Deals ë¦¬í¬íŠ¸ ë‚´ë³´ë‚´ê¸°

    Args:
        days: ë¶„ì„ ê¸°ê°„ (ì¼)
        format: ì¶œë ¥ í˜•ì‹ (excel, json)

    Returns:
        - ì—‘ì…€: íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        - JSON: ë°ì´í„° ë°˜í™˜
    """
    try:
        storage = get_sqlite_storage()
        await storage.initialize()

        if format == "json":
            # JSON í˜•ì‹ ë°˜í™˜
            summary = await storage.get_deals_summary(days=days)

            # ì „ì²´ ë”œ ë°ì´í„°
            with storage.get_connection() as conn:
                cutoff_date = (datetime.now() - timedelta(days=days)).strftime("%Y-%m-%d")
                cursor = conn.execute(
                    """
                    SELECT * FROM deals
                    WHERE DATE(snapshot_datetime) >= ?
                    ORDER BY snapshot_datetime DESC
                """,
                    (cutoff_date,),
                )
                all_deals = [dict(row) for row in cursor.fetchall()]

            return {
                "success": True,
                "summary": summary,
                "deals": all_deals,
                "export_date": datetime.now().isoformat(),
                "period_days": days,
            }

        else:  # Excel
            # ì—‘ì…€ íŒŒì¼ ìƒì„±
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = f"./data/exports/Deals_Report_{timestamp}.xlsx"

            result = storage.export_deals_report(output_path=output_path, days=days)

            if not result.get("success"):
                raise HTTPException(status_code=500, detail=result.get("error", "Export failed"))

            file_path = Path(result["file_path"])
            if not file_path.exists():
                raise HTTPException(status_code=500, detail="Generated file not found")

            return FileResponse(
                path=str(file_path),
                media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                headers={"Content-Disposition": f"attachment; filename={file_path.name}"},
            )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Deals export error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Deals ë‚´ë³´ë‚´ê¸° ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤") from e


# ============= ì•Œë¦¼ ì„œë¹„ìŠ¤ API =============

from src.tools.alert_service import get_alert_service


class AlertSendRequest(BaseModel):
    """ì•Œë¦¼ ë°œì†¡ ìš”ì²­"""

    alert_ids: list[int] | None = None  # ë°œì†¡í•  ì•Œë¦¼ ID (ì—†ìœ¼ë©´ ë¯¸ë°œì†¡ ì „ì²´)


@app.get("/api/alerts/status")
async def get_alert_service_status():
    """ì•Œë¦¼ ì„œë¹„ìŠ¤ ìƒíƒœ ì¡°íšŒ"""
    try:
        service = get_alert_service()
        return {"success": True, **service.get_status()}
    except Exception as e:
        logging.error(f"Alert service status error: {e}")
        return {"success": False, "error": str(e)}


@app.post("/api/alerts/send")
async def send_pending_alerts(request: AlertSendRequest | None = None):
    """
    ë¯¸ë°œì†¡ ì•Œë¦¼ ë°œì†¡

    íŠ¹ì • alert_idsë¥¼ ì§€ì •í•˜ë©´ í•´ë‹¹ ì•Œë¦¼ë§Œ, ì—†ìœ¼ë©´ ë¯¸ë°œì†¡ ì „ì²´ ë°œì†¡
    """
    try:
        storage = get_sqlite_storage()
        await storage.initialize()

        alert_service = get_alert_service()

        # ë¯¸ë°œì†¡ ì•Œë¦¼ ì¡°íšŒ
        unsent_alerts = await storage.get_unsent_alerts(limit=50)

        if not unsent_alerts:
            return {"success": True, "message": "No pending alerts to send", "sent_count": 0}

        # íŠ¹ì • ID í•„í„°ë§
        if request and request.alert_ids:
            unsent_alerts = [a for a in unsent_alerts if a.get("id") in request.alert_ids]

        if not unsent_alerts:
            return {"success": True, "message": "No matching alerts found", "sent_count": 0}

        # ì•Œë¦¼ ë°œì†¡
        sent_count = 0
        for alert in unsent_alerts:
            result = await alert_service.send_single_alert(alert)

            # ì„±ê³µ ì‹œ ë°œì†¡ ì™„ë£Œ í‘œì‹œ
            if result.get("slack") or result.get("email"):
                await storage.mark_alert_sent(alert["id"])
                sent_count += 1

        return {
            "success": True,
            "sent_count": sent_count,
            "total_pending": len(unsent_alerts),
            "channels": {
                "slack": alert_service._slack_enabled,
                "email": alert_service._email_enabled,
            },
        }

    except Exception as e:
        logging.error(f"Alert send error: {e}")
        return {"success": False, "error": str(e), "sent_count": 0}


@app.post("/api/alerts/test")
async def send_test_alert():
    """í…ŒìŠ¤íŠ¸ ì•Œë¦¼ ë°œì†¡"""
    try:
        alert_service = get_alert_service()

        test_alert = {
            "alert_datetime": datetime.now().isoformat(),
            "brand": "TEST BRAND",
            "asin": "B000TEST01",
            "product_name": "Test Product - Alert System Verification",
            "deal_type": "lightning",
            "discount_percent": 50.0,
            "deal_price": 19.99,
            "original_price": 39.99,
            "time_remaining": "2h 30m",
            "claimed_percent": 45,
            "product_url": "https://amazon.com/dp/B000TEST01",
            "alert_type": "lightning_deal",
            "alert_message": "Test Alert - ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì•Œë¦¼ì…ë‹ˆë‹¤",
        }

        result = await alert_service.send_single_alert(test_alert)

        return {
            "success": True,
            "test_alert": test_alert,
            "send_result": result,
            "message": "Test alert sent successfully"
            if any(result.values())
            else "No channels enabled",
        }

    except Exception as e:
        logging.error(f"Test alert error: {e}")
        return {"success": False, "error": str(e)}


# ============= Email Verification API =============

import jwt

# JWT ì„¤ì •
JWT_SECRET_KEY = os.getenv("JWT_SECRET_KEY")
JWT_ALGORITHM = "HS256"
EMAIL_VERIFICATION_EXPIRES_MINUTES = 30  # 30ë¶„ ë§Œë£Œ


def create_email_verification_token(
    email: str, expires_minutes: int = EMAIL_VERIFICATION_EXPIRES_MINUTES
) -> str:
    """
    ì´ë©”ì¼ ì¸ì¦ìš© JWT í† í° ìƒì„±

    Args:
        email: ì¸ì¦í•  ì´ë©”ì¼ ì£¼ì†Œ
        expires_minutes: í† í° ë§Œë£Œ ì‹œê°„ (ë¶„)

    Returns:
        JWT í† í° ë¬¸ìì—´
    """
    if not JWT_SECRET_KEY:
        raise ValueError("JWT_SECRET_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    payload = {
        "email": email,
        "purpose": "email_verification",
        "exp": datetime.now(UTC) + timedelta(minutes=expires_minutes),
        "iat": datetime.now(UTC),
    }
    return jwt.encode(payload, JWT_SECRET_KEY, algorithm=JWT_ALGORITHM)


def verify_jwt_email_token(token: str) -> dict:
    """
    JWT ì´ë©”ì¼ ì¸ì¦ í† í° ê²€ì¦

    Args:
        token: JWT í† í°

    Returns:
        {"valid": True, "email": "..."} ë˜ëŠ” {"valid": False, "error": "..."}
    """
    if not JWT_SECRET_KEY:
        return {"valid": False, "error": "JWT_SECRET_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}

    try:
        payload = jwt.decode(token, JWT_SECRET_KEY, algorithms=[JWT_ALGORITHM])

        # purpose ê²€ì¦
        if payload.get("purpose") != "email_verification":
            return {"valid": False, "error": "ìœ íš¨í•˜ì§€ ì•Šì€ í† í°ì…ë‹ˆë‹¤."}

        return {"valid": True, "email": payload["email"]}

    except jwt.ExpiredSignatureError:
        return {"valid": False, "error": "ì¸ì¦ í† í°ì´ ë§Œë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì¸ì¦í•´ì£¼ì„¸ìš”."}
    except jwt.InvalidTokenError:
        return {"valid": False, "error": "ìœ íš¨í•˜ì§€ ì•Šì€ ì¸ì¦ í† í°ì…ë‹ˆë‹¤."}


@app.post("/api/alerts/send-verification")
@limiter.limit("3/minute")  # ë¶„ë‹¹ 3íšŒ ì œí•œ (ìŠ¤íŒ¸ ë°©ì§€)
async def send_verification_email(request: Request):
    """
    ì´ë©”ì¼ ì¸ì¦ ìš”ì²­ - ì¸ì¦ ì´ë©”ì¼ ë°œì†¡ (JWT ë°©ì‹)

    ë³´ì•ˆ: Rate Limitìœ¼ë¡œ ìŠ¤íŒ¸ ë°©ì§€ (ë¶„ë‹¹ 3íšŒ)
    ì‚¬ìš©ìê°€ ì´ë©”ì¼ì„ ì…ë ¥í•˜ê³  'ì¸ì¦í•˜ê¸°' ë²„íŠ¼ì„ ëˆ„ë¥´ë©´
    í•´ë‹¹ ì´ë©”ì¼ë¡œ JWT í† í°ì´ í¬í•¨ëœ ì¸ì¦ ë§í¬ë¥¼ ë°œì†¡í•©ë‹ˆë‹¤.

    JWT í† í°ì€ 30ë¶„ê°„ ìœ íš¨í•˜ë©°, ì„œë²„ ì¬ì‹œì‘ê³¼ ë¬´ê´€í•˜ê²Œ ê²€ì¦ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    """
    try:
        body = await request.json()
        email = body.get("email", "").strip()

        # ì´ë©”ì¼ í˜•ì‹ ê²€ì¦
        import re

        email_regex = r"^[^\s@]+@[^\s@]+\.[^\s@]+$"
        if not email or not re.match(email_regex, email):
            raise HTTPException(status_code=400, detail="ì˜¬ë°”ë¥¸ ì´ë©”ì¼ ì£¼ì†Œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

        # ì´ë¯¸ ì¸ì¦ëœ ì´ë©”ì¼ì¸ì§€ í™•ì¸
        state_manager = get_state_manager()
        existing = state_manager.get_subscription(email)
        if existing and existing.verified:
            return {
                "success": True,
                "already_verified": True,
                "message": "ì´ë¯¸ ì¸ì¦ ì™„ë£Œëœ ì´ë©”ì¼ì…ë‹ˆë‹¤.",
            }

        # JWT í† í° ìƒì„± (30ë¶„ ìœ íš¨)
        token = create_email_verification_token(email)

        # ì¸ì¦ ì „ìš© í˜ì´ì§€ URL ìƒì„± (ëŒ€ì‹œë³´ë“œ ëŒ€ì‹  ì „ìš© í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸)
        base_url = get_base_url()
        verify_url = f"{base_url}/api/alerts/confirm-email?token={token}&email={email}"

        # EmailSender ì§ì ‘ ì‚¬ìš©
        from src.tools.email_sender import EmailSender

        email_sender = EmailSender()

        if not email_sender.is_enabled():
            raise HTTPException(status_code=503, detail="ì´ë©”ì¼ ì„œë¹„ìŠ¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # ì¸ì¦ ì´ë©”ì¼ ë°œì†¡
        result = await email_sender.send_verification_email(
            recipient=email, verify_url=verify_url, token=token
        )

        if result.success:
            logging.info(
                f"Verification email sent to {email} (JWT, expires in {EMAIL_VERIFICATION_EXPIRES_MINUTES}min)"
            )
            return {
                "success": True,
                "message": "ì¸ì¦ ì´ë©”ì¼ì´ ë°œì†¡ë˜ì—ˆìŠµë‹ˆë‹¤. (30ë¶„ ë‚´ ì¸ì¦í•´ì£¼ì„¸ìš”)",
            }
        else:
            raise HTTPException(status_code=500, detail=f"ì´ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨: {result.message}")

    except ValueError as e:
        # JWT_SECRET_KEY ë¯¸ì„¤ì • ì—ëŸ¬
        logging.error(f"JWT configuration error: {e}")
        raise HTTPException(status_code=500, detail=str(e)) from e
    except HTTPException:
        raise
    except Exception as e:
        logging.error(f"Send verification email error: {e}")
        raise HTTPException(status_code=500, detail=str(e)) from e


@app.post("/api/alerts/verify-email")
@limiter.limit("10/minute")  # ë¶„ë‹¹ 10íšŒ ì œí•œ (brute force ë°©ì§€)
async def verify_email_token_endpoint(request: Request):
    """
    ì´ë©”ì¼ ì¸ì¦ í† í° ê²€ì¦ (JWT ë°©ì‹)

    ë³´ì•ˆ: Rate Limitìœ¼ë¡œ brute force ë°©ì§€ (ë¶„ë‹¹ 10íšŒ)
    ì‚¬ìš©ìê°€ ì´ë©”ì¼ì˜ ì¸ì¦ ë²„íŠ¼ì„ í´ë¦­í•˜ë©´
    JWT í† í°ì„ ê²€ì¦í•˜ê³  ì´ë©”ì¼ ì¸ì¦ ìƒíƒœë¥¼ StateManagerì— ì˜êµ¬ ì €ì¥í•©ë‹ˆë‹¤.

    JWT í† í°ì€ statelessì´ë¯€ë¡œ ì„œë²„ ì¬ì‹œì‘ê³¼ ë¬´ê´€í•˜ê²Œ ê²€ì¦ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    """
    try:
        body = await request.json()
        token = body.get("token", "")
        email = body.get("email", "").strip()

        if not token or not email:
            raise HTTPException(status_code=400, detail="í† í°ê³¼ ì´ë©”ì¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")

        # JWT í† í° ê²€ì¦
        result = verify_jwt_email_token(token)

        if not result["valid"]:
            raise HTTPException(status_code=400, detail=result["error"])

        # í† í°ì˜ ì´ë©”ì¼ê³¼ ìš”ì²­ ì´ë©”ì¼ ì¼ì¹˜ í™•ì¸
        token_email = result["email"]
        if token_email != email:
            raise HTTPException(status_code=400, detail="ì´ë©”ì¼ì´ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

        # StateManagerì— ì¸ì¦ ì™„ë£Œ ìƒíƒœ ì˜êµ¬ ì €ì¥
        try:
            state_manager = get_state_manager()

            # ê¸°ì¡´ êµ¬ë… ì •ë³´ í™•ì¸
            existing = state_manager.get_subscription(email)

            if existing:
                # ê¸°ì¡´ êµ¬ë…ì´ ìˆìœ¼ë©´ verified ìƒíƒœë§Œ ì—…ë°ì´íŠ¸
                existing.verified = True
                existing.verified_at = datetime.now()
                state_manager._save_subscriptions()
            else:
                # ìƒˆ êµ¬ë… ë“±ë¡ (verified=Trueë¡œ ìƒì„±)
                state_manager.register_email(
                    email=email,
                    consent=True,
                    alert_types=["rank_change", "important_insight", "error", "daily_summary"],
                )
                # verified ìƒíƒœ ì¶”ê°€ ì„¤ì •
                subscription = state_manager.get_subscription(email)
                if subscription:
                    subscription.verified = True
                    subscription.verified_at = datetime.now()
                    state_manager._save_subscriptions()

            logging.info(f"Email verified and saved to StateManager: {email}")
        except Exception as e:
            logging.warning(f"Failed to save verification status: {e}")

        return {"verified": True, "email": email, "message": "ì´ë©”ì¼ ì¸ì¦ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!"}

    except HTTPException:
        raise
    except Exception as e:
        logging.error(f"Verify email error: {e}")
        raise HTTPException(status_code=500, detail=str(e)) from e


@app.get("/api/alerts/confirm-email")
async def confirm_email_page(token: str, email: str):
    """
    ì´ë©”ì¼ ì¸ì¦ í™•ì¸ í˜ì´ì§€ (GET ìš”ì²­ìœ¼ë¡œ ì ‘ê·¼)

    ì‚¬ìš©ìê°€ ì´ë©”ì¼ì˜ ì¸ì¦ ë§í¬ë¥¼ í´ë¦­í•˜ë©´ ì´ í˜ì´ì§€ê°€ í‘œì‹œë©ë‹ˆë‹¤.
    í† í°ì„ ê²€ì¦í•˜ê³  ì¸ì¦ ì™„ë£Œ ìƒíƒœë¥¼ ì €ì¥í•œ í›„, ì°½ì„ ë‹«ì•„ë„ ë˜ëŠ” ì•ˆë‚´ í˜ì´ì§€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì›ë˜ ëŒ€ì‹œë³´ë“œ íƒ­ì€ í´ë§ìœ¼ë¡œ ì¸ì¦ ì™„ë£Œë¥¼ ê°ì§€í•˜ì—¬ ìë™ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì´ë™í•©ë‹ˆë‹¤.
    """
    from fastapi.responses import HTMLResponse

    # JWT í† í° ê²€ì¦
    result = verify_jwt_email_token(token)

    if not result["valid"]:
        error_html = f"""
        <!DOCTYPE html>
        <html lang="ko">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>ì¸ì¦ ì‹¤íŒ¨ - AMORE Pacific</title>
            <style>
                * {{ margin: 0; padding: 0; box-sizing: border-box; }}
                body {{
                    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
                    background: linear-gradient(135deg, #001C58 0%, #1F5795 100%);
                    min-height: 100vh;
                    display: flex;
                    align-items: center;
                    justify-content: center;
                    padding: 20px;
                }}
                .card {{
                    background: white;
                    border-radius: 20px;
                    padding: 48px;
                    max-width: 420px;
                    width: 100%;
                    text-align: center;
                    box-shadow: 0 20px 60px rgba(0,0,0,0.3);
                }}
                .icon {{
                    width: 80px;
                    height: 80px;
                    background: #fee2e2;
                    border-radius: 50%;
                    display: flex;
                    align-items: center;
                    justify-content: center;
                    margin: 0 auto 24px;
                }}
                .icon svg {{ width: 40px; height: 40px; color: #ef4444; }}
                h1 {{ color: #001C58; font-size: 24px; margin-bottom: 12px; }}
                p {{ color: #64748b; font-size: 15px; line-height: 1.6; }}
                .error-msg {{ color: #ef4444; font-size: 13px; margin-top: 16px; padding: 12px; background: #fef2f2; border-radius: 8px; }}
            </style>
        </head>
        <body>
            <div class="card">
                <div class="icon">
                    <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12"/>
                    </svg>
                </div>
                <h1>ì¸ì¦ ì‹¤íŒ¨</h1>
                <p>ì´ë©”ì¼ ì¸ì¦ ë§í¬ê°€ ë§Œë£Œë˜ì—ˆê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.</p>
                <div class="error-msg">{result.get('error', 'í† í°ì´ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.')}</div>
                <p style="margin-top: 20px; font-size: 13px;">ëŒ€ì‹œë³´ë“œì—ì„œ ë‹¤ì‹œ ì¸ì¦ì„ ìš”ì²­í•´ì£¼ì„¸ìš”.</p>
            </div>
        </body>
        </html>
        """
        return HTMLResponse(content=error_html, status_code=400)

    # í† í°ì˜ ì´ë©”ì¼ê³¼ ìš”ì²­ ì´ë©”ì¼ ì¼ì¹˜ í™•ì¸
    token_email = result["email"]
    if token_email != email:
        return HTMLResponse(content="ì´ë©”ì¼ì´ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.", status_code=400)

    # StateManagerì— ì¸ì¦ ì™„ë£Œ ìƒíƒœ ì €ì¥
    try:
        state_manager = get_state_manager()
        existing = state_manager.get_subscription(email)

        if existing:
            existing.verified = True
            existing.verified_at = datetime.now()
            state_manager._save_subscriptions()
        else:
            state_manager.register_email(
                email=email,
                consent=True,
                alert_types=["rank_change", "important_insight", "error", "daily_summary"],
            )
            subscription = state_manager.get_subscription(email)
            if subscription:
                subscription.verified = True
                subscription.verified_at = datetime.now()
                state_manager._save_subscriptions()

        logging.info(f"Email verified via confirm page: {email}")
    except Exception as e:
        logging.warning(f"Failed to save verification status: {e}")

    # ì¸ì¦ ì„±ê³µ í˜ì´ì§€ ë°˜í™˜
    success_html = f"""
    <!DOCTYPE html>
    <html lang="ko">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>ì¸ì¦ ì™„ë£Œ - AMORE Pacific</title>
        <style>
            * {{ margin: 0; padding: 0; box-sizing: border-box; }}
            body {{
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
                background: linear-gradient(135deg, #001C58 0%, #1F5795 100%);
                min-height: 100vh;
                display: flex;
                align-items: center;
                justify-content: center;
                padding: 20px;
            }}
            .card {{
                background: white;
                border-radius: 20px;
                padding: 48px;
                max-width: 420px;
                width: 100%;
                text-align: center;
                box-shadow: 0 20px 60px rgba(0,0,0,0.3);
            }}
            .icon {{
                width: 80px;
                height: 80px;
                background: #d1fae5;
                border-radius: 50%;
                display: flex;
                align-items: center;
                justify-content: center;
                margin: 0 auto 24px;
                animation: pulse 2s infinite;
            }}
            @keyframes pulse {{
                0%, 100% {{ transform: scale(1); }}
                50% {{ transform: scale(1.05); }}
            }}
            .icon svg {{ width: 40px; height: 40px; color: #10b981; }}
            h1 {{ color: #001C58; font-size: 24px; margin-bottom: 12px; }}
            p {{ color: #64748b; font-size: 15px; line-height: 1.6; }}
            .email {{
                color: #1F5795;
                font-weight: 600;
                background: #f0f9ff;
                padding: 8px 16px;
                border-radius: 8px;
                display: inline-block;
                margin: 16px 0;
            }}
            .hint {{
                margin-top: 24px;
                padding: 16px;
                background: #f8fafc;
                border-radius: 12px;
                font-size: 13px;
                color: #475569;
            }}
            .close-btn {{
                margin-top: 24px;
                padding: 14px 32px;
                background: #001C58;
                color: white;
                border: none;
                border-radius: 10px;
                font-size: 15px;
                font-weight: 600;
                cursor: pointer;
                transition: background 0.2s;
            }}
            .close-btn:hover {{ background: #1F5795; }}
        </style>
    </head>
    <body>
        <div class="card">
            <div class="icon">
                <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 13l4 4L19 7"/>
                </svg>
            </div>
            <h1>ì´ë©”ì¼ ì¸ì¦ ì™„ë£Œ!</h1>
            <div class="email">{email}</div>
            <p>ì´ë©”ì¼ ì£¼ì†Œê°€ ì„±ê³µì ìœ¼ë¡œ ì¸ì¦ë˜ì—ˆìŠµë‹ˆë‹¤.</p>
            <div class="hint">
                ì´ ì°½ì€ ë‹«ì•„ë„ ë©ë‹ˆë‹¤.<br>
                ì›ë˜ ëŒ€ì‹œë³´ë“œ í™”ë©´ì—ì„œ ìë™ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì´ë™í•©ë‹ˆë‹¤.
            </div>
            <button class="close-btn" onclick="window.close()">ì´ ì°½ ë‹«ê¸°</button>
        </div>
    </body>
    </html>
    """
    return HTMLResponse(content=success_html)


@app.get("/api/alerts/verification-status")
async def get_verification_status(email: str):
    """
    ì´ë©”ì¼ ì¸ì¦ ìƒíƒœ í™•ì¸ (StateManager ê¸°ë°˜)

    JWT ë°©ì‹ìœ¼ë¡œ ë³€ê²½ë˜ì–´ ì¸ì¦ ì™„ë£Œ ìƒíƒœëŠ” StateManagerì— ì˜êµ¬ ì €ì¥ë©ë‹ˆë‹¤.
    """
    try:
        state_manager = get_state_manager()
        subscription = state_manager.get_subscription(email)

        if subscription:
            return {
                "verified": subscription.verified,
                "status": "verified" if subscription.verified else "pending",
                "verified_at": subscription.verified_at.isoformat()
                if subscription.verified_at
                else None,
            }

        return {"verified": False, "status": "not_found"}

    except Exception as e:
        logging.error(f"Get verification status error: {e}")
        return {"verified": False, "status": "error", "error": str(e)}


# ============= Insight Email API =============


@app.post("/api/alerts/send-insight-report")
async def send_insight_report_email(request: Request):
    """
    ì¸ì‚¬ì´íŠ¸ ë¦¬í¬íŠ¸ ì´ë©”ì¼ ë°œì†¡ (ìˆ˜ë™)

    ëŒ€ì‹œë³´ë“œì—ì„œ 'ì´ë©”ì¼ë¡œ ë³´ë‚´ê¸°' ë²„íŠ¼ í´ë¦­ ì‹œ í˜¸ì¶œë©ë‹ˆë‹¤.
    í˜„ì¬ ì¸ì‚¬ì´íŠ¸ì™€ KPI ë°ì´í„°ë¥¼ ì´ë©”ì¼ë¡œ ë°œì†¡í•©ë‹ˆë‹¤.

    StateManager ê¸°ë°˜ ì¸ì¦ ìƒíƒœ í™•ì¸ (JWT ë°©ì‹ ë³€ê²½ì— ë”°ë¥¸ ì—…ë°ì´íŠ¸)
    """
    try:
        body = await request.json()
        recipient_email = body.get("email", "").strip()

        if not recipient_email:
            raise HTTPException(status_code=400, detail="ì´ë©”ì¼ ì£¼ì†Œê°€ í•„ìš”í•©ë‹ˆë‹¤.")

        # StateManagerì—ì„œ ì´ë©”ì¼ ì¸ì¦ ìƒíƒœ í™•ì¸
        state_manager = get_state_manager()
        subscription = state_manager.get_subscription(recipient_email)

        if not subscription or not subscription.verified:
            raise HTTPException(
                status_code=403, detail="ì´ë©”ì¼ ì¸ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤. ë¨¼ì € ì´ë©”ì¼ì„ ì¸ì¦í•´ì£¼ì„¸ìš”."
            )

        # EmailSender ì´ˆê¸°í™”
        from src.tools.email_sender import EmailSender

        email_sender = EmailSender()

        if not email_sender.is_enabled():
            raise HTTPException(status_code=503, detail="ì´ë©”ì¼ ì„œë¹„ìŠ¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # í˜„ì¬ ëŒ€ì‹œë³´ë“œ ë°ì´í„° ë¡œë“œ
        dashboard_data = load_dashboard_data()
        if not dashboard_data:
            raise HTTPException(status_code=404, detail="ëŒ€ì‹œë³´ë“œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        # KPI ê³„ì‚°
        products = dashboard_data.get("products", [])
        laneige_products = [p for p in products if p.get("brand") == "LANEIGE"]
        avg_rank = (
            sum(p.get("rank", 100) for p in laneige_products) / len(laneige_products)
            if laneige_products
            else 0
        )

        # SoS ê³„ì‚° (Top 100 ê¸°ì¤€)
        top100 = products[:100]
        laneige_in_top100 = len([p for p in top100 if p.get("brand") == "LANEIGE"])
        sos = (laneige_in_top100 / len(top100) * 100) if top100 else 0

        # HHI ê³„ì‚°
        brand_counts = {}
        for p in top100:
            brand = p.get("brand", "Unknown")
            brand_counts[brand] = brand_counts.get(brand, 0) + 1
        hhi = (
            sum((count / len(top100) * 100) ** 2 for count in brand_counts.values())
            if top100
            else 0
        )

        # ì¸ì‚¬ì´íŠ¸ ê°€ì ¸ì˜¤ê¸° (ìºì‹œëœ ê²ƒ ë˜ëŠ” ìƒˆë¡œ ìƒì„±)
        insight_content = dashboard_data.get("latest_insight", "")
        if not insight_content:
            insight_content = (
                "<p>í˜„ì¬ ìƒì„±ëœ ì¸ì‚¬ì´íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ëŒ€ì‹œë³´ë“œì—ì„œ ì¸ì‚¬ì´íŠ¸ë¥¼ ë¨¼ì € ìƒì„±í•´ì£¼ì„¸ìš”.</p>"
            )
        else:
            # ë§ˆí¬ë‹¤ìš´ì„ HTMLë¡œ ê°„ë‹¨ ë³€í™˜
            insight_content = insight_content.replace("\n\n", "</p><p>").replace("\n", "<br>")
            insight_content = f"<p>{insight_content}</p>"

        # Top 10 ì œí’ˆ ë°ì´í„°
        top10_products = []
        for i, p in enumerate(products[:10]):
            top10_products.append(
                {
                    "rank": i + 1,
                    "name": p.get("title", "N/A"),
                    "brand": p.get("brand", "Unknown"),
                    "change": p.get("rank_change", 0),
                }
            )

        # ë¸Œëœë“œë³„ ë³€ë™
        brand_changes = []
        for brand in ["LANEIGE", "e.l.f.", "Maybelline", "Summer Fridays", "COSRX"]:
            brand_products = [p for p in products if p.get("brand") == brand]
            if brand_products:
                avg_change = sum(p.get("rank_change", 0) for p in brand_products) / len(
                    brand_products
                )
                if avg_change > 0:
                    brand_changes.append(
                        {
                            "brand": brand,
                            "change_text": f"í‰ê·  â–²{avg_change:.1f} ìƒìŠ¹",
                            "color": "#28a745",
                        }
                    )
                elif avg_change < 0:
                    brand_changes.append(
                        {
                            "brand": brand,
                            "change_text": f"í‰ê·  â–¼{abs(avg_change):.1f} í•˜ë½",
                            "color": "#dc3545",
                        }
                    )

        # ë¦¬í¬íŠ¸ ë‚ ì§œ
        report_date = datetime.now().strftime("%Yë…„ %mì›” %dì¼")

        # ëŒ€ì‹œë³´ë“œ URL (Railway ìë™ ê°ì§€)
        dashboard_url = get_base_url() + "/dashboard"

        # ì´ë©”ì¼ ë°œì†¡
        result = await email_sender.send_insight_report(
            recipients=[recipient_email],
            report_date=report_date,
            avg_rank=avg_rank,
            sos=sos,
            hhi=hhi,
            insight_content=insight_content,
            top10_products=top10_products,
            brand_changes=brand_changes,
            dashboard_url=dashboard_url,
        )

        if result.success:
            logging.info(f"Insight report sent to {recipient_email}")
            return {
                "success": True,
                "message": f"ì¸ì‚¬ì´íŠ¸ ë¦¬í¬íŠ¸ê°€ {recipient_email}ë¡œ ë°œì†¡ë˜ì—ˆìŠµë‹ˆë‹¤.",
                "sent_to": result.sent_to,
            }
        else:
            raise HTTPException(status_code=500, detail=f"ì´ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨: {result.message}")

    except HTTPException:
        raise
    except Exception as e:
        logging.error(f"Send insight report error: {e}")
        raise HTTPException(status_code=500, detail=str(e)) from e


# ============= Category KPI API =============


@app.get("/api/category/kpi")
async def get_category_kpi(
    category_id: str,
    start_date: str | None = None,
    end_date: str | None = None,
    brand: str = "LANEIGE",
):
    """
    ì¹´í…Œê³ ë¦¬ë³„ KPI ë°ì´í„° ì¡°íšŒ (ê¸°ê°„ í•„í„°ë§ ì§€ì›)

    Args:
        category_id: ì¹´í…Œê³ ë¦¬ ID (beauty_personal_care, skin_care, lip_care, lip_makeup, face_powder)
        start_date: ì‹œì‘ì¼ (YYYY-MM-DD)
        end_date: ì¢…ë£Œì¼ (YYYY-MM-DD)
        brand: íƒ€ê²Ÿ ë¸Œëœë“œ (ê¸°ë³¸ê°’: LANEIGE)

    Returns:
        KPI ë°ì´í„°: sos, best_rank, cpi, new_competitors
    """
    try:
        # ë‚ ì§œ ë²”ìœ„ ì„¤ì •
        if not end_date:
            end_date = datetime.now().strftime("%Y-%m-%d")
        if not start_date:
            start_date = (datetime.now() - timedelta(days=7)).strftime("%Y-%m-%d")

        rows = []

        # SQLiteì—ì„œ ë°ì´í„° ì¡°íšŒ
        try:
            from src.tools.sqlite_storage import get_sqlite_storage

            sqlite = get_sqlite_storage()
            await sqlite.initialize()

            query = """
                SELECT snapshot_date, rank, brand, price
                FROM raw_data
                WHERE snapshot_date BETWEEN ? AND ?
                AND category_id = ?
                ORDER BY snapshot_date DESC, rank ASC
            """
            with sqlite.get_connection() as conn:
                cursor = conn.execute(query, (start_date, end_date, category_id))
                rows = cursor.fetchall()
        except Exception as db_err:
            logging.warning(f"SQLite query failed for category KPI: {db_err}")

        # JSON fallback
        if not rows:
            crawl_data = _load_crawl_data_for_sos()
            if crawl_data and crawl_data.get("categories", {}).get(category_id):
                cat_data = crawl_data["categories"][category_id]
                snapshot_date = crawl_data.get("snapshot_date", end_date)
                for product in cat_data.get("products", []):
                    rows.append(
                        (
                            snapshot_date,
                            product.get("rank", 100),
                            product.get("brand", "Unknown"),
                            product.get("price"),
                        )
                    )

        if not rows:
            return {
                "success": True,
                "message": f"í•´ë‹¹ ê¸°ê°„({start_date} ~ {end_date})ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.",
                "data": None,
                "period": {"start": start_date, "end": end_date},
            }

        # KPI ê³„ì‚°
        total_products = len(rows)
        brand_products = [r for r in rows if r[2] and brand.lower() in r[2].lower()]
        brand_count = len(brand_products)

        # SoS (Share of Shelf)
        sos = (brand_count / total_products * 100) if total_products > 0 else 0

        # Best Rank
        brand_ranks = [r[1] for r in brand_products if r[1]]
        best_rank = min(brand_ranks) if brand_ranks else None

        # CPI (Competitive Price Index) - ë¸Œëœë“œ í‰ê· ê°€ / ì „ì²´ í‰ê· ê°€ * 100
        brand_prices = [r[3] for r in brand_products if r[3] and r[3] > 0]
        all_prices = [r[3] for r in rows if r[3] and r[3] > 0]

        if brand_prices and all_prices:
            brand_avg_price = sum(brand_prices) / len(brand_prices)
            all_avg_price = sum(all_prices) / len(all_prices)
            cpi = (brand_avg_price / all_avg_price * 100) if all_avg_price > 0 else 100
        else:
            cpi = 100

        # New Competitors (ìµœê·¼ 7ì¼ ë‚´ ì‹ ê·œ ì§„ì… - ê°„ì†Œí™”ëœ ê³„ì‚°)
        # ì‹¤ì œë¡œëŠ” ì´ì „ ê¸°ê°„ ë°ì´í„°ì™€ ë¹„êµ í•„ìš”, ì—¬ê¸°ì„œëŠ” ì¶”ì •ê°’
        new_competitors = max(0, total_products - brand_count - 50)  # ê°„ì†Œí™”ëœ ì¶”ì •

        return {
            "success": True,
            "data": {
                "category_id": category_id,
                "sos": round(sos, 1),
                "best_rank": best_rank,
                "cpi": round(cpi, 0),
                "new_competitors": new_competitors,
                "brand": brand,
                "product_count": brand_count,
                "total_products": total_products,
            },
            "period": {"start": start_date, "end": end_date},
        }

    except Exception as e:
        logging.error(f"Category KPI API error: {e}")
        return {"success": False, "error": str(e), "data": None}


# ============= SoS (Share of Shelf) API =============


def _load_crawl_data_for_sos():
    """JSON íŒŒì¼ì—ì„œ í¬ë¡¤ë§ ë°ì´í„° ë¡œë“œ (SQLite fallback)"""
    import json
    from pathlib import Path

    # latest_crawl_result.jsonì—ì„œ ë°ì´í„° ë¡œë“œ
    crawl_path = Path("./data/latest_crawl_result.json")
    if crawl_path.exists():
        with open(crawl_path, encoding="utf-8") as f:
            return json.load(f)
    return None


@app.get("/api/sos/category")
async def get_sos_by_category(
    start_date: str | None = None,
    end_date: str | None = None,
    compare_brands: str | None = None,  # comma-separated brand names
):
    """
    ì¹´í…Œê³ ë¦¬ë³„ SoS (Share of Shelf) ë°ì´í„° ì¡°íšŒ

    SoS = (í•´ë‹¹ ë¸Œëœë“œ ì œí’ˆ ìˆ˜ / Top 100) * 100

    Args:
        start_date: ì‹œì‘ì¼ (YYYY-MM-DD)
        end_date: ì¢…ë£Œì¼ (YYYY-MM-DD)
        compare_brands: ë¹„êµí•  ë¸Œëœë“œ (ì½¤ë§ˆë¡œ êµ¬ë¶„)

    Returns:
        ì¹´í…Œê³ ë¦¬ë³„ SoS ë°ì´í„°
    """
    try:
        # ë¹„êµ ë¸Œëœë“œ íŒŒì‹±
        compare_brand_list = []
        if compare_brands:
            compare_brand_list = [b.strip() for b in compare_brands.split(",") if b.strip()]

        # ë‚ ì§œ ë²”ìœ„ ì„¤ì •
        if not end_date:
            end_date = datetime.now().strftime("%Y-%m-%d")
        if not start_date:
            start_date = end_date

        # SQLite ë¨¼ì € ì‹œë„
        rows = []
        try:
            from src.tools.sqlite_storage import get_sqlite_storage

            sqlite = get_sqlite_storage()
            await sqlite.initialize()

            query = """
                SELECT snapshot_date, category_id, brand, COUNT(*) as product_count
                FROM raw_data
                WHERE snapshot_date BETWEEN ? AND ?
                GROUP BY snapshot_date, category_id, brand
                ORDER BY snapshot_date DESC, category_id, product_count DESC
            """
            with sqlite.get_connection() as conn:
                cursor = conn.execute(query, (start_date, end_date))
                rows = cursor.fetchall()
        except Exception as db_err:
            logging.warning(f"SQLite query failed, using JSON fallback: {db_err}")

        # SQLite ë°ì´í„° ì—†ìœ¼ë©´ JSON fallback
        if not rows:
            crawl_data = _load_crawl_data_for_sos()
            if crawl_data and crawl_data.get("categories"):
                # JSONì—ì„œ ë°ì´í„° ì¶”ì¶œ
                snapshot_date = crawl_data.get("snapshot_date", end_date)
                for cat_id, cat_data in crawl_data.get("categories", {}).items():
                    for product in cat_data.get("products", []):
                        brand = product.get("brand", "Unknown")
                        rows.append((snapshot_date, cat_id, brand, 1))

        if not rows:
            return {
                "success": True,
                "message": f"í•´ë‹¹ ê¸°ê°„({start_date} ~ {end_date})ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.",
                "data": [],
                "period": {"start": start_date, "end": end_date},
            }

        # ë°ì´í„° ì§‘ê³„
        # êµ¬ì¡°: {category_id: {brand: {dates: [count, ...], total_count: N}}}
        category_data = {}
        dates_set = set()

        for row in rows:
            if len(row) == 4:
                snapshot_date, category_id, brand, count = row
            else:
                snapshot_date, category_id, brand, count = row[0], row[1], row[2], row[3]
            dates_set.add(snapshot_date)

            if category_id not in category_data:
                category_data[category_id] = {}
            if brand not in category_data[category_id]:
                category_data[category_id][brand] = {"dates": {}, "total_count": 0}

            category_data[category_id][brand]["dates"][snapshot_date] = count
            category_data[category_id][brand]["total_count"] += count

        # SoS ê³„ì‚° (ê¸°ê°„ í‰ê· )
        num_dates = len(dates_set)
        result_data = []

        # ì¹´í…Œê³ ë¦¬ ê³„ì¸µ êµ¬ì¡° ë¡œë“œ
        hierarchy_path = Path("./config/category_hierarchy.json")
        hierarchy_data = {}
        if hierarchy_path.exists():
            with open(hierarchy_path, encoding="utf-8") as f:
                hierarchy_data = json.load(f).get("categories", {})

        # ì¹´í…Œê³ ë¦¬ ë©”íƒ€ ì •ë³´ (ê³„ì¸µ êµ¬ì¡° í¬í•¨)
        category_meta = {
            "beauty": {
                "name": "Beauty & Personal Care",
                "level": 0,
                "parent_id": None,
                "indent": 0,
                "order": 0,
            },
            "skin_care": {
                "name": "Skin Care",
                "level": 1,
                "parent_id": "beauty",
                "indent": 1,
                "order": 1,
            },
            "lip_care": {
                "name": "Lip Care",
                "level": 2,
                "parent_id": "skin_care",
                "indent": 2,
                "order": 2,
            },
            "lip_makeup": {
                "name": "Lip Makeup",
                "level": 2,
                "parent_id": "makeup",
                "indent": 1,
                "order": 3,
            },
            "face_powder": {
                "name": "Face Powder",
                "level": 3,
                "parent_id": "face_makeup",
                "indent": 2,
                "order": 4,
            },
        }

        # hierarchy_dataì—ì„œ ì •ë³´ ì—…ë°ì´íŠ¸
        for cat_id, meta in category_meta.items():
            if cat_id in hierarchy_data:
                meta["name"] = hierarchy_data[cat_id].get("name", meta["name"])
                meta["level"] = hierarchy_data[cat_id].get("level", meta["level"])
                meta["parent_id"] = hierarchy_data[cat_id].get("parent_id", meta["parent_id"])

        for category_id, brands in category_data.items():
            # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ ì´ ì œí’ˆ ìˆ˜ (ê¸°ê°„ í•©ê³„)
            total_products_in_category = sum(b["total_count"] for b in brands.values())

            # LANEIGE SoS
            laneige_count = 0
            laneige_appearance_days = 0  # Top 100ì— ì§„ì…í•œ ê³ ìœ  ë‚ ì§œ ìˆ˜
            laneige_dates = set()  # ì¤‘ë³µ ì œê±°ìš©
            laneige_variants = ["LANEIGE", "Laneige", "laneige"]
            for variant in laneige_variants:
                if variant in brands:
                    laneige_count += brands[variant]["total_count"]
                    # ì¶œí˜„ ë‚ ì§œ ìˆ˜ì§‘
                    if "dates" in brands[variant]:
                        laneige_dates.update(brands[variant]["dates"])
            laneige_appearance_days = len(laneige_dates)

            laneige_sos = (
                (laneige_count / total_products_in_category * 100)
                if total_products_in_category > 0
                else 0
            )

            # í‰ê·  SoS (ì „ì²´ ë¸Œëœë“œ ìˆ˜ ê¸°ì¤€)
            num_brands = len(brands)
            avg_sos = (100 / num_brands) if num_brands > 0 else 0

            # ë¹„êµ ë¸Œëœë“œ SoS
            compare_sos = {}
            for compare_brand in compare_brand_list:
                brand_count = 0
                for brand_name, brand_data in brands.items():
                    if compare_brand.lower() in brand_name.lower():
                        brand_count += brand_data["total_count"]
                compare_sos[compare_brand] = (
                    (brand_count / total_products_in_category * 100)
                    if total_products_in_category > 0
                    else 0
                )

            # LANEIGE ê°œë³„ ì œí’ˆ ë°ì´í„° (í•´ë‹¹ ì¹´í…Œê³ ë¦¬ ë‚´)
            # ì œí’ˆë³„ ìƒì„¸ëŠ” ë³„ë„ ì¿¼ë¦¬ í•„ìš” - ì—¬ê¸°ì„œëŠ” ë¸Œëœë“œ ë ˆë²¨ë§Œ

            # ì¹´í…Œê³ ë¦¬ ë©”íƒ€ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            meta = category_meta.get(
                category_id,
                {"name": category_id, "level": 0, "parent_id": None, "indent": 0, "order": 99},
            )

            result_data.append(
                {
                    "category_id": category_id,
                    "category_name": meta["name"],
                    "level": meta["level"],
                    "parent_id": meta["parent_id"],
                    "indent": meta["indent"],
                    "order": meta["order"],
                    "total_products": total_products_in_category // num_dates
                    if num_dates > 0
                    else 0,
                    "laneige_sos": round(laneige_sos, 2),
                    "laneige_count": round(laneige_count / num_dates, 1)
                    if num_dates > 0
                    else 0,  # ì†Œìˆ˜ì  1ìë¦¬ (ì¼ í‰ê· )
                    "laneige_appearance_days": laneige_appearance_days,  # ì¶œí˜„ ì¼ìˆ˜
                    "laneige_appearance_rate": round(laneige_appearance_days / num_dates * 100, 1)
                    if num_dates > 0
                    else 0,  # ì¶œí˜„ìœ¨ %
                    "avg_sos": round(avg_sos, 2),
                    "compare_brands": compare_sos,
                    "num_dates": num_dates,
                }
            )

        # ê³„ì¸µ êµ¬ì¡° ìˆœì„œëŒ€ë¡œ ì •ë ¬
        result_data.sort(key=lambda x: x.get("order", 99))

        return {
            "success": True,
            "period": {"start": start_date, "end": end_date, "days": num_dates},
            "data": result_data,
            "compare_brands": compare_brand_list,
            "hierarchy_info": {
                "description": "ê° ì¹´í…Œê³ ë¦¬ëŠ” ìì²´ Top 100 ê¸°ì¤€ìœ¼ë¡œ ë…ë¦½ ê³„ì‚°ë©ë‹ˆë‹¤.",
                "note": "ìƒìœ„ ì¹´í…Œê³ ë¦¬ì™€ í•˜ìœ„ ì¹´í…Œê³ ë¦¬ì˜ SoSëŠ” ì„œë¡œ ë‹¤ë¥¸ ë­í‚¹ì—ì„œ ê³„ì‚°ë©ë‹ˆë‹¤.",
            },
        }

    except Exception as e:
        logging.error(f"SoS category API error: {e}")
        return {"success": False, "error": str(e)}


@app.get("/api/sos/brands")
async def get_available_brands(category_id: str | None = None, min_count: int = 1):
    """
    ë¹„êµ ê°€ëŠ¥í•œ ë¸Œëœë“œ ëª©ë¡ ì¡°íšŒ (Top 100ì— í¬í•¨ëœ ë¸Œëœë“œë“¤)

    Args:
        category_id: íŠ¹ì • ì¹´í…Œê³ ë¦¬ë§Œ ì¡°íšŒ (ì„ íƒ)
        min_count: ìµœì†Œ ì œí’ˆ ìˆ˜ (ê¸°ë³¸: 1)

    Returns:
        ë¸Œëœë“œ ëª©ë¡ (ì œí’ˆ ìˆ˜ ê¸°ì¤€ ì •ë ¬)
    """
    try:
        end_date = datetime.now().strftime("%Y-%m-%d")
        start_date = (datetime.now() - timedelta(days=7)).strftime("%Y-%m-%d")

        rows = []
        # SQLite ë¨¼ì € ì‹œë„
        try:
            from src.tools.sqlite_storage import get_sqlite_storage

            sqlite = get_sqlite_storage()
            await sqlite.initialize()

            if category_id:
                query = """
                    SELECT brand, COUNT(DISTINCT asin) as product_count,
                           COUNT(DISTINCT snapshot_date) as days_present
                    FROM raw_data
                    WHERE snapshot_date BETWEEN ? AND ?
                    AND category_id = ?
                    AND LOWER(brand) != 'unknown'
                    GROUP BY brand
                    HAVING product_count >= ?
                    ORDER BY product_count DESC
                """
                params = (start_date, end_date, category_id, min_count)
            else:
                query = """
                    SELECT brand, COUNT(DISTINCT asin) as product_count,
                           COUNT(DISTINCT snapshot_date) as days_present
                    FROM raw_data
                    WHERE snapshot_date BETWEEN ? AND ?
                    AND LOWER(brand) != 'unknown'
                    GROUP BY brand
                    HAVING product_count >= ?
                    ORDER BY product_count DESC
                """
                params = (start_date, end_date, min_count)

            with sqlite.get_connection() as conn:
                cursor = conn.execute(query, params)
                rows = cursor.fetchall()
        except Exception as db_err:
            logging.warning(f"SQLite query failed for brands: {db_err}")

        # SQLite ë°ì´í„° ì—†ìœ¼ë©´ JSON fallback
        brands = []
        if not rows:
            crawl_data = _load_crawl_data_for_sos()
            if crawl_data and crawl_data.get("categories"):
                brand_counts = {}
                for cat_id, cat_data in crawl_data.get("categories", {}).items():
                    if category_id and cat_id != category_id:
                        continue
                    for product in cat_data.get("products", []):
                        brand = product.get("brand", "Unknown")
                        if brand:
                            brand_counts[brand] = brand_counts.get(brand, 0) + 1

                for brand_name, count in sorted(brand_counts.items(), key=lambda x: -x[1]):
                    # Unknown ë¸Œëœë“œ ì œì™¸
                    if (
                        count >= min_count
                        and brand_name.strip()
                        and brand_name.lower() != "unknown"
                    ):
                        brands.append(
                            {
                                "name": brand_name,
                                "product_count": count,
                                "days_present": 1,
                                "is_laneige": "laneige" in brand_name.lower(),
                            }
                        )
        else:
            for row in rows:
                brand_name, product_count, days_present = row
                # Unknown ë¸Œëœë“œ ì œì™¸ (SQLì—ì„œë„ í•„í„°ë§í•˜ì§€ë§Œ ì´ì¤‘ ì²´í¬)
                if brand_name and brand_name.strip() and brand_name.lower() != "unknown":
                    brands.append(
                        {
                            "name": brand_name,
                            "product_count": product_count,
                            "days_present": days_present,
                            "is_laneige": "laneige" in brand_name.lower(),
                        }
                    )

        return {
            "success": True,
            "period": {"start": start_date, "end": end_date},
            "category_id": category_id,
            "brands": brands,
            "total_brands": len(brands),
        }

    except Exception as e:
        logging.error(f"SoS brands API error: {e}")
        return {"success": False, "error": str(e)}


@app.get("/api/sos/trend")
async def get_sos_trend(
    brand: str = "LANEIGE",
    category_id: str | None = None,
    days: int = 7,
    start_date: str | None = None,
    end_date: str | None = None,
):
    """
    ë¸Œëœë“œì˜ SoS ì¶”ì„¸ ë°ì´í„° (ì¼ë³„)

    Args:
        brand: ë¸Œëœë“œëª… (ê¸°ë³¸: LANEIGE)
        category_id: ì¹´í…Œê³ ë¦¬ (ì„ íƒ, ì—†ìœ¼ë©´ ì „ì²´)
        days: ì¡°íšŒ ê¸°ê°„ (ê¸°ë³¸: 7ì¼, start_date/end_dateê°€ ì—†ì„ ë•Œë§Œ ì‚¬ìš©)
        start_date: ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)
        end_date: ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)

    Returns:
        ì¼ë³„ SoS ì¶”ì„¸ ë°ì´í„°
    """
    try:
        from src.tools.sqlite_storage import get_sqlite_storage

        sqlite = get_sqlite_storage()
        await sqlite.initialize()

        # start_date/end_dateê°€ ì œê³µë˜ë©´ ì‚¬ìš©, ì•„ë‹ˆë©´ days ê¸°ë°˜ìœ¼ë¡œ ê³„ì‚°
        if start_date and end_date:
            pass  # ê·¸ëŒ€ë¡œ ì‚¬ìš©
        else:
            end_date = datetime.now().strftime("%Y-%m-%d")
            start_date = (datetime.now() - timedelta(days=days)).strftime("%Y-%m-%d")

        # ì¼ë³„ ì „ì²´ ì œí’ˆ ìˆ˜
        if category_id:
            total_query = """
                SELECT snapshot_date, COUNT(*) as total_count
                FROM raw_data
                WHERE snapshot_date BETWEEN ? AND ?
                AND category_id = ?
                GROUP BY snapshot_date
                ORDER BY snapshot_date
            """
            total_params = (start_date, end_date, category_id)

            brand_query = """
                SELECT snapshot_date, COUNT(*) as brand_count
                FROM raw_data
                WHERE snapshot_date BETWEEN ? AND ?
                AND category_id = ?
                AND LOWER(brand) LIKE ?
                GROUP BY snapshot_date
                ORDER BY snapshot_date
            """
            brand_params = (start_date, end_date, category_id, f"%{brand.lower()}%")
        else:
            total_query = """
                SELECT snapshot_date, COUNT(*) as total_count
                FROM raw_data
                WHERE snapshot_date BETWEEN ? AND ?
                GROUP BY snapshot_date
                ORDER BY snapshot_date
            """
            total_params = (start_date, end_date)

            brand_query = """
                SELECT snapshot_date, COUNT(*) as brand_count
                FROM raw_data
                WHERE snapshot_date BETWEEN ? AND ?
                AND LOWER(brand) LIKE ?
                GROUP BY snapshot_date
                ORDER BY snapshot_date
            """
            brand_params = (start_date, end_date, f"%{brand.lower()}%")

        with sqlite.get_connection() as conn:
            # ì „ì²´ ì¹´ìš´íŠ¸
            cursor = conn.execute(total_query, total_params)
            total_rows = cursor.fetchall()
            total_by_date = {row[0]: row[1] for row in total_rows}

            # ë¸Œëœë“œ ì¹´ìš´íŠ¸
            cursor = conn.execute(brand_query, brand_params)
            brand_rows = cursor.fetchall()
            brand_by_date = {row[0]: row[1] for row in brand_rows}

        # SoS ê³„ì‚°
        trend_data = []
        for date, total in sorted(total_by_date.items()):
            brand_count = brand_by_date.get(date, 0)
            sos = (brand_count / total * 100) if total > 0 else 0
            trend_data.append(
                {
                    "date": date,
                    "total_products": total,
                    "brand_count": brand_count,
                    "sos": round(sos, 2),
                }
            )

        return {
            "success": True,
            "brand": brand,
            "category_id": category_id,
            "period": {"start": start_date, "end": end_date, "days": days},
            "trend": trend_data,
        }

    except Exception as e:
        logging.error(f"SoS trend API error: {e}")
        return {"success": False, "error": str(e)}


@app.get("/api/sos/trend/competitors-avg")
async def get_competitors_avg_sos_trend(
    category_id: str | None = None,
    days: int = 7,
    start_date: str | None = None,
    end_date: str | None = None,
    top_n: int = 10,
    exclude_brand: str = "LANEIGE",
):
    """
    ê²½ìŸ ë¸Œëœë“œ í‰ê·  SoS ì¶”ì„¸ ë°ì´í„° (ì¼ë³„)
    Top N ë¸Œëœë“œ(LANEIGE ì œì™¸)ì˜ í‰ê·  ì‹œì¥ì ìœ ìœ¨ ì¶”ì´

    Args:
        category_id: ì¹´í…Œê³ ë¦¬ (ì„ íƒ, ì—†ìœ¼ë©´ ì „ì²´)
        days: ì¡°íšŒ ê¸°ê°„ (ê¸°ë³¸: 7ì¼)
        start_date: ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)
        end_date: ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)
        top_n: ìƒìœ„ ëª‡ ê°œ ë¸Œëœë“œ (ê¸°ë³¸: 10)
        exclude_brand: ì œì™¸í•  ë¸Œëœë“œ (ê¸°ë³¸: LANEIGE)

    Returns:
        ê²½ìŸ ë¸Œëœë“œ í‰ê·  SoS ì¶”ì„¸ ë°ì´í„°
    """
    try:
        from src.tools.sqlite_storage import get_sqlite_storage

        sqlite = get_sqlite_storage()
        await sqlite.initialize()

        # ë‚ ì§œ ë²”ìœ„ ê²°ì •
        if start_date and end_date:
            pass
        else:
            end_date = datetime.now().strftime("%Y-%m-%d")
            start_date = (datetime.now() - timedelta(days=days)).strftime("%Y-%m-%d")

        # ì¼ë³„ ì „ì²´ ì œí’ˆ ìˆ˜ ì¿¼ë¦¬
        if category_id:
            total_query = """
                SELECT snapshot_date, COUNT(*) as total_count
                FROM raw_data
                WHERE snapshot_date BETWEEN ? AND ?
                AND category_id = ?
                GROUP BY snapshot_date
                ORDER BY snapshot_date
            """
            total_params = (start_date, end_date, category_id)

            # ì¼ë³„/ë¸Œëœë“œë³„ ì œí’ˆ ìˆ˜ (LANEIGE ì œì™¸)
            brand_daily_query = """
                SELECT snapshot_date, brand, COUNT(*) as brand_count
                FROM raw_data
                WHERE snapshot_date BETWEEN ? AND ?
                AND category_id = ?
                AND LOWER(brand) NOT LIKE ?
                AND brand IS NOT NULL
                AND brand != ''
                GROUP BY snapshot_date, brand
                ORDER BY snapshot_date, brand_count DESC
            """
            brand_daily_params = (start_date, end_date, category_id, f"%{exclude_brand.lower()}%")
        else:
            total_query = """
                SELECT snapshot_date, COUNT(*) as total_count
                FROM raw_data
                WHERE snapshot_date BETWEEN ? AND ?
                GROUP BY snapshot_date
                ORDER BY snapshot_date
            """
            total_params = (start_date, end_date)

            brand_daily_query = """
                SELECT snapshot_date, brand, COUNT(*) as brand_count
                FROM raw_data
                WHERE snapshot_date BETWEEN ? AND ?
                AND LOWER(brand) NOT LIKE ?
                AND brand IS NOT NULL
                AND brand != ''
                GROUP BY snapshot_date, brand
                ORDER BY snapshot_date, brand_count DESC
            """
            brand_daily_params = (start_date, end_date, f"%{exclude_brand.lower()}%")

        with sqlite.get_connection() as conn:
            # ì „ì²´ ì¹´ìš´íŠ¸
            cursor = conn.execute(total_query, total_params)
            total_rows = cursor.fetchall()
            total_by_date = {row[0]: row[1] for row in total_rows}

            # ì¼ë³„/ë¸Œëœë“œë³„ ì¹´ìš´íŠ¸
            cursor = conn.execute(brand_daily_query, brand_daily_params)
            brand_rows = cursor.fetchall()

        # ì¼ë³„ë¡œ Top N ë¸Œëœë“œì˜ í‰ê·  SoS ê³„ì‚°
        from collections import defaultdict

        # ì¼ë³„ ë¸Œëœë“œ ë°ì´í„° ê·¸ë£¹í™”
        daily_brands: dict[str, list[tuple[str, int]]] = defaultdict(list)
        for date, brand, count in brand_rows:
            daily_brands[date].append((brand, count))

        # ì¼ë³„ ê²½ìŸ ë¸Œëœë“œ í‰ê·  SoS ê³„ì‚°
        trend_data = []
        for date, total in sorted(total_by_date.items()):
            brands_for_date = daily_brands.get(date, [])
            # ì´ë¯¸ brand_count DESCë¡œ ì •ë ¬ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ìƒìœ„ Nê°œ ì„ íƒ
            top_brands = brands_for_date[:top_n]

            if top_brands and total > 0:
                # ê° ë¸Œëœë“œì˜ SoS ê³„ì‚°
                sos_values = [(count / total * 100) for _, count in top_brands]
                avg_sos = sum(sos_values) / len(sos_values)
            else:
                avg_sos = 0

            trend_data.append(
                {
                    "date": date,
                    "total_products": total,
                    "top_brands_count": len(top_brands),
                    "avg_sos": round(avg_sos, 2),
                }
            )

        return {
            "success": True,
            "category_id": category_id,
            "excluded_brand": exclude_brand,
            "top_n": top_n,
            "period": {"start": start_date, "end": end_date, "days": days},
            "trend": trend_data,
        }

    except Exception as e:
        logging.error(f"Competitors avg SoS trend API error: {e}")
        return {"success": False, "error": str(e)}


# ============= ë°ì´í„° ë™ê¸°í™” API =============


@app.get("/api/sync/status")
async def sync_status():
    """
    Railway Volumeì˜ ë°ì´í„° í˜„í™© ë°˜í™˜

    Returns:
        - latest: ìµœì‹  ë°ì´í„° ë‚ ì§œ
        - oldest: ê°€ì¥ ì˜¤ë˜ëœ ë°ì´í„° ë‚ ì§œ
        - total_days: ì´ ì¼ìˆ˜
        - total_records: SQLite raw_data ì´ ë ˆì½”ë“œ ìˆ˜
    """
    try:
        sqlite = get_sqlite_storage()
        if not sqlite:
            raise HTTPException(status_code=500, detail="SQLite not available")

        await sqlite.initialize()

        # raw_data í…Œì´ë¸”ì—ì„œ ë‚ ì§œ ë²”ìœ„ ì¡°íšŒ
        with sqlite.get_connection() as conn:
            cursor = conn.execute("""
                SELECT
                    MIN(snapshot_date) as oldest,
                    MAX(snapshot_date) as latest,
                    COUNT(DISTINCT snapshot_date) as total_days,
                    COUNT(*) as total_records
                FROM raw_data
            """)
            row = cursor.fetchone()

            if not row or not row[0]:
                return {
                    "success": True,
                    "latest": None,
                    "oldest": None,
                    "total_days": 0,
                    "total_records": 0,
                    "message": "No data available",
                }

            return {
                "success": True,
                "latest": row[1],
                "oldest": row[0],
                "total_days": row[2],
                "total_records": row[3],
            }
    except Exception as e:
        logging.error(f"Sync status error: {e}")
        raise HTTPException(status_code=500, detail=str(e)) from e


@app.get("/api/sync/dates")
async def sync_dates():
    """
    ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ë‚ ì§œ ëª©ë¡ ë°˜í™˜ (ì •ë ¬ë¨)

    Returns:
        - dates: ["2026-01-17", "2026-01-18", ..., "2026-01-25"]
    """
    try:
        sqlite = get_sqlite_storage()
        if not sqlite:
            raise HTTPException(status_code=500, detail="SQLite not available")

        await sqlite.initialize()

        with sqlite.get_connection() as conn:
            cursor = conn.execute("""
                SELECT DISTINCT snapshot_date
                FROM raw_data
                ORDER BY snapshot_date
            """)
            dates = [row[0] for row in cursor.fetchall()]

        return {"success": True, "dates": dates, "count": len(dates)}
    except Exception as e:
        logging.error(f"Sync dates error: {e}")
        raise HTTPException(status_code=500, detail=str(e)) from e


@app.get("/api/sync/download/{date}")
async def sync_download(date: str):
    """
    íŠ¹ì • ë‚ ì§œì˜ raw_dataë¥¼ JSONìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ

    Args:
        date: ë‚ ì§œ (YYYY-MM-DD í˜•ì‹)

    Returns:
        JSON array of raw_data records for the specified date
    """
    import re

    # ë‚ ì§œ í˜•ì‹ ê²€ì¦
    if not re.match(r"^\d{4}-\d{2}-\d{2}$", date):
        raise HTTPException(status_code=400, detail="Invalid date format. Use YYYY-MM-DD")

    try:
        sqlite = get_sqlite_storage()
        if not sqlite:
            raise HTTPException(status_code=500, detail="SQLite not available")

        await sqlite.initialize()

        with sqlite.get_connection() as conn:
            # ì»¬ëŸ¼ëª… ê°€ì ¸ì˜¤ê¸°
            cursor = conn.execute("PRAGMA table_info(raw_data)")
            columns = [row[1] for row in cursor.fetchall()]

            # í•´ë‹¹ ë‚ ì§œ ë°ì´í„° ì¡°íšŒ
            cursor = conn.execute(
                """
                SELECT * FROM raw_data
                WHERE snapshot_date = ?
                ORDER BY category_id, rank
            """,
                (date,),
            )
            rows = cursor.fetchall()

        if not rows:
            raise HTTPException(status_code=404, detail=f"No data found for date: {date}")

        # ë”•ì…”ë„ˆë¦¬ ë³€í™˜
        records = []
        for row in rows:
            record = dict(zip(columns, row, strict=False))
            records.append(record)

        return {"success": True, "date": date, "count": len(records), "records": records}
    except HTTPException:
        raise
    except Exception as e:
        logging.error(f"Sync download error for {date}: {e}")
        raise HTTPException(status_code=500, detail=str(e)) from e


@app.post("/api/sync/upload")
async def sync_upload(request: Request):
    """
    ë¡œì»¬ì—ì„œ Railwayë¡œ raw_data ì—…ë¡œë“œ

    Request Body:
        {
            "records": [...],  # raw_data ë ˆì½”ë“œ ë°°ì—´
            "api_key": "..."   # ì¸ì¦ í‚¤ (ì„ íƒ)
        }

    Returns:
        {"success": True, "inserted": N, "updated": M}
    """
    import os

    try:
        body = await request.json()
        records = body.get("records", [])
        api_key = body.get("api_key", "")

        # API í‚¤ ê²€ì¦ (ì„¤ì •ëœ ê²½ìš°)
        expected_key = os.getenv("API_KEY", "")
        if expected_key and api_key != expected_key:
            raise HTTPException(status_code=401, detail="Invalid API key")

        if not records:
            raise HTTPException(status_code=400, detail="No records provided")

        sqlite = get_sqlite_storage()
        if not sqlite:
            raise HTTPException(status_code=500, detail="SQLite not available")

        await sqlite.initialize()

        inserted = 0
        updated = 0

        with sqlite.get_connection() as conn:
            # ìŠ¤í‚¤ë§ˆ ë§ˆì´ê·¸ë ˆì´ì…˜: ëˆ„ë½ëœ ì»¬ëŸ¼ ì¶”ê°€
            migration_columns = [
                ("image_url", "TEXT"),
                ("is_best_seller", "INTEGER DEFAULT 0"),
                ("is_amazon_choice", "INTEGER DEFAULT 0"),
            ]
            for col_name, col_type in migration_columns:
                try:
                    conn.execute(f"ALTER TABLE raw_data ADD COLUMN {col_name} {col_type}")
                    conn.commit()
                    logging.info(f"Added {col_name} column to raw_data table")
                except Exception:
                    pass  # ì´ë¯¸ ì¡´ì¬í•˜ë©´ ë¬´ì‹œ

            for record in records:
                # UPSERT: snapshot_date + category_id + asin ì¡°í•©ì´ unique key
                cursor = conn.execute(
                    """
                    SELECT id FROM raw_data
                    WHERE snapshot_date = ? AND category_id = ? AND asin = ?
                """,
                    (
                        record.get("snapshot_date"),
                        record.get("category_id"),
                        record.get("asin"),
                    ),
                )
                existing = cursor.fetchone()

                if existing:
                    # UPDATE
                    conn.execute(
                        """
                        UPDATE raw_data SET
                            rank = ?, product_name = ?, brand = ?, price = ?,
                            rating = ?, reviews_count = ?, product_url = ?,
                            image_url = ?, is_best_seller = ?, is_amazon_choice = ?
                        WHERE id = ?
                    """,
                        (
                            record.get("rank"),
                            record.get("product_name"),
                            record.get("brand"),
                            record.get("price"),
                            record.get("rating"),
                            record.get("reviews_count"),
                            record.get("product_url"),
                            record.get("image_url"),
                            record.get("is_best_seller", 0),
                            record.get("is_amazon_choice", 0),
                            existing[0],
                        ),
                    )
                    updated += 1
                else:
                    # INSERT
                    conn.execute(
                        """
                        INSERT INTO raw_data (
                            snapshot_date, category_id, asin, rank, product_name,
                            brand, price, rating, reviews_count, product_url,
                            image_url, is_best_seller, is_amazon_choice
                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """,
                        (
                            record.get("snapshot_date"),
                            record.get("category_id"),
                            record.get("asin"),
                            record.get("rank"),
                            record.get("product_name"),
                            record.get("brand"),
                            record.get("price"),
                            record.get("rating"),
                            record.get("reviews_count"),
                            record.get("product_url"),
                            record.get("image_url"),
                            record.get("is_best_seller", 0),
                            record.get("is_amazon_choice", 0),
                        ),
                    )
                    inserted += 1

            conn.commit()

        logging.info(f"Sync upload: inserted={inserted}, updated={updated}")
        return {"success": True, "inserted": inserted, "updated": updated}

    except HTTPException:
        raise
    except Exception as e:
        logging.error(f"Sync upload error: {e}")
        raise HTTPException(status_code=500, detail=str(e)) from e


# =============================================================================
# Market Intelligence API (v2026.01.26)
# =============================================================================


@app.get("/api/market-intelligence/status", response_model=MarketIntelligenceStatusResponse)
async def get_market_intelligence_status():
    """
    Market Intelligence ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ

    Returns:
        ì´ˆê¸°í™” ìƒíƒœ, ìˆ˜ì§‘ëœ ë ˆì´ì–´, í†µê³„
    """
    try:
        engine = await get_market_intelligence()
        stats = engine.get_stats()

        # ë§ˆì§€ë§‰ ìˆ˜ì§‘ ì‹œê°„
        last_collection = None
        if engine.layer_data:
            times = [ld.collected_at for ld in engine.layer_data.values()]
            if times:
                last_collection = max(times)

        return MarketIntelligenceStatusResponse(
            initialized=engine._initialized,
            layers_collected=list(engine.layer_data.keys()),
            last_collection=last_collection,
            stats=stats,
        )
    except Exception as e:
        logger.error(f"Market Intelligence status error: {e}")
        raise HTTPException(status_code=500, detail=str(e)) from e


@app.get("/api/market-intelligence/layers")
async def get_market_intelligence_layers(layer: int | None = None):
    """
    4-Layer ë°ì´í„° ì¡°íšŒ

    Args:
        layer: íŠ¹ì • ë ˆì´ì–´ë§Œ ì¡°íšŒ (1-4, Noneì´ë©´ ì „ì²´)

    Returns:
        ë ˆì´ì–´ë³„ ë°ì´í„°
    """
    try:
        engine = await get_market_intelligence()

        if layer is not None:
            layer_data = engine.layer_data.get(layer)
            if not layer_data:
                return {
                    "error": f"Layer {layer} ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.",
                    "available_layers": list(engine.layer_data.keys()),
                }

            return {
                "layer": layer_data.layer,
                "layer_name": layer_data.layer_name,
                "collected_at": layer_data.collected_at,
                "data": layer_data.data,
                "sources": layer_data.sources,
            }

        # ì „ì²´ ë ˆì´ì–´
        result = {}
        for layer_num, layer_data in engine.layer_data.items():
            result[f"layer_{layer_num}"] = {
                "layer": layer_data.layer,
                "layer_name": layer_data.layer_name,
                "collected_at": layer_data.collected_at,
                "data": layer_data.data,
                "sources": layer_data.sources,
            }

        return result
    except Exception as e:
        logger.error(f"Market Intelligence layers error: {e}")
        raise HTTPException(status_code=500, detail=str(e)) from e


@app.post("/api/market-intelligence/collect", dependencies=[Depends(verify_api_key)])
async def collect_market_intelligence(layers: list[int] | None = None):
    """
    Market Intelligence ë°ì´í„° ìˆ˜ì§‘ íŠ¸ë¦¬ê±°

    Args:
        layers: ìˆ˜ì§‘í•  ë ˆì´ì–´ ëª©ë¡ (Noneì´ë©´ ì „ì²´)

    Returns:
        ìˆ˜ì§‘ ê²°ê³¼
    """
    try:
        engine = await get_market_intelligence()

        if layers:
            # íŠ¹ì • ë ˆì´ì–´ë§Œ ìˆ˜ì§‘
            results = {}
            for layer_num in layers:
                layer_data = await engine.collect_layer(layer_num)
                if layer_data:
                    results[f"layer_{layer_num}"] = {
                        "status": "collected",
                        "collected_at": layer_data.collected_at,
                        "sources_count": len(layer_data.sources),
                    }
                else:
                    results[f"layer_{layer_num}"] = {"status": "skipped"}
        else:
            # ì „ì²´ ìˆ˜ì§‘
            await engine.collect_all_layers()
            results = {
                f"layer_{k}": {
                    "status": "collected",
                    "collected_at": v.collected_at,
                    "sources_count": len(v.sources),
                }
                for k, v in engine.layer_data.items()
            }

        # ë°ì´í„° ì €ì¥
        engine.save_data()

        return {"status": "success", "collected": results, "timestamp": datetime.now().isoformat()}
    except Exception as e:
        logger.error(f"Market Intelligence collection error: {e}")
        raise HTTPException(status_code=500, detail=str(e)) from e


@app.get("/api/market-intelligence/insight")
async def get_market_intelligence_insight(include_amazon: bool = False):
    """
    4-Layer ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ìƒì„±

    Args:
        include_amazon: Layer 1 Amazon ë°ì´í„° í¬í•¨ ì—¬ë¶€

    Returns:
        ìƒì„±ëœ ì¸ì‚¬ì´íŠ¸ í…ìŠ¤íŠ¸
    """
    try:
        engine = await get_market_intelligence()

        # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ë¨¼ì € ìˆ˜ì§‘
        if not engine.layer_data:
            await engine.collect_all_layers()

        # Amazon ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ì„ íƒ)
        amazon_data = None
        if include_amazon:
            try:
                await get_sqlite_storage()
                # ìµœì‹  LANEIGE ë°ì´í„° ì¡°íšŒ
                # (ì‹¤ì œ êµ¬í˜„ì€ storageì˜ ë©”ì„œë“œì— ë”°ë¼ ë‹¤ë¦„)
                amazon_data = {"sos": 5.2, "laneige_rank": 15}  # placeholder
            except Exception:
                pass

        insight = engine.generate_layered_insight(amazon_data=amazon_data)

        return {
            "insight": insight,
            "generated_at": datetime.now().isoformat(),
            "layers_used": list(engine.layer_data.keys()),
        }
    except Exception as e:
        logger.error(f"Market Intelligence insight error: {e}")
        raise HTTPException(status_code=500, detail=str(e)) from e


@app.get("/api/insights/sources")
async def get_insight_sources():
    """
    ì¸ì‚¬ì´íŠ¸ ì¶œì²˜ ì •ë³´ ì¡°íšŒ

    Returns:
        ì¶œì²˜ ëª©ë¡ ë° í†µê³„
    """
    try:
        engine = await get_market_intelligence()

        all_sources = []
        for layer_data in engine.layer_data.values():
            all_sources.extend(layer_data.sources)

        # ì¶œì²˜ ìœ í˜•ë³„ í†µê³„
        by_type = {}
        for source in all_sources:
            source_type = source.get("source_type", "unknown")
            by_type[source_type] = by_type.get(source_type, 0) + 1

        return {
            "total_sources": len(all_sources),
            "by_type": by_type,
            "sources": all_sources[:20],  # ìµœê·¼ 20ê°œ
            "source_manager_stats": engine.source_manager.get_stats(),
        }
    except Exception as e:
        logger.error(f"Insight sources error: {e}")
        raise HTTPException(status_code=500, detail=str(e)) from e


# ============= ì„œë²„ ì‹¤í–‰ =============

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8001)
