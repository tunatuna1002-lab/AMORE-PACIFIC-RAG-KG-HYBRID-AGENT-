"""
Dashboard API Server
====================
ëŒ€ì‹œë³´ë“œìš© FastAPI ë°±ì—”ë“œ ì„œë²„ (ë©”ì¸ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸)

## í•µì‹¬ ê¸°ëŠ¥
- ì±—ë´‡ API (ChatGPT + RAG + Ontology ì—°ë™)
- DOCX ì¸ì‚¬ì´íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±
- ëŒ€í™” ë©”ëª¨ë¦¬ ì§€ì› (ì„¸ì…˜ë³„ TTL ê¸°ë°˜)
- Audit Trail ë¡œê¹…

## ì•„í‚¤í…ì²˜ íë¦„
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           FastAPI Server                                â”‚
â”‚   dashboard_api.py (PORT 8001)                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  /api/chat â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º HybridChatbotAgent â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º LLM (GPT-4.1)  â”‚
â”‚                                   â”‚                                     â”‚
â”‚                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚                         â–¼                   â–¼                          â”‚
â”‚                  KnowledgeGraph      DocumentRetriever                  â”‚
â”‚                  (ì˜¨í†¨ë¡œì§€)          (RAG ê°€ì´ë“œë¼ì¸)                   â”‚
â”‚                                                                         â”‚
â”‚  /api/crawl/start â”€â”€â”€â”€â–º UnifiedBrain â”€â”€â”€â”€â–º AmazonScraper               â”‚
â”‚                              â”‚              (Playwright)                â”‚
â”‚                              â–¼                                          â”‚
â”‚                        MetricCalculator                                 â”‚
â”‚                              â”‚                                          â”‚
â”‚                              â–¼                                          â”‚
â”‚                       SheetsWriter / SQLite                             â”‚
â”‚                                                                         â”‚
â”‚  /api/data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º dashboard_data.json (ìºì‹œëœ ë°ì´í„°)            â”‚
â”‚                                                                         â”‚
â”‚  /dashboard â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º amore_unified_dashboard_v4.html                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ì£¼ìš” ì—”ë“œí¬ì¸íŠ¸
- GET  /           : í—¬ìŠ¤ì²´í¬
- GET  /api/data   : ëŒ€ì‹œë³´ë“œ ë°ì´í„° JSON
- POST /api/chat   : ì±—ë´‡ v1 (RAG)
- POST /api/v2/chat: ì±—ë´‡ v2 (Unified Brain)
- POST /api/v3/chat: ì±—ë´‡ v3 (Simple Chat)
- POST /api/crawl/start: í¬ë¡¤ë§ ì‹œì‘ (API Key í•„ìš”)
- GET  /dashboard  : ëŒ€ì‹œë³´ë“œ UI

## í™˜ê²½ ë³€ìˆ˜
- OPENAI_API_KEY: OpenAI API í‚¤ (í•„ìˆ˜)
- API_KEY: ë³´í˜¸ëœ ì—”ë“œí¬ì¸íŠ¸ìš© ì¸ì¦í‚¤
- AUTO_START_SCHEDULER: ì„œë²„ ì‹œì‘ ì‹œ ìŠ¤ì¼€ì¤„ëŸ¬ ìë™ ì‹œì‘ (default: true)
"""

import json
import os
import asyncio
import logging
from datetime import datetime

logger = logging.getLogger(__name__)
from typing import Dict, Any, List, Optional
from io import BytesIO
from collections import defaultdict
from pathlib import Path

from fastapi import FastAPI, HTTPException, Depends, Security, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from fastapi.security import APIKeyHeader
from starlette.middleware.base import BaseHTTPMiddleware
from pydantic import BaseModel, Field
from dotenv import load_dotenv
from litellm import acompletion

# Rate Limiting
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded

from docx import Document
from docx.shared import Inches, Pt, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH
from docx.enum.table import WD_TABLE_ALIGNMENT

# RAG ì‹œìŠ¤í…œ ì—°ë™
from src.rag.router import RAGRouter, QueryType
from src.rag.retriever import DocumentRetriever

# Ontology ìŠ¤í‚¤ë§ˆ
from src.domain.entities import ProductMetrics, BrandMetrics, MarketMetrics

# í†µí•© ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° (deprecated - use UnifiedBrain instead)
from src.core.brain import UnifiedBrain, get_brain
from src.core.crawl_manager import get_crawl_manager, CrawlStatus

# SQLite Storage
from src.tools.sqlite_storage import SQLiteStorage, get_sqlite_storage

# Level 4 Brain (LLM-First Autonomous Agent)
from src.core.brain import UnifiedBrain, get_brain, get_initialized_brain, BrainMode, TaskPriority

# External Signal Routes
from src.api.routes.signals import router as signals_router

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# Rate Limiter ì„¤ì • (IP ê¸°ë°˜)
limiter = Limiter(key_func=get_remote_address)

app = FastAPI(
    title="AMORE Dashboard API",
    description="LANEIGE Amazon ëŒ€ì‹œë³´ë“œ ë°±ì—”ë“œ API (RAG + Ontology í†µí•©)",
    version="2.0.0"
)

# Rate Limit ì´ˆê³¼ ì‹œ ì—ëŸ¬ í•¸ë“¤ëŸ¬
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

# CORS ì„¤ì • (í™˜ê²½ë³€ìˆ˜ë¡œ í—ˆìš© ë„ë©”ì¸ ì„¤ì • ê°€ëŠ¥)
ALLOWED_ORIGINS = os.getenv("ALLOWED_ORIGINS", "http://localhost:8001,http://127.0.0.1:8001").split(",")

app.add_middleware(
    CORSMiddleware,
    allow_origins=ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["GET", "POST", "DELETE", "OPTIONS"],
    allow_headers=["Content-Type", "X-API-Key", "Authorization"],
)


# Security Headers Middleware
class SecurityHeadersMiddleware(BaseHTTPMiddleware):
    """ë³´ì•ˆ í—¤ë” ì¶”ê°€ ë¯¸ë“¤ì›¨ì–´"""
    async def dispatch(self, request, call_next):
        response = await call_next(request)
        response.headers["X-Content-Type-Options"] = "nosniff"
        response.headers["X-Frame-Options"] = "SAMEORIGIN"  # iframe ì„ë² ë”©ì€ ê°™ì€ ë„ë©”ì¸ë§Œ í—ˆìš©
        response.headers["X-XSS-Protection"] = "1; mode=block"
        return response


app.add_middleware(SecurityHeadersMiddleware)

# External Signals Router ë“±ë¡
app.include_router(signals_router)

# ============= ì„œë²„ ì‹œì‘ ì‹œ ìë™ ìŠ¤ì¼€ì¤„ëŸ¬ =============

AUTO_START_SCHEDULER = os.getenv("AUTO_START_SCHEDULER", "true").lower() == "true"


@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ìë™ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ ë° ì¦‰ì‹œ í¬ë¡¤ë§ ì²´í¬"""
    # 1. ì¦‰ì‹œ í¬ë¡¤ë§ í•„ìš” ì—¬ë¶€ ì²´í¬ (ì„œë²„ ì¬ì‹œì‘ í›„ ì˜¤ëŠ˜ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ë°”ë¡œ í¬ë¡¤ë§)
    try:
        crawl_manager = await get_crawl_manager()
        if crawl_manager.needs_crawl():
            logging.info(f"ì„œë²„ ì‹œì‘: ì˜¤ëŠ˜({crawl_manager.get_kst_today()}) ë°ì´í„° ì—†ìŒ â†’ í¬ë¡¤ë§ ì‹œì‘")
            await crawl_manager.start_crawl()
        else:
            logging.info(f"ì„œë²„ ì‹œì‘: ì˜¤ëŠ˜ ë°ì´í„° ìˆìŒ ë˜ëŠ” í¬ë¡¤ë§ ì¤‘ (data_date={crawl_manager.get_data_date()})")
    except Exception as e:
        logging.error(f"ì„œë²„ ì‹œì‘ í¬ë¡¤ë§ ì²´í¬ ì‹¤íŒ¨: {e}")

    # 2. ììœ¨ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ (ë§¤ì¼ 06:00 ì •ê¸° í¬ë¡¤ë§ìš©)
    if AUTO_START_SCHEDULER:
        try:
            brain = await get_initialized_brain()
            await brain.start_scheduler()
            logging.info("ììœ¨ ìŠ¤ì¼€ì¤„ëŸ¬ ìë™ ì‹œì‘ ì™„ë£Œ (ë§¤ì¼ í•œêµ­ì‹œê°„ 06:00 í¬ë¡¤ë§)")
        except Exception as e:
            logging.error(f"ììœ¨ ìŠ¤ì¼€ì¤„ëŸ¬ ìë™ ì‹œì‘ ì‹¤íŒ¨: {e}")

# ë°ì´í„° ê²½ë¡œ
DATA_PATH = "./data/dashboard_data.json"
DOCS_PATH = "./"  # MD íŒŒì¼ë“¤ì´ ë£¨íŠ¸ì— ìˆìŒ
AUDIT_LOG_DIR = "./logs"

# ============= API Key ì¸ì¦ ì„¤ì • =============

# API_KEY: í™˜ê²½ë³€ìˆ˜ í•„ìˆ˜ - ê¸°ë³¸ê°’ ì—†ìŒ (ë³´ì•ˆìƒ í•˜ë“œì½”ë”© ê¸ˆì§€)
API_KEY = os.getenv("API_KEY")
if not API_KEY:
    logging.warning("âš ï¸ API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë³´í˜¸ëœ ì—”ë“œí¬ì¸íŠ¸ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
api_key_header = APIKeyHeader(name="X-API-Key", auto_error=False)


async def verify_api_key(api_key: str = Security(api_key_header)):
    """
    API Key ê²€ì¦ (ë¯¼ê°í•œ ì—”ë“œí¬ì¸íŠ¸ìš©)

    ì‚¬ìš©ë²•: ì—”ë“œí¬ì¸íŠ¸ì— dependencies=[Depends(verify_api_key)] ì¶”ê°€
    """
    if api_key is None:
        raise HTTPException(
            status_code=401,
            detail="API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤. í—¤ë”ì— X-API-Keyë¥¼ ì¶”ê°€í•˜ì„¸ìš”."
        )
    if api_key != API_KEY:
        raise HTTPException(
            status_code=403,
            detail="ìœ íš¨í•˜ì§€ ì•Šì€ API Keyì…ë‹ˆë‹¤."
        )
    return api_key


# ============= Audit Trail Logger ì„¤ì • =============

def setup_audit_logger():
    """Audit Trail ë¡œê±° ì„¤ì •"""
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    Path(AUDIT_LOG_DIR).mkdir(parents=True, exist_ok=True)

    # ì˜¤ëŠ˜ ë‚ ì§œ ê¸°ë°˜ ë¡œê·¸ íŒŒì¼
    today = datetime.now().strftime("%Y-%m-%d")
    log_file = Path(AUDIT_LOG_DIR) / f"chatbot_audit_{today}.log"

    # ë¡œê±° ìƒì„±
    audit_logger = logging.getLogger("audit_trail")
    audit_logger.setLevel(logging.INFO)

    # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±° (ì¤‘ë³µ ë°©ì§€)
    audit_logger.handlers.clear()

    # íŒŒì¼ í•¸ë“¤ëŸ¬ ì¶”ê°€
    file_handler = logging.FileHandler(log_file, encoding="utf-8")
    file_handler.setLevel(logging.INFO)

    # í¬ë§· ì„¤ì •
    formatter = logging.Formatter(
        '%(asctime)s | %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    file_handler.setFormatter(formatter)
    audit_logger.addHandler(file_handler)

    return audit_logger

audit_logger = setup_audit_logger()

def log_chat_interaction(
    session_id: str,
    user_query: str,
    ai_response: str,
    query_type: str,
    confidence: float,
    entities: Dict,
    sources: List[str],
    response_time_ms: float
):
    """ì±—ë´‡ ëŒ€í™” Audit Trail ê¸°ë¡"""
    audit_entry = {
        "session_id": session_id,
        "timestamp": datetime.now().isoformat(),
        "user_query": user_query,
        "ai_response": ai_response[:500] + "..." if len(ai_response) > 500 else ai_response,
        "query_type": query_type,
        "confidence": round(confidence, 4),
        "entities": entities,
        "sources": sources,
        "response_time_ms": round(response_time_ms, 2)
    }

    # JSON í˜•ì‹ìœ¼ë¡œ ë¡œê·¸ ê¸°ë¡
    audit_logger.info(json.dumps(audit_entry, ensure_ascii=False))

# ============= Global Instances =============

# RAG ì‹œìŠ¤í…œ
rag_router = RAGRouter()
doc_retriever = DocumentRetriever(DOCS_PATH)

# ì„¸ì…˜ë³„ ëŒ€í™” ë©”ëª¨ë¦¬ (TTL ê¸°ë°˜ ìë™ ì •ë¦¬)
conversation_memory: Dict[str, List[Dict[str, str]]] = defaultdict(list)
session_last_activity: Dict[str, datetime] = {}  # ì„¸ì…˜ë³„ ë§ˆì§€ë§‰ í™œë™ ì‹œê°„
MAX_MEMORY_TURNS = 10
SESSION_TTL_HOURS = 1  # ì„¸ì…˜ ë§Œë£Œ ì‹œê°„ (1ì‹œê°„)
MAX_SESSIONS = 1000  # ìµœëŒ€ ì„¸ì…˜ ìˆ˜


def cleanup_expired_sessions() -> int:
    """ë§Œë£Œëœ ì„¸ì…˜ ì •ë¦¬ (TTL ê¸°ë°˜)"""
    now = datetime.now()
    expired = [
        sid for sid, last_time in session_last_activity.items()
        if (now - last_time).total_seconds() > SESSION_TTL_HOURS * 3600
    ]
    for sid in expired:
        if sid in conversation_memory:
            del conversation_memory[sid]
        if sid in session_last_activity:
            del session_last_activity[sid]
    return len(expired)

# í†µí•© ì‹œìŠ¤í…œì€ UnifiedBrain (get_brain())ìœ¼ë¡œ ê´€ë¦¬ë¨


# ============= Pydantic Models =============

class ChatRequest(BaseModel):
    """ì±—ë´‡ ìš”ì²­"""
    message: str = Field(..., max_length=10000, description="ìµœëŒ€ 10,000ì")
    session_id: Optional[str] = Field(default="default", max_length=100)
    context: Optional[Dict] = None


class ChatResponse(BaseModel):
    """ì±—ë´‡ ì‘ë‹µ"""
    response: str
    query_type: str
    confidence: float
    sources: List[str]
    suggestions: List[str]
    entities: Dict[str, Any]


class ExportRequest(BaseModel):
    """ë‚´ë³´ë‚´ê¸° ìš”ì²­"""
    start_date: Optional[str] = None
    end_date: Optional[str] = None
    include_strategy: bool = True


# ============= Helper Functions =============

def load_dashboard_data() -> Dict[str, Any]:
    """ëŒ€ì‹œë³´ë“œ ë°ì´í„° ë¡œë“œ"""
    try:
        with open(DATA_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        return {}


def get_conversation_history(session_id: str, limit: int = 5) -> str:
    """ëŒ€í™” ê¸°ë¡ ì¡°íšŒ (ë¬¸ìì—´ í˜•íƒœ)"""
    history = conversation_memory.get(session_id, [])[-limit:]
    if not history:
        return ""

    lines = []
    for turn in history:
        role = "ì‚¬ìš©ì" if turn["role"] == "user" else "AI"
        content = turn["content"][:150] + "..." if len(turn["content"]) > 150 else turn["content"]
        lines.append(f"[{role}]: {content}")

    return "\n".join(lines)


def add_to_memory(session_id: str, role: str, content: str) -> None:
    """ëŒ€í™” ë©”ëª¨ë¦¬ì— ì¶”ê°€"""
    now = datetime.now()

    # ì£¼ê¸°ì  ì •ë¦¬ (ë§¤ 100ë²ˆì§¸ í˜¸ì¶œë§ˆë‹¤ ë˜ëŠ” ì„¸ì…˜ ìˆ˜ ì´ˆê³¼ ì‹œ)
    if len(session_last_activity) > MAX_SESSIONS or len(session_last_activity) % 100 == 0:
        cleanup_expired_sessions()

    # ë§ˆì§€ë§‰ í™œë™ ì‹œê°„ ì—…ë°ì´íŠ¸
    session_last_activity[session_id] = now

    conversation_memory[session_id].append({
        "role": role,
        "content": content,
        "timestamp": now.isoformat()
    })
    # ìµœëŒ€ ê°œìˆ˜ ìœ ì§€
    if len(conversation_memory[session_id]) > MAX_MEMORY_TURNS * 2:
        conversation_memory[session_id] = conversation_memory[session_id][-MAX_MEMORY_TURNS * 2:]


def build_data_context(data: Dict, query_type: QueryType, entities: Dict) -> str:
    """
    ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (Ontology ê¸°ë°˜)

    ì§ˆë¬¸ ìœ í˜•ê³¼ ì¶”ì¶œëœ ì—”í‹°í‹°ì— ë”°ë¼ í•„ìš”í•œ ë°ì´í„°ë§Œ ì„ íƒ
    """
    if not data:
        return "í˜„ì¬ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

    context_parts = []

    # ë©”íƒ€ë°ì´í„° (í•­ìƒ í¬í•¨)
    metadata = data.get("metadata", {})
    context_parts.append(f"""[ë°ì´í„° í˜„í™©]
- ê¸°ì¤€ì¼: {metadata.get('data_date', 'N/A')}
- ì´ ì œí’ˆ ìˆ˜: {metadata.get('total_products', 0)}ê°œ
- LANEIGE ì œí’ˆ ìˆ˜: {metadata.get('laneige_products', 0)}ê°œ""")

    # ì§ˆë¬¸ ìœ í˜•ë³„ ë°ì´í„° ì„ íƒ
    brand_kpis = data.get("brand", {}).get("kpis", {})

    # ì‹œì¥/ë¸Œëœë“œ ì§€í‘œ (DEFINITION, INTERPRETATION, ANALYSIS)
    if query_type in [QueryType.DEFINITION, QueryType.INTERPRETATION, QueryType.ANALYSIS, QueryType.COMBINATION]:
        if brand_kpis:
            context_parts.append(f"""
[LANEIGE ë¸Œëœë“œ KPI] (Ontology: BrandMetrics)
- SoS (Share of Shelf): {brand_kpis.get('sos', 0)}% {brand_kpis.get('sos_delta', '')}
- Top 10 ì œí’ˆ ìˆ˜: {brand_kpis.get('top10_count', 0)}ê°œ
- í‰ê·  ìˆœìœ„: {brand_kpis.get('avg_rank', 0)}ìœ„
- HHI (ì‹œì¥ ì§‘ì¤‘ë„): {brand_kpis.get('hhi', 0)}""")

    # ê²½ìŸì‚¬ ì •ë³´ (ANALYSIS, DATA_QUERYì—ì„œ ê²½ìŸì‚¬ ì–¸ê¸‰ ì‹œ)
    competitors = data.get("brand", {}).get("competitors", [])
    brands_mentioned = entities.get("brands", [])

    if query_type == QueryType.ANALYSIS or any(b for b in brands_mentioned if b.lower() != "laneige"):
        if competitors:
            top_comps = competitors[:5]
            comp_lines = [f"  - {c['brand']}: SoS {c['sos']}%, í‰ê·  ìˆœìœ„ {c['avg_rank']}ìœ„, ì œí’ˆ {c['product_count']}ê°œ" for c in top_comps]
            context_parts.append("[ê²½ìŸì‚¬ í˜„í™©]\n" + "\n".join(comp_lines))

    # ì œí’ˆ ì •ë³´ (DATA_QUERY, íŠ¹ì • ì œí’ˆ ì–¸ê¸‰ ì‹œ)
    products = data.get("products", {})
    products_mentioned = entities.get("products", [])

    if query_type == QueryType.DATA_QUERY or products_mentioned:
        if products:
            prod_lines = []
            for asin, p in list(products.items())[:5]:
                prod_lines.append(f"""  - {p['name'][:40]}
    ìˆœìœ„: #{p['rank']} ({p['rank_delta']}), í‰ì : {p['rating']}, ë³€ë™ì„±: {p.get('volatility_status', 'N/A')}""")
            context_parts.append("[LANEIGE ì œí’ˆ í˜„í™©] (Ontology: ProductMetrics)\n" + "\n".join(prod_lines))

    # ì¹´í…Œê³ ë¦¬ ì •ë³´
    categories = data.get("categories", {})
    categories_mentioned = entities.get("categories", [])

    if categories_mentioned or query_type in [QueryType.ANALYSIS, QueryType.INTERPRETATION]:
        if categories:
            cat_lines = []
            for cat_id, cat in categories.items():
                cat_lines.append(f"  - {cat['name']}: SoS {cat['sos']}%, ìµœê³  ìˆœìœ„ #{cat['best_rank']}, CPI {cat.get('cpi', 100)}")
            context_parts.append("[ì¹´í…Œê³ ë¦¬ í˜„í™©] (Ontology: MarketMetrics)\n" + "\n".join(cat_lines))

    # ì•¡ì…˜ ì•„ì´í…œ (ì „ëµ ì§ˆë¬¸)
    if query_type == QueryType.ANALYSIS:
        action_items = data.get("home", {}).get("action_items", [])
        if action_items:
            action_lines = [f"  - [{a['priority']}] {a['product_name']}: {a['signal']} â†’ {a['action_tag']}" for a in action_items[:4]]
            context_parts.append("[í˜„ì¬ ì•¡ì…˜ ì•„ì´í…œ]\n" + "\n".join(action_lines))

    return "\n\n".join(context_parts)


async def get_rag_context(query: str, query_type: QueryType) -> tuple[str, List[str]]:
    """
    RAG ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰

    Returns:
        (ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´, ì¶œì²˜ ëª©ë¡)
    """
    # DocumentRetriever ì´ˆê¸°í™” (ì²˜ìŒ í˜¸ì¶œ ì‹œ)
    if not doc_retriever._initialized:
        await doc_retriever.initialize()

    # ì§ˆë¬¸ ìœ í˜•ì— ë§ëŠ” ë¬¸ì„œ ê²€ìƒ‰
    target_doc = rag_router.get_target_document(query_type)

    # ê²€ìƒ‰ ì‹¤í–‰
    results = await doc_retriever.search(query, top_k=3, doc_filter=target_doc)

    if not results:
        return "", []

    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context_parts = []
    sources = []

    for result in results:
        metadata = result.get("metadata", {})
        content = result.get("content", "")
        title = metadata.get("title", "Unknown")
        doc_id = metadata.get("doc_id", "")

        context_parts.append(f"[{title}]\n{content}")

        # ì¶œì²˜ ì¶”ê°€
        doc_name_map = {
            "strategic_indicators": "Strategic Indicators Definition",
            "metric_interpretation": "Metric Interpretation Guide",
            "indicator_combination": "Indicator Combination Playbook",
            "home_insight_rules": "Home Page Insight Rules"
        }
        if doc_id in doc_name_map and doc_name_map[doc_id] not in sources:
            sources.append(doc_name_map[doc_id])

    return "\n\n---\n\n".join(context_parts), sources


def get_dynamic_suggestions(query_type: QueryType, entities: Dict, response: str) -> List[str]:
    """
    ë™ì  í›„ì† ì§ˆë¬¸ ì œì•ˆ

    ì‘ë‹µ ë‚´ìš©ê³¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë§ì¶¤í˜• ì œì•ˆ ìƒì„±
    """
    suggestions = []

    # ì—”í‹°í‹° ê¸°ë°˜ ì œì•ˆ
    brands = entities.get("brands", [])
    indicators = entities.get("indicators", [])
    categories = entities.get("categories", [])

    if query_type == QueryType.DEFINITION:
        # ì •ì˜ ì§ˆë¬¸ â†’ í•´ì„/í™œìš© ì§ˆë¬¸ ì œì•ˆ
        if indicators:
            ind = indicators[0].upper()
            suggestions.append(f"{ind}ê°€ ë†’ìœ¼ë©´ ì–´ë–¤ ì˜ë¯¸ì¸ê°€ìš”?")
            suggestions.append(f"{ind} ê°œì„ ì„ ìœ„í•œ ì „ëµì€?")
        suggestions.append("ë‹¤ë¥¸ ì§€í‘œì™€ í•¨ê»˜ í•´ì„í•˜ë©´ ì–´ë–¤ê°€ìš”?")

    elif query_type == QueryType.INTERPRETATION:
        # í•´ì„ ì§ˆë¬¸ â†’ ì•¡ì…˜/ì¡°í•© ì œì•ˆ
        suggestions.append("í˜„ì¬ LANEIGE ìˆ˜ì¹˜ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”")
        suggestions.append("ê²½ìŸì‚¬ì™€ ë¹„êµí•˜ë©´ ì–´ë–¤ê°€ìš”?")
        suggestions.append("ê°œì„ ì„ ìœ„í•œ ì•¡ì…˜ ì•„ì´í…œì€?")

    elif query_type == QueryType.DATA_QUERY:
        # ë°ì´í„° ì¡°íšŒ â†’ ì‹¬í™” ë¶„ì„ ì œì•ˆ
        suggestions.append("ì´ ìˆ˜ì¹˜ê°€ ì¢‹ì€ ê±´ê°€ìš”?")
        suggestions.append("ìµœê·¼ 7ì¼ ì¶”ì´ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”")
        suggestions.append("ê²½ìŸì‚¬ ëŒ€ë¹„ ì–´ë–¤ê°€ìš”?")

    elif query_type == QueryType.ANALYSIS:
        # ë¶„ì„ â†’ ì „ëµ/ì•¡ì…˜ ì œì•ˆ
        suggestions.append("ê°€ì¥ ì‹œê¸‰í•œ ì•¡ì…˜ì€ ë¬´ì—‡ì¸ê°€ìš”?")
        suggestions.append("Top 10 ì§„ì…ì„ ìœ„í•œ ì „ëµì€?")
        suggestions.append("ë¦¬ìŠ¤í¬ ìš”ì¸ì´ ìˆë‚˜ìš”?")

    elif query_type == QueryType.COMBINATION:
        # ì¡°í•© ì§ˆë¬¸ â†’ ë‹¤ë¥¸ ì¡°í•© ì œì•ˆ
        suggestions.append("ë‹¤ë¥¸ ì‹œë‚˜ë¦¬ì˜¤ë„ ë¶„ì„í•´ì£¼ì„¸ìš”")
        suggestions.append("í˜„ì¬ ë°ì´í„°ì—ì„œ í•´ë‹¹ ìƒí™©ì´ ìˆë‚˜ìš”?")

    else:
        # ê¸°ë³¸ ì œì•ˆ
        suggestions = [
            "SoS(ì ìœ ìœ¨)ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”",
            "í˜„ì¬ LANEIGE ìˆœìœ„ëŠ”?",
            "ì „ëµì  ê¶Œê³ ì‚¬í•­ì´ ìˆë‚˜ìš”?"
        ]

    return suggestions[:3]


# ============= API Endpoints =============

@app.get("/")
async def root():
    """í—¬ìŠ¤ ì²´í¬"""
    return {
        "status": "ok",
        "message": "AMORE Dashboard API v2.0 (RAG + Ontology)",
        "features": ["chatbot", "rag", "ontology", "memory", "docx_export"]
    }


@app.get("/api/data")
async def get_data():
    """ëŒ€ì‹œë³´ë“œ ë°ì´í„° ì¡°íšŒ"""
    data = load_dashboard_data()
    if not data:
        raise HTTPException(status_code=404, detail="Dashboard data not found")
    return data


@app.post("/api/chat", response_model=ChatResponse)
@limiter.limit("30/minute")  # ë¶„ë‹¹ 30íšŒ ì œí•œ
async def chat(request: Request, body: ChatRequest):
    """
    ChatGPT + RAG + Ontology í†µí•© ì±—ë´‡ API

    1. ì§ˆë¬¸ ë¶„ì„ (RAGRouter)
    2. ì—”í‹°í‹° ì¶”ì¶œ (Ontology ê¸°ë°˜)
    3. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ (RAG)
    4. ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    5. ëŒ€í™” ê¸°ë¡ ì°¸ì¡°
    6. LLM ì‘ë‹µ ìƒì„±
    7. Audit Trail ë¡œê¹…
    """
    import time
    start_time = time.time()

    message = body.message.strip()
    session_id = body.session_id or "default"

    if not message:
        raise HTTPException(status_code=400, detail="Message is required")

    # 1. ì§ˆë¬¸ ë¶„ë¥˜ (RAGRouter ì‚¬ìš©)
    route_result = rag_router.route(message)
    query_type = route_result["query_type"]
    confidence = route_result["confidence"]

    # 2. ì—”í‹°í‹° ì¶”ì¶œ (Ontology ê¸°ë°˜)
    entities = rag_router.extract_entities(message)

    # 3. ëª…í™•í™” í•„ìš” ì—¬ë¶€ í™•ì¸
    clarification = rag_router.needs_clarification(route_result, entities)
    if clarification and confidence < 0.5:
        # ëª…í™•í™” ìš”ì²­
        add_to_memory(session_id, "user", message)
        add_to_memory(session_id, "assistant", clarification)

        return ChatResponse(
            response=clarification,
            query_type=query_type.value if hasattr(query_type, 'value') else str(query_type),
            confidence=confidence,
            sources=[],
            suggestions=["ì˜ˆ, ì „ì²´ ë¸Œëœë“œ ë¶„ì„í•´ì£¼ì„¸ìš”", "LANEIGEë§Œ ë¶„ì„í•´ì£¼ì„¸ìš”", "Lip Care ì¹´í…Œê³ ë¦¬ë§Œ"],
            entities=entities
        )

    # 4. RAG ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
    rag_context, sources = await get_rag_context(message, query_type)

    # 5. ë°ì´í„° ë¡œë“œ ë° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    data = load_dashboard_data()
    data_context = build_data_context(data, query_type, entities)

    # 6. ëŒ€í™” ê¸°ë¡ ì¡°íšŒ
    conversation_history = get_conversation_history(session_id)

    # 7. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    system_prompt = """ë‹¹ì‹ ì€ AMORE Pacificì˜ LANEIGE ë¸Œëœë“œ Amazon ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì—­í• :
- Amazon US ë² ìŠ¤íŠ¸ì…€ëŸ¬ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì¸ì‚¬ì´íŠ¸ ì œê³µ
- LANEIGE ë¸Œëœë“œì˜ ì‹œì¥ í¬ì§€ì…˜ ë¶„ì„
- ê²½ìŸì‚¬ ëŒ€ë¹„ ì „ëµì  ê¶Œê³  ì œê³µ
- ì§€í‘œ ì •ì˜ ë° í•´ì„ ê°€ì´ë“œ ì œê³µ

Ontology ì—”í‹°í‹° ì´í•´:
- Brand: ë¸Œëœë“œ ì •ë³´ (LANEIGE, ê²½ìŸì‚¬ ë“±)
- Product: ì œí’ˆ ì •ë³´ (ASIN, ìˆœìœ„, í‰ì , ê°€ê²© ë“±)
- Category: ì¹´í…Œê³ ë¦¬ (Lip Care, Skin Care ë“±)
- BrandMetrics: SoS, í‰ê· ìˆœìœ„, ì œí’ˆìˆ˜ ë“±
- ProductMetrics: ìˆœìœ„ë³€ë™ì„±, ì—°ì†ì²´ë¥˜ì¼, í‰ì ì¶”ì„¸ ë“±
- MarketMetrics: HHI(ì‹œì¥ì§‘ì¤‘ë„), êµì²´ìœ¨ ë“±

ì‘ë‹µ ê°€ì´ë“œë¼ì¸:
1. ë°ì´í„°ì— ê¸°ë°˜í•œ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ ì¸ìš©
2. RAG ë¬¸ì„œì˜ ì •ì˜/í•´ì„ ê¸°ì¤€ í™œìš©
3. ì´ì „ ëŒ€í™” ë§¥ë½ ê³ ë ¤
4. ê°„ê²°í•˜ê³  ì•¡ì…˜ ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸ ì œê³µ
5. ë¶ˆí™•ì‹¤í•œ ê²½ìš° ëª…í™•íˆ ë°í ê²ƒ
6. ë‹¨ì •ì  í‘œí˜„ í”¼í•˜ê¸°
7. í•œêµ­ì–´ë¡œ ì‘ë‹µ

ì§ˆë¬¸ ìœ í˜•ë³„ ì‘ë‹µ ìŠ¤íƒ€ì¼:
- ì •ì˜(DEFINITION): ì§€í‘œì˜ ì •ì˜, ì‚°ì¶œì‹, ì˜ë¯¸ë¥¼ ì„¤ëª…
- í•´ì„(INTERPRETATION): ìˆ˜ì¹˜ì˜ ì˜ë¯¸, ì¢‹ê³  ë‚˜ì¨ì˜ ê¸°ì¤€ ì„¤ëª…
- ì¡°í•©(COMBINATION): ì—¬ëŸ¬ ì§€í‘œë¥¼ í•¨ê»˜ í•´ì„, ì‹œë‚˜ë¦¬ì˜¤ë³„ ì•¡ì…˜ ì œì•ˆ
- ë°ì´í„°ì¡°íšŒ(DATA_QUERY): í˜„ì¬ ìˆ˜ì¹˜ì™€ ë³€ë™ í˜„í™© ì•ˆë‚´
- ë¶„ì„(ANALYSIS): ì¢…í•© ë¶„ì„ê³¼ ì „ëµì  ê¶Œê³  ì œê³µ
"""

    # 8. ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    user_prompt = f"""## ì‚¬ìš©ì ì§ˆë¬¸
{message}

## ì§ˆë¬¸ ìœ í˜•
{query_type.value if hasattr(query_type, 'value') else str(query_type)} (ì‹ ë¢°ë„: {confidence:.1%})

## ì¶”ì¶œëœ ì—”í‹°í‹°
- ë¸Œëœë“œ: {', '.join(entities.get('brands', [])) or 'ì—†ìŒ'}
- ì¹´í…Œê³ ë¦¬: {', '.join(entities.get('categories', [])) or 'ì—†ìŒ'}
- ì§€í‘œ: {', '.join(entities.get('indicators', [])) or 'ì—†ìŒ'}
- ê¸°ê°„: {entities.get('time_range') or 'ì—†ìŒ'}

## RAG ì°¸ì¡° ë¬¸ì„œ
{rag_context if rag_context else 'ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ'}

## í˜„ì¬ ë°ì´í„°
{data_context}

## ì´ì „ ëŒ€í™”
{conversation_history if conversation_history else 'ì´ì „ ëŒ€í™” ì—†ìŒ'}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
- ì§ˆë¬¸ ìœ í˜•ì— ë§ëŠ” ì‘ë‹µ ìŠ¤íƒ€ì¼ì„ ì‚¬ìš©í•˜ì„¸ìš”.
- RAG ë¬¸ì„œì— ê´€ë ¨ ì •ì˜/í•´ì„ì´ ìˆìœ¼ë©´ ì¸ìš©í•˜ì„¸ìš”.
- ì´ì „ ëŒ€í™” ë§¥ë½ì´ ìˆìœ¼ë©´ ê³ ë ¤í•˜ì„¸ìš”.
"""

    try:
        response = await acompletion(
            model="gpt-4.1-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.3,
            max_tokens=1000
        )

        answer = response.choices[0].message.content

        # 9. ëŒ€í™” ë©”ëª¨ë¦¬ì— ì €ì¥
        add_to_memory(session_id, "user", message)
        add_to_memory(session_id, "assistant", answer)

        # 10. ë™ì  í›„ì† ì§ˆë¬¸ ì œì•ˆ
        suggestions = get_dynamic_suggestions(query_type, entities, answer)

        # 11. Audit Trail ë¡œê¹…
        response_time_ms = (time.time() - start_time) * 1000
        log_chat_interaction(
            session_id=session_id,
            user_query=message,
            ai_response=answer,
            query_type=query_type.value if hasattr(query_type, 'value') else str(query_type),
            confidence=confidence,
            entities=entities,
            sources=sources,
            response_time_ms=response_time_ms
        )

        return ChatResponse(
            response=answer,
            query_type=query_type.value if hasattr(query_type, 'value') else str(query_type),
            confidence=confidence,
            sources=sources,
            suggestions=suggestions,
            entities=entities
        )

    except Exception as e:
        logger.error(f"LLM Error: {e}")

        # Fallback ì‘ë‹µ
        fallback = route_result.get("fallback_message") or rag_router.get_fallback_response("unknown")

        # ë°ì´í„° ê¸°ë°˜ ê¸°ë³¸ ì‘ë‹µ ì¶”ê°€
        if data and query_type == QueryType.DATA_QUERY:
            brand_kpis = data.get("brand", {}).get("kpis", {})
            fallback = f"""í˜„ì¬ LANEIGE í˜„í™©:
- SoS: {brand_kpis.get('sos', 0)}%
- Top 10 ì œí’ˆ: {brand_kpis.get('top10_count', 0)}ê°œ
- í‰ê·  ìˆœìœ„: {brand_kpis.get('avg_rank', 0)}ìœ„

(ìƒì„¸ ë¶„ì„ì„ ìœ„í•´ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”)"""

        # Fallback ì‘ë‹µë„ Audit Trail ê¸°ë¡
        response_time_ms = (time.time() - start_time) * 1000
        log_chat_interaction(
            session_id=session_id,
            user_query=message,
            ai_response=f"[ERROR] {str(e)[:100]} | Fallback: {fallback[:200]}",
            query_type=query_type.value if hasattr(query_type, 'value') else str(query_type),
            confidence=0.0,
            entities=entities,
            sources=["fallback"],
            response_time_ms=response_time_ms
        )

        return ChatResponse(
            response=fallback,
            query_type=query_type.value if hasattr(query_type, 'value') else str(query_type),
            confidence=0.0,
            sources=[],
            suggestions=["ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”", "SoSê°€ ë­”ê°€ìš”?", "í˜„ì¬ ìˆœìœ„ ì•Œë ¤ì£¼ì„¸ìš”"],
            entities=entities
        )


@app.delete("/api/chat/memory/{session_id}")
async def clear_memory(session_id: str):
    """ì„¸ì…˜ ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"""
    if session_id in conversation_memory:
        del conversation_memory[session_id]
    return {"status": "ok", "message": f"Session {session_id} memory cleared"}


# ============= Simple Chat API (v3 - ë‹¨ìˆœí™”) =============

from src.core.simple_chat import get_chat_service


class SimpleChatRequest(BaseModel):
    """Simple Chat ìš”ì²­"""
    message: str
    session_id: Optional[str] = "default"


class SimpleChatResponse(BaseModel):
    """Simple Chat ì‘ë‹µ"""
    text: str
    suggestions: List[str]
    tools_used: List[str]
    sources: List[Dict[str, Any]] = []  # AI ì¶œì²˜ ì •ë³´ ì¶”ê°€
    data_date: str
    processing_time_ms: float


@app.post("/api/v3/chat", response_model=SimpleChatResponse)
@limiter.limit("30/minute")  # ë¶„ë‹¹ 30íšŒ ì œí•œ
async def chat_v3(request: Request, body: SimpleChatRequest):
    """
    Simple LLM Chat API (v3)

    ë‹¨ìˆœí™”ëœ êµ¬ì¡°:
    - LLMì´ ëª¨ë“  íŒë‹¨ ë‹´ë‹¹
    - Function Callingìœ¼ë¡œ ë„êµ¬ ì‚¬ìš©
    - ë¶ˆí•„ìš”í•œ ë ˆì´ì–´ ì œê±°
    """
    message = body.message.strip()
    session_id = body.session_id or "default"

    if not message:
        raise HTTPException(status_code=400, detail="Message is required")

    # í¬ë¡¤ë§ ìƒíƒœ ì²´í¬
    crawl_manager = await get_crawl_manager()
    crawl_notification = None
    crawl_started = False

    if crawl_manager.needs_crawl():
        crawl_started = await crawl_manager.start_crawl()

    if crawl_manager.should_notify(session_id):
        crawl_notification = crawl_manager.get_notification_message()
        crawl_manager.mark_notified(session_id)

    # Simple Chat Serviceë¡œ ì²˜ë¦¬
    chat_service = get_chat_service()
    result = await chat_service.chat(message, session_id)

    # í¬ë¡¤ë§ ì•Œë¦¼ ì¶”ê°€
    response_text = result["text"]
    if crawl_notification:
        response_text = f"{crawl_notification}\n\n---\n\n{response_text}"
    elif crawl_started:
        data_date = crawl_manager.get_data_date() or "ì—†ìŒ"
        response_text = (
            f"ğŸ“¡ **ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì˜¤ëŠ˜ ë°ì´í„° ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤.**\n"
            f"í˜„ì¬ ë°ì´í„°: {data_date}\n"
            f"ìˆ˜ì§‘ì´ ì™„ë£Œë˜ë©´ ì•Œë ¤ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n\n---\n\n{response_text}"
        )

    return SimpleChatResponse(
        text=response_text,
        suggestions=result.get("suggestions", []),
        tools_used=result.get("tools_used", []),
        sources=result.get("sources", []),  # AI ì¶œì²˜ ì •ë³´ ì „ë‹¬
        data_date=result.get("data_date", "N/A"),
        processing_time_ms=result.get("processing_time_ms", 0)
    )


# ============= LLM Orchestrator API (v2 - ê¸°ì¡´, deprecated) =============

class OrchestratorChatRequest(BaseModel):
    """LLM Orchestrator ì±—ë´‡ ìš”ì²­"""
    message: str
    session_id: Optional[str] = "default"
    skip_cache: bool = False


class OrchestratorChatResponse(BaseModel):
    """LLM Orchestrator ì±—ë´‡ ì‘ë‹µ"""
    text: str
    query_type: str
    confidence_level: str
    confidence_score: float
    sources: List[str]
    entities: Dict[str, Any]
    tools_called: List[str]
    suggestions: List[str]
    is_fallback: bool
    is_clarification: bool
    processing_time_ms: float


@app.post("/api/v2/chat", response_model=OrchestratorChatResponse)
@limiter.limit("30/minute")  # ë¶„ë‹¹ 30íšŒ ì œí•œ
async def chat_v2(request: Request, body: OrchestratorChatRequest):
    """
    í†µí•© ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ê¸°ë°˜ ì±—ë´‡ API (v2)

    ë™ì‘ íë¦„:
    1. ì§ˆë¬¸ ìˆ˜ì‹ 
    2. ì‹œìŠ¤í…œ ìƒíƒœ ì ê²€ (ë°ì´í„° ì‹ ì„ ë„, ì‚¬ìš© ê°€ëŠ¥ ì—ì´ì „íŠ¸)
    3. LLMì´ ìƒí™© íŒë‹¨ â†’ ì—ì´ì „íŠ¸ ì„ íƒ
    4. ì—ì´ì „íŠ¸ ì‹¤í–‰ (ì—ëŸ¬ ì‹œ ì „ëµì— ë”°ë¼ ì²˜ë¦¬)
    5. ì‘ë‹µ ìƒì„±

    ì—ëŸ¬ ì „ëµ:
    - RETRY: ì¬ì‹œë„ (ìµœëŒ€ 2íšŒ)
    - FALLBACK: ìºì‹œ ë°ì´í„° ì‚¬ìš©
    - SKIP: ê±´ë„ˆë›°ê³  ê³„ì†
    - ABORT: ì¤‘ë‹¨ + ì‚¬ìš©ì ì•Œë¦¼
    """
    import time
    start_time = time.time()

    message = body.message.strip()
    session_id = body.session_id or "default"

    if not message:
        raise HTTPException(status_code=400, detail="Message is required")

    # === í¬ë¡¤ë§ ìƒíƒœ ì²´í¬ ===
    crawl_manager = await get_crawl_manager()
    crawl_notification = None
    crawl_started = False

    # ì˜¤ëŠ˜ ë°ì´í„°ê°€ ì—†ê³ , í¬ë¡¤ë§ ì¤‘ì´ ì•„ë‹ˆë©´ ì‹œì‘
    if crawl_manager.needs_crawl():
        crawl_started = await crawl_manager.start_crawl()
        if crawl_started:
            logging.info("Background crawl started for today's data")

    # í¬ë¡¤ë§ ì™„ë£Œ ì•Œë¦¼ ì²´í¬ (ì´ ì„¸ì…˜ì—ì„œ ì•„ì§ ì•ˆ ì•Œë ¸ìœ¼ë©´)
    if crawl_manager.should_notify(session_id):
        crawl_notification = crawl_manager.get_notification_message()
        crawl_manager.mark_notified(session_id)

    try:
        # UnifiedBrainìœ¼ë¡œ ì²˜ë¦¬
        brain = get_brain()

        # í˜„ì¬ ë©”íŠ¸ë¦­ ë°ì´í„° ë¡œë“œ
        data = load_dashboard_data()
        current_metrics = data if data else None

        # ì²˜ë¦¬
        response = await brain.process_query(
            query=message,
            session_id=session_id,
            current_metrics=current_metrics,
            skip_cache=body.skip_cache
        )

        # ì‘ë‹µ ë³€í™˜ (UnifiedBrain response ì²˜ë¦¬)
        response_dict = response.to_dict() if hasattr(response, 'to_dict') else response

        # ì‘ë‹µ í…ìŠ¤íŠ¸ êµ¬ì„±
        response_text = response_dict.get('text', response_dict.get('content', ''))

        # í¬ë¡¤ë§ ì•Œë¦¼ ì¶”ê°€
        if crawl_notification:
            response_text = f"{crawl_notification}\n\n---\n\n{response_text}"
        elif crawl_started:
            # í¬ë¡¤ë§ ì‹œì‘ ì•Œë¦¼
            data_date = crawl_manager.get_data_date() or "ì—†ìŒ"
            response_text = (
                f"ğŸ“¡ **ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì˜¤ëŠ˜ ë°ì´í„° ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤.**\n"
                f"í˜„ì¬ ë°ì´í„°: {data_date}\n"
                f"ìˆ˜ì§‘ì´ ì™„ë£Œë˜ë©´ ì•Œë ¤ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n\n---\n\n{response_text}"
            )

        # ì‘ë‹µ ë³€í™˜
        return OrchestratorChatResponse(
            text=response_text,
            query_type=response_dict.get('query_type', 'unknown'),
            confidence_level=response_dict.get('confidence_level', 'medium'),
            confidence_score=response_dict.get('confidence_score', response_dict.get('confidence', 0.5)),
            sources=response_dict.get('sources', []),
            entities=response_dict.get('entities', {}),
            tools_called=response_dict.get('tools_called', response_dict.get('tools_used', [])),
            suggestions=response_dict.get('suggestions', []),
            is_fallback=response_dict.get('is_fallback', False),
            is_clarification=response_dict.get('is_clarification', False),
            processing_time_ms=response_dict.get('processing_time_ms', 0)
        )

    except Exception as e:
        logging.error(f"Orchestrator error: {e}")
        return OrchestratorChatResponse(
            text=f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            query_type="error",
            confidence_level="unknown",
            confidence_score=0.0,
            sources=[],
            entities={},
            tools_called=[],
            suggestions=["ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”"],
            is_fallback=True,
            is_clarification=False,
            processing_time_ms=(time.time() - start_time) * 1000
        )


@app.get("/api/v2/stats")
async def get_orchestrator_stats():
    """UnifiedBrain í†µê³„ ì¡°íšŒ"""
    brain = get_brain()
    return brain.get_stats() if hasattr(brain, 'get_stats') else {"status": "ok"}


@app.get("/api/v2/state")
async def get_orchestrator_state():
    """UnifiedBrain ìƒíƒœ ì¡°íšŒ"""
    brain = get_brain()
    return {
        "summary": brain.get_state_summary() if hasattr(brain, 'get_state_summary') else {},
        "state": brain.state.to_dict() if hasattr(brain, 'state') and hasattr(brain.state, 'to_dict') else {}
    }


@app.get("/api/v2/errors")
async def get_orchestrator_errors():
    """UnifiedBrain ìµœê·¼ ì—ëŸ¬ ì¡°íšŒ"""
    brain = get_brain()
    return {
        "recent_errors": brain.get_recent_errors(limit=20) if hasattr(brain, 'get_recent_errors') else [],
        "stats": brain.get_stats() if hasattr(brain, 'get_stats') else {}
    }


@app.post("/api/v2/reset-errors")
async def reset_orchestrator_errors():
    """ì‹¤íŒ¨í•œ ì—ì´ì „íŠ¸ ëª©ë¡ ì´ˆê¸°í™”"""
    brain = get_brain()
    if hasattr(brain, 'reset_failed_agents'):
        brain.reset_failed_agents()
    return {"status": "ok", "message": "Failed agents list cleared"}


@app.get("/api/crawl/status")
async def get_crawl_status():
    """
    í¬ë¡¤ë§ ìƒíƒœ ì¡°íšŒ

    Returns:
        - status: idle/running/completed/failed
        - date: í¬ë¡¤ë§ ëŒ€ìƒ ë‚ ì§œ
        - progress: ì§„í–‰ë¥  (0-100)
        - data_date: í˜„ì¬ ë°ì´í„° ë‚ ì§œ
        - needs_crawl: í¬ë¡¤ë§ í•„ìš” ì—¬ë¶€
    """
    crawl_manager = await get_crawl_manager()
    return {
        **crawl_manager.state.to_dict(),
        "data_date": crawl_manager.get_data_date(),
        "needs_crawl": crawl_manager.needs_crawl(),
        "is_today_available": crawl_manager.is_today_data_available(),
        "status_message": crawl_manager.get_status_message()
    }


@app.post("/api/crawl/start", dependencies=[Depends(verify_api_key)])
async def start_crawl():
    """
    ìˆ˜ë™ìœ¼ë¡œ í¬ë¡¤ë§ ì‹œì‘ (API Key í•„ìš”)

    Returns:
        - started: í¬ë¡¤ë§ ì‹œì‘ ì—¬ë¶€
        - message: ìƒíƒœ ë©”ì‹œì§€
    """
    crawl_manager = await get_crawl_manager()

    if crawl_manager.is_crawling():
        return {
            "started": False,
            "message": "í¬ë¡¤ë§ì´ ì´ë¯¸ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤.",
            "status": crawl_manager.state.to_dict()
        }

    if crawl_manager.is_today_data_available():
        return {
            "started": False,
            "message": "ì˜¤ëŠ˜ ë°ì´í„°ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.",
            "status": crawl_manager.state.to_dict()
        }

    started = await crawl_manager.start_crawl()
    return {
        "started": started,
        "message": "í¬ë¡¤ë§ì„ ì‹œì‘í–ˆìŠµë‹ˆë‹¤." if started else "í¬ë¡¤ë§ ì‹œì‘ ì‹¤íŒ¨",
        "status": crawl_manager.state.to_dict()
    }


# ============= Historical Data API =============

from src.tools.sheets_writer import SheetsWriter
from datetime import timedelta

# SheetsWriter ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_sheets_writer: Optional[SheetsWriter] = None


def get_sheets_writer() -> SheetsWriter:
    """SheetsWriter ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _sheets_writer
    if _sheets_writer is None:
        _sheets_writer = SheetsWriter()
    return _sheets_writer


@app.get("/api/historical")
async def get_historical_data(
    start_date: str,
    end_date: str,
    category_id: Optional[str] = None,
    brand: Optional[str] = "LANEIGE"
):
    """
    íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„° ì¡°íšŒ (SQLite ìš°ì„ , Google Sheets fallback)

    Args:
        start_date: ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)
        end_date: ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)
        category_id: ì¹´í…Œê³ ë¦¬ í•„í„° (ì„ íƒ)
        brand: ë¸Œëœë“œ í•„í„° (ê¸°ë³¸ê°’: LANEIGE)

    Returns:
        - data: ë‚ ì§œë³„ ì§€í‘œ ë°ì´í„°
        - sos_history: SoS ì¶”ì´ ë°ì´í„°
        - raw_data: ìˆœìœ„ ì¶”ì´ ë°ì´í„°
    """
    try:
        records = []
        data_source = None

        # 1ì°¨: SQLiteì—ì„œ ì¡°íšŒ (ë¹ ë¦„)
        try:
            sqlite = get_sqlite_storage()
            await sqlite.initialize()
            records = await sqlite.get_raw_data(
                start_date=start_date,
                end_date=end_date,
                category_id=category_id,
                limit=50000  # ì¶©ë¶„íˆ í° limit
            )
            if records:
                data_source = "sqlite"
                logging.info(f"Historical: loaded {len(records)} records from SQLite ({start_date} ~ {end_date})")
        except Exception as sqlite_err:
            logging.warning(f"Historical: SQLite ì¡°íšŒ ì‹¤íŒ¨: {sqlite_err}")

        # 2ì°¨: SQLite ì‹¤íŒ¨/ë¹ˆ ê²°ê³¼ ì‹œ Google Sheets fallback
        if not records:
            try:
                sheets_writer = get_sheets_writer()
                if not sheets_writer._initialized:
                    await sheets_writer.initialize()
                records = await sheets_writer.get_raw_data(
                    start_date=start_date,
                    end_date=end_date,
                    category_id=category_id
                )
                if records:
                    data_source = "sheets"
                    logging.info(f"Historical: loaded {len(records)} records from Sheets ({start_date} ~ {end_date})")
            except Exception as sheets_err:
                logging.warning(f"Historical: Google Sheets ì¡°íšŒ ì‹¤íŒ¨: {sheets_err}")

        if not records:
            # ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ë°ì´í„° ì—†ìŒ - ë¡œì»¬ JSON íŒŒì¼ì—ì„œ ì‹œë„
            return await _get_historical_from_local(start_date, end_date, brand)

        # ë‚ ì§œ ë²”ìœ„ ê³„ì‚°
        start_dt = datetime.strptime(start_date, "%Y-%m-%d")
        end_dt = datetime.strptime(end_date, "%Y-%m-%d")
        days = (end_dt - start_dt).days + 1

        # ë‚ ì§œë³„ ë°ì´í„° ì§‘ê³„ (íŠ¹ì • ë¸Œëœë“œ í•„í„°ë§)
        daily_data = {}
        brand_lower = brand.lower() if brand else ""
        for record in records:
            snapshot_date = record.get("snapshot_date", "")
            if not snapshot_date or snapshot_date < start_date or snapshot_date > end_date:
                continue

            # íŠ¹ì • ë¸Œëœë“œ í•„í„°ë§ (SoS ì¶”ì´ ê³„ì‚°ìš©)
            record_brand = record.get("brand", "")
            if brand_lower and record_brand.lower() != brand_lower:
                continue

            if snapshot_date not in daily_data:
                daily_data[snapshot_date] = {
                    "date": snapshot_date,
                    "products": [],
                    "total_count": 0,
                    "top10_count": 0
                }

            rank = int(record.get("rank", 0)) if record.get("rank") else 0
            daily_data[snapshot_date]["products"].append({
                "asin": record.get("asin", ""),
                "product_name": record.get("product_name", ""),
                "brand": record_brand,
                "rank": rank,
                "price": record.get("price", ""),
                "rating": record.get("rating", "")
            })
            daily_data[snapshot_date]["total_count"] += 1
            if rank <= 10:
                daily_data[snapshot_date]["top10_count"] += 1

        # SoS ì¶”ì´ ê³„ì‚° (Top 100 ê¸°ì¤€, í•´ë‹¹ ë¸Œëœë“œ ê¸°ì¤€)
        sos_history = []
        raw_data = []
        for date_str in sorted(daily_data.keys()):
            day_data = daily_data[date_str]
            products = day_data["products"]

            # SoS = (ë¸Œëœë“œ ì œí’ˆ ìˆ˜ / 100) * 100
            sos = round(len(products) / 100 * 100, 1) if products else 0
            sos_history.append({
                "date": date_str,
                "sos": sos,
                "product_count": len(products),
                "top10_count": day_data["top10_count"]
            })

            # í‰ê·  ìˆœìœ„ (ìˆëŠ” ê²½ìš°)
            if products:
                avg_rank = round(sum(p["rank"] for p in products) / len(products), 1)
                raw_data.append({
                    "date": date_str,
                    "rank": avg_rank,
                    "best_rank": min(p["rank"] for p in products),
                    "worst_rank": max(p["rank"] for p in products)
                })

        # available_dates ê³„ì‚°
        available_dates = sorted(daily_data.keys())

        # brand_metrics ê³„ì‚° (ì „ì²´ ê¸°ê°„ í†µí•© - ëª¨ë“  ë¸Œëœë“œ í¬í•¨)
        brand_metrics = await _calculate_brand_metrics_for_period(records, daily_data, brand)

        # rank_history ìƒì„± (Product View ì°¨íŠ¸ìš©)
        # í˜•ì‹: { "2026-01-14": { "products": [{ "name": "...", "rank": 5, "price": 21.5 }, ...] } }
        rank_history = {}
        for record in records:
            snapshot_date = record.get("snapshot_date", "")
            if not snapshot_date or snapshot_date < start_date or snapshot_date > end_date:
                continue

            if snapshot_date not in rank_history:
                rank_history[snapshot_date] = {"products": []}

            rank = int(record.get("rank", 0)) if record.get("rank") else 0
            price_val = record.get("price", 0)
            try:
                price = float(str(price_val).replace("$", "").replace(",", "")) if price_val else 0
            except (ValueError, TypeError):
                price = 0

            rank_history[snapshot_date]["products"].append({
                "name": record.get("product_name", ""),
                "product_name": record.get("product_name", ""),
                "brand": record.get("brand", ""),
                "asin": record.get("asin", ""),
                "rank": rank,
                "price": price,
                "rating": record.get("rating", ""),
                "discount_percent": record.get("discount_percent", 0)
            })

        # ì „ì²´ ë°ì´í„°ì˜ ì‚¬ìš© ê°€ëŠ¥í•œ ë‚ ì§œ ë²”ìœ„ ì¡°íšŒ (SQLiteì—ì„œ)
        available_date_range = {"min": None, "max": None}
        try:
            sqlite = get_sqlite_storage()
            stats = sqlite.get_stats()
            if "date_range" in stats:
                available_date_range = stats["date_range"]
        except Exception:
            pass

        return {
            "success": True,
            "available_dates": available_dates,
            "available_date_range": available_date_range,
            "data_source": data_source,
            "brand_metrics": brand_metrics,
            "rank_history": rank_history,
            "data": {
                "sos_history": sos_history,
                "raw_data": raw_data,
                "daily_data": list(daily_data.values()),
                "period": {
                    "start": start_date,
                    "end": end_date,
                    "days": days
                },
                "brand": brand
            }
        }

    except Exception as e:
        logging.error(f"Historical data error: {e}")
        # í´ë°±: ë¡œì»¬ ë°ì´í„°ì—ì„œ ì‹œë„
        return await _get_historical_from_local(start_date, end_date, brand)


async def _calculate_brand_metrics_for_period(
    records: List[Dict],
    daily_data: Dict,
    target_brand: str
) -> List[Dict]:
    """
    ê¸°ê°„ ë‚´ ëª¨ë“  ë¸Œëœë“œì˜ ë©”íŠ¸ë¦­ ê³„ì‚° (SoS Ã— Avg Rank ì°¨íŠ¸ìš©)

    Returns:
        ë¸Œëœë“œë³„ SoS, í‰ê·  ìˆœìœ„, ì œí’ˆ ìˆ˜ ë“±
    """
    # ì „ì²´ ì œí’ˆ ë°ì´í„° ì§‘ê³„ (ëª¨ë“  ë¸Œëœë“œ)
    brand_data = {}

    for record in records:
        brand_name = record.get("brand", "Unknown")
        rank = int(record.get("rank", 0)) if record.get("rank") else 0

        if not brand_name or rank == 0:
            continue

        if brand_name not in brand_data:
            brand_data[brand_name] = {
                "brand": brand_name,
                "ranks": [],
                "product_count": 0
            }

        brand_data[brand_name]["ranks"].append(rank)
        brand_data[brand_name]["product_count"] += 1

    # ì´ ì œí’ˆ ìˆ˜ (ëª¨ë“  ë¸Œëœë“œ)
    total_products = sum(b["product_count"] for b in brand_data.values())

    # ë©”íŠ¸ë¦­ ê³„ì‚°
    brand_metrics = []
    for brand_name, data in brand_data.items():
        if not data["ranks"]:
            continue

        sos = round(data["product_count"] / max(total_products, 100) * 100, 2)
        avg_rank = round(sum(data["ranks"]) / len(data["ranks"]), 1)

        # ë²„ë¸” í¬ê¸°: ì œí’ˆ ìˆ˜ ê¸°ë°˜ (ìµœì†Œ 5, ìµœëŒ€ 25)
        bubble_size = max(5, min(25, data["product_count"] * 2))

        brand_metrics.append({
            "brand": brand_name,
            "sos": sos,
            "avg_rank": avg_rank,
            "product_count": data["product_count"],
            "bubble_size": bubble_size,
            "is_laneige": target_brand.upper() in brand_name.upper()
        })

    # SoS ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬, ìƒìœ„ 10ê°œë§Œ
    brand_metrics.sort(key=lambda x: x["sos"], reverse=True)
    return brand_metrics[:10]


def _get_brand_metrics_from_dashboard(
    dashboard_data: Optional[Dict],
    target_brand: str
) -> List[Dict]:
    """
    ëŒ€ì‹œë³´ë“œ ë°ì´í„°ì—ì„œ ë¸Œëœë“œ ë©”íŠ¸ë¦­ ì¶”ì¶œ (ë¡œì»¬ í´ë°±ìš©)
    """
    if not dashboard_data:
        return []

    # ëŒ€ì‹œë³´ë“œì˜ brand_matrix ë°ì´í„° ì‚¬ìš©
    brand_matrix = dashboard_data.get("charts", {}).get("brand_matrix", [])
    if brand_matrix:
        return brand_matrix

    # ê²½ìŸì‚¬ ë°ì´í„°ì—ì„œ ìƒì„±
    competitors = dashboard_data.get("brand", {}).get("competitors", [])
    if not competitors:
        return []

    brand_metrics = []
    for comp in competitors:
        brand_metrics.append({
            "brand": comp.get("brand", "Unknown"),
            "sos": comp.get("sos", 0),
            "avg_rank": comp.get("avg_rank", 50),
            "product_count": comp.get("product_count", 0),
            "bubble_size": max(5, min(25, comp.get("product_count", 0) * 2)),
            "is_laneige": target_brand.upper() in comp.get("brand", "").upper()
        })

    return brand_metrics


async def _get_historical_from_local(
    start_date: str,
    end_date: str,
    brand: str = "LANEIGE"
) -> Dict[str, Any]:
    """
    ë¡œì»¬ JSON íŒŒì¼ì—ì„œ íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„° ì¡°íšŒ (í´ë°±)

    data/ í´ë”ì˜ ë‚ ì§œë³„ JSON íŒŒì¼ì´ë‚˜ dashboard_data.jsonì˜ íˆìŠ¤í† ë¦¬ ë°ì´í„° í™œìš©
    """
    try:
        # ë©”ì¸ ëŒ€ì‹œë³´ë“œ ë°ì´í„° ë¡œë“œ
        data = load_dashboard_data()
        sos_history = []
        raw_data = []

        # 1. ëŒ€ì‹œë³´ë“œ ë°ì´í„°ì—ì„œ í˜„ì¬ SoS/ìˆœìœ„ ì •ë³´ ì¶”ì¶œ
        if data:
            brand_kpis = data.get("brand", {}).get("kpis", {})
            current_sos = brand_kpis.get("sos", 0)
            data_date = data.get("metadata", {}).get("data_date", datetime.now().strftime("%Y-%m-%d"))

            # í˜„ì¬ ë‚ ì§œê°€ ìš”ì²­ ë²”ìœ„ì— í¬í•¨ë˜ë©´ ì¶”ê°€
            if start_date <= data_date <= end_date:
                sos_history.append({
                    "date": data_date,
                    "sos": current_sos,
                    "product_count": brand_kpis.get("product_count", 0),
                    "top10_count": brand_kpis.get("top10_count", 0)
                })

                avg_rank = brand_kpis.get("avg_rank", 0)
                if avg_rank:
                    raw_data.append({
                        "date": data_date,
                        "rank": avg_rank,
                        "best_rank": brand_kpis.get("best_rank", avg_rank),
                        "worst_rank": brand_kpis.get("worst_rank", avg_rank)
                    })

        # 2. latest_crawl_result.jsonì—ì„œ ë°ì´í„° ì¶”ì¶œ
        latest_crawl_path = Path("./data/latest_crawl_result.json")
        if latest_crawl_path.exists():
            try:
                with open(latest_crawl_path, "r", encoding="utf-8") as f:
                    crawl_data = json.load(f)

                # ëª¨ë“  ì¹´í…Œê³ ë¦¬ì—ì„œ ë¸Œëœë“œ ì œí’ˆ ì°¾ê¸°
                brand_products = []
                crawl_date = None

                for cat_id, cat_data in crawl_data.get("categories", {}).items():
                    for product in cat_data.get("products", []):
                        product_brand = product.get("brand", "")
                        product_name = product.get("product_name", "")

                        # ë¸Œëœë“œ ë§¤ì¹­ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ, ë¶€ë¶„ ë§¤ì¹­)
                        if brand.upper() in product_brand.upper() or brand.upper() in product_name.upper():
                            brand_products.append(product)
                            if not crawl_date:
                                crawl_date = product.get("snapshot_date")

                if brand_products and crawl_date and start_date <= crawl_date <= end_date:
                    # ì¤‘ë³µ ì œê±° í™•ì¸
                    if not any(h["date"] == crawl_date for h in sos_history):
                        # ì¹´í…Œê³ ë¦¬ë³„ ì´ ì œí’ˆ ìˆ˜ (Top 100 ê¸°ì¤€)
                        total_products = sum(
                            len(cat.get("products", []))
                            for cat in crawl_data.get("categories", {}).values()
                        )

                        sos = round(len(brand_products) / max(total_products, 100) * 100, 2)
                        avg_rank = round(sum(p.get("rank", 0) for p in brand_products) / len(brand_products), 1)

                        sos_history.append({
                            "date": crawl_date,
                            "sos": sos,
                            "product_count": len(brand_products),
                            "top10_count": sum(1 for p in brand_products if p.get("rank", 100) <= 10)
                        })
                        raw_data.append({
                            "date": crawl_date,
                            "rank": avg_rank,
                            "best_rank": min(p.get("rank", 100) for p in brand_products),
                            "worst_rank": max(p.get("rank", 100) for p in brand_products)
                        })

            except (json.JSONDecodeError, ValueError) as e:
                logging.warning(f"Failed to parse latest_crawl_result.json: {e}")

        # 3. raw_products í´ë”ì—ì„œ ë‚ ì§œë³„ ë°ì´í„° ê²€ìƒ‰ (ê¸°ì¡´ ë¡œì§)
        raw_data_dir = Path("./data/raw_products")
        if raw_data_dir.exists():
            for json_file in raw_data_dir.glob("*.json"):
                try:
                    file_date = json_file.stem  # íŒŒì¼ëª…ì´ YYYY-MM-DD í˜•ì‹ì´ë¼ê³  ê°€ì •
                    if start_date <= file_date <= end_date:
                        with open(json_file, "r", encoding="utf-8") as f:
                            daily_raw = json.load(f)

                        # ë¸Œëœë“œ ì œí’ˆë§Œ í•„í„°ë§
                        brand_products = [
                            p for p in daily_raw
                            if brand.upper() in p.get("brand", "").upper() or brand.upper() in p.get("product_name", "").upper()
                        ]

                        if brand_products:
                            sos = round(len(brand_products) / 100 * 100, 1)
                            avg_rank = round(sum(p.get("rank", 0) for p in brand_products) / len(brand_products), 1)

                            # ì¤‘ë³µ ì œê±°
                            if not any(h["date"] == file_date for h in sos_history):
                                sos_history.append({
                                    "date": file_date,
                                    "sos": sos,
                                    "product_count": len(brand_products),
                                    "top10_count": sum(1 for p in brand_products if p.get("rank", 100) <= 10)
                                })
                                raw_data.append({
                                    "date": file_date,
                                    "rank": avg_rank,
                                    "best_rank": min(p.get("rank", 100) for p in brand_products),
                                    "worst_rank": max(p.get("rank", 100) for p in brand_products)
                                })
                except (json.JSONDecodeError, ValueError):
                    continue

        # ë‚ ì§œìˆœ ì •ë ¬
        sos_history.sort(key=lambda x: x["date"])
        raw_data.sort(key=lambda x: x["date"])

        # available_dates ê³„ì‚°
        available_dates = [h["date"] for h in sos_history]

        # brand_metrics ê³„ì‚° (í˜„ì¬ ëŒ€ì‹œë³´ë“œ ë°ì´í„°ì—ì„œ)
        brand_metrics = _get_brand_metrics_from_dashboard(data, brand)

        if not sos_history:
            return {
                "success": False,
                "error": "No historical data found for the specified period",
                "available_dates": [],
                "brand_metrics": [],
                "data": None
            }

        return {
            "success": True,
            "available_dates": available_dates,
            "brand_metrics": brand_metrics,
            "data": {
                "sos_history": sos_history,
                "raw_data": raw_data,
                "period": {
                    "start": start_date,
                    "end": end_date
                },
                "brand": brand,
                "source": "local"
            }
        }

    except Exception as e:
        logging.error(f"Local historical data error: {e}")
        return {
            "success": False,
            "error": str(e),
            "available_dates": [],
            "brand_metrics": [],
            "data": None
        }


@app.post("/api/export/docx")
async def export_docx(request: ExportRequest):
    """
    ì¸ì‚¬ì´íŠ¸ ë¦¬í¬íŠ¸ DOCX ìƒì„± ë° ë‹¤ìš´ë¡œë“œ
    """
    data = load_dashboard_data()
    if not data:
        raise HTTPException(status_code=404, detail="Dashboard data not found")

    # DOCX ë¬¸ì„œ ìƒì„±
    doc = Document()

    # ìŠ¤íƒ€ì¼ ì„¤ì •
    style = doc.styles['Normal']
    font = style.font
    font.name = 'Arial'
    font.size = Pt(11)

    # ===== í‘œì§€ =====
    title = doc.add_heading('AMORE INSIGHT Report', 0)
    title.alignment = WD_ALIGN_PARAGRAPH.CENTER

    subtitle = doc.add_paragraph('LANEIGE Amazon US ë¶„ì„ ë¦¬í¬íŠ¸')
    subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER

    # ë‚ ì§œ
    metadata = data.get("metadata", {})
    date_para = doc.add_paragraph()
    date_para.alignment = WD_ALIGN_PARAGRAPH.CENTER
    date_para.add_run(f"ë¶„ì„ ê¸°ì¤€ì¼: {metadata.get('data_date', datetime.now().strftime('%Y-%m-%d'))}")
    date_para.add_run(f"\nìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M')}")

    doc.add_page_break()

    # ===== 1. Executive Summary =====
    doc.add_heading('1. Executive Summary', level=1)

    brand_kpis = data.get("brand", {}).get("kpis", {})
    home_status = data.get("home", {}).get("status", {})

    summary_text = f"""
LANEIGE ë¸Œëœë“œëŠ” Amazon US ì‹œì¥ì—ì„œ {home_status.get('exposure', 'N/A')} ìƒíƒœì…ë‹ˆë‹¤.

â€¢ Share of Shelf (SoS): {brand_kpis.get('sos', 0)}%
â€¢ Top 10 ì§„ì… ì œí’ˆ: {brand_kpis.get('top10_count', 0)}ê°œ
â€¢ í‰ê·  ìˆœìœ„: {brand_kpis.get('avg_rank', 0)}ìœ„
â€¢ ì‹œì¥ ì§‘ì¤‘ë„ (HHI): {brand_kpis.get('hhi', 0)}

í˜„ì¬ ì‹œì¥ í¬ì§€ì…˜: {home_status.get('position', 'N/A')}
ì£¼ì˜ í•„ìš” ì œí’ˆ: {home_status.get('warning_count', 0)}ê°œ
"""
    doc.add_paragraph(summary_text)

    # ===== 2. ì œí’ˆë³„ í˜„í™© =====
    doc.add_heading('2. LANEIGE ì œí’ˆ í˜„í™©', level=1)

    products = data.get("products", {})
    if products:
        # í…Œì´ë¸” ìƒì„±
        table = doc.add_table(rows=1, cols=5)
        table.style = 'Table Grid'
        table.alignment = WD_TABLE_ALIGNMENT.CENTER

        # í—¤ë”
        header_cells = table.rows[0].cells
        headers = ['ì œí’ˆëª…', 'ìˆœìœ„', 'ë³€ë™', 'í‰ì ', 'ë³€ë™ì„±']
        for i, header in enumerate(headers):
            header_cells[i].text = header
            header_cells[i].paragraphs[0].runs[0].bold = True

        # ë°ì´í„° í–‰
        for asin, product in products.items():
            row = table.add_row().cells
            row[0].text = product.get('name', '')[:40]
            row[1].text = f"#{product.get('rank', 'N/A')}"
            row[2].text = product.get('rank_delta', '-')
            row[3].text = str(product.get('rating', '-'))
            row[4].text = product.get('volatility_status', '-')

    doc.add_paragraph()

    # ===== 3. ê²½ìŸì‚¬ ë¶„ì„ =====
    doc.add_heading('3. ê²½ìŸì‚¬ ë¶„ì„', level=1)

    competitors = data.get("brand", {}).get("competitors", [])
    if competitors:
        table = doc.add_table(rows=1, cols=4)
        table.style = 'Table Grid'

        header_cells = table.rows[0].cells
        headers = ['ë¸Œëœë“œ', 'SoS (%)', 'í‰ê·  ìˆœìœ„', 'ì œí’ˆ ìˆ˜']
        for i, header in enumerate(headers):
            header_cells[i].text = header
            header_cells[i].paragraphs[0].runs[0].bold = True

        for comp in competitors[:10]:
            row = table.add_row().cells
            row[0].text = comp.get('brand', '')
            row[1].text = str(comp.get('sos', 0))
            row[2].text = str(comp.get('avg_rank', '-'))
            row[3].text = str(comp.get('product_count', 0))

    doc.add_paragraph()

    # ===== 4. ì•¡ì…˜ ì•„ì´í…œ =====
    doc.add_heading('4. ì•¡ì…˜ ì•„ì´í…œ', level=1)

    action_items = data.get("home", {}).get("action_items", [])
    if action_items:
        for item in action_items:
            priority_marker = "ğŸ”´" if item.get('priority') == 'P1' else "ğŸŸ "
            para = doc.add_paragraph()
            para.add_run(f"{priority_marker} [{item.get('priority')}] ").bold = True
            para.add_run(f"{item.get('product_name', '')}\n")
            para.add_run(f"   ì‹ í˜¸: {item.get('signal', '')}\n")
            para.add_run(f"   ê¶Œì¥ ì•¡ì…˜: {item.get('action_tag', '')}")
    else:
        doc.add_paragraph("í˜„ì¬ íŠ¹ë³„í•œ ì•¡ì…˜ ì•„ì´í…œì´ ì—†ìŠµë‹ˆë‹¤.")

    # ===== 5. ì „ëµì  ê¶Œê³ ì‚¬í•­ =====
    if request.include_strategy:
        doc.add_heading('5. ì „ëµì  ê¶Œê³ ì‚¬í•­', level=1)

        # ChatGPTë¡œ ì „ëµ ìƒì„± (RAG ì»¨í…ìŠ¤íŠ¸ í™œìš©)
        try:
            # RAGì—ì„œ ì „ëµ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
            strategy_context, _ = await get_rag_context("ì „ëµ ì•¡ì…˜ ê¶Œê³ ", QueryType.ANALYSIS)

            strategy_prompt = f"""ë‹¤ìŒ ë°ì´í„°ì™€ ê°€ì´ë“œë¼ì¸ì„ ë°”íƒ•ìœ¼ë¡œ LANEIGE ë¸Œëœë“œì˜ ì „ëµì  ê¶Œê³ ì‚¬í•­ 3ê°€ì§€ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

ë°ì´í„°:
- SoS: {brand_kpis.get('sos', 0)}%
- Top 10 ì œí’ˆ: {brand_kpis.get('top10_count', 0)}ê°œ
- í‰ê·  ìˆœìœ„: {brand_kpis.get('avg_rank', 0)}ìœ„
- ì£¼ìš” ê²½ìŸì‚¬: {', '.join([c['brand'] for c in competitors[:3]])}

ì°¸ê³  ê°€ì´ë“œë¼ì¸:
{strategy_context if strategy_context else 'ê¸°ë³¸ ì „ëµ ê¸°ì¤€ ì ìš©'}

ê° ê¶Œê³ ì‚¬í•­ì€ 1-2ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
"""
            response = await acompletion(
                model="gpt-4.1-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ë·°í‹° ì´ì»¤ë¨¸ìŠ¤ ì „ë¬¸ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤. ê°„ê²°í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì „ëµì„ ì œì•ˆí•©ë‹ˆë‹¤."},
                    {"role": "user", "content": strategy_prompt}
                ],
                temperature=0.3,
                max_tokens=500
            )

            strategy_text = response.choices[0].message.content
            doc.add_paragraph(strategy_text)

        except Exception as e:
            # í´ë°± ì „ëµ
            doc.add_paragraph("""
1. Top 10 ìœ ì§€ ì „ëµ: í˜„ì¬ ìƒìœ„ê¶Œ ì œí’ˆì˜ ë¦¬ë·° ê´€ë¦¬ ë° ì¬ê³  í™•ë³´ë¥¼ í†µí•œ í¬ì§€ì…˜ ìœ ì§€

2. ê²½ìŸì‚¬ ëª¨ë‹ˆí„°ë§: e.l.f., Maybelline ë“± ì£¼ìš” ê²½ìŸì‚¬ì˜ ê°€ê²© ë° í”„ë¡œëª¨ì…˜ ë™í–¥ íŒŒì•…

3. ì‹ ê·œ ì§„ì… ê¸°íšŒ: Lip Care ì¹´í…Œê³ ë¦¬ ì™¸ Face Powder, Toner ë“± í™•ì¥ ê°€ëŠ¥ì„± ê²€í† 
""")

    # ===== í‘¸í„° =====
    doc.add_paragraph()
    footer = doc.add_paragraph()
    footer.alignment = WD_ALIGN_PARAGRAPH.CENTER
    footer.add_run("Â© 2025 AMORE Pacific - Confidential").italic = True

    # BytesIOë¡œ ì €ì¥
    buffer = BytesIO()
    doc.save(buffer)
    buffer.seek(0)

    # íŒŒì¼ëª… ìƒì„±
    filename = f"AMORE_Insight_Report_{datetime.now().strftime('%Y%m%d_%H%M')}.docx"

    return StreamingResponse(
        buffer,
        media_type="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
        headers={"Content-Disposition": f"attachment; filename={filename}"}
    )


@app.post("/api/export/excel")
async def export_excel(request: Request):
    """
    ì—‘ì…€ ë°ì´í„° ë‚´ë³´ë‚´ê¸° (JSON íŒŒì¼ â†’ Excel)

    ë°ì´í„° ì†ŒìŠ¤:
    - Railway: /data/latest_crawl_result.json (Volume)
    - Local: ./data/latest_crawl_result.json
    """
    import pandas as pd
    import os

    try:
        # Parse request body
        body = await request.json()
        start_date = body.get("start_date")
        end_date = body.get("end_date")
        include_metrics = body.get("include_metrics", True)

        # ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì •
        data_dir = Path("./data")

        # ========================================
        # 1ì°¨: SQLiteì—ì„œ ê¸°ê°„ë³„ ë°ì´í„° ì¡°íšŒ (ê°€ì¥ ë¹ ë¦„)
        # ========================================
        all_records = []
        data_source = None

        if start_date and end_date:
            # 1-1. SQLite ì‹œë„
            try:
                from src.tools.sqlite_storage import get_sqlite_storage
                sqlite = get_sqlite_storage()
                await sqlite.initialize()

                # limitì„ í¬ê²Œ ì„¤ì • (5ê°œ ì¹´í…Œê³ ë¦¬ Ã— 100ê°œ Ã— ê¸°ê°„ì¼ìˆ˜)
                start_dt = datetime.strptime(start_date, "%Y-%m-%d")
                end_dt = datetime.strptime(end_date, "%Y-%m-%d")
                days = (end_dt - start_dt).days + 1
                max_records = 500 * days  # ì¶©ë¶„í•œ ì—¬ìœ 

                records = await sqlite.get_raw_data(
                    start_date=start_date,
                    end_date=end_date,
                    limit=max_records
                )

                if records:
                    all_records = records
                    data_source = "sqlite"
                    logging.info(f"Excel export: loaded {len(all_records)} records from SQLite ({start_date} ~ {end_date})")

            except Exception as sqlite_err:
                logging.warning(f"Excel export: SQLite ì¡°íšŒ ì‹¤íŒ¨: {sqlite_err}")

            # 1-2. SQLite ì‹¤íŒ¨ ì‹œ Google Sheets ì‹œë„
            if not all_records:
                try:
                    sheets_writer = get_sheets_writer()
                    if not sheets_writer._initialized:
                        await sheets_writer.initialize()

                    records = await sheets_writer.get_raw_data(days=days)

                    if records:
                        for record in records:
                            snapshot_date = record.get("snapshot_date", "")
                            if snapshot_date and start_date <= snapshot_date <= end_date:
                                all_records.append(record)

                        if all_records:
                            data_source = "sheets"
                            logging.info(f"Excel export: loaded {len(all_records)} records from Google Sheets ({start_date} ~ {end_date})")

                except Exception as sheets_err:
                    logging.warning(f"Excel export: Google Sheets ì¡°íšŒ ì‹¤íŒ¨: {sheets_err}")

        # ========================================
        # 2ì°¨: ë¡œì»¬ JSON íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ (í´ë°±)
        # ========================================
        crawl_data = None
        json_path = None

        if not data_source:
            possible_paths = [
                data_dir / "latest_crawl_result.json",
                data_dir / "dashboard_data.json",
            ]

            for path in possible_paths:
                if path.exists():
                    json_path = path
                    break

            if json_path is None:
                raise HTTPException(
                    status_code=404,
                    detail=f"í¬ë¡¤ë§ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € í¬ë¡¤ë§ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”."
                )

            with open(json_path, "r", encoding="utf-8") as f:
                crawl_data = json.load(f)

            logging.info(f"Excel export: loaded data from {json_path}")

        # ë°ì´í„° ì†ŒìŠ¤ ìœ í˜• íŒë‹¨
        # 1. data_source: SQLite ë˜ëŠ” Google Sheetsì—ì„œ ê¸°ê°„ë³„ ë°ì´í„° ë¡œë“œë¨
        # 2. is_crawl_data: latest_crawl_result.jsonì˜ raw ë°ì´í„°
        # 3. is_dashboard_data: dashboard_data.jsonì˜ ì§‘ê³„ ë°ì´í„°
        is_crawl_data = False
        is_dashboard_data = False

        if crawl_data:
            if "categories" in crawl_data:
                first_cat = next(iter(crawl_data["categories"].values()), {})
                is_crawl_data = isinstance(first_cat, dict) and (
                    "rank_records" in first_cat or "products" in first_cat
                )
            is_dashboard_data = "metadata" in crawl_data and "brand" in crawl_data

        # ì¶œë ¥ ê²½ë¡œ (Railway í™˜ê²½ ê³ ë ¤)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = data_dir / "exports"
        output_dir.mkdir(parents=True, exist_ok=True)
        output_path = output_dir / f"AMORE_Data_{timestamp}.xlsx"

        sheets_created = []
        total_rows = 0

        # ì¹´í…Œê³ ë¦¬ ë§¤í•‘
        categories_info = {
            "beauty": "Beauty & Personal Care",
            "skin_care": "Skin Care",
            "lip_care": "Lip Care",
            "lip_makeup": "Lip Makeup",
            "face_powder": "Face Powder"
        }

        with pd.ExcelWriter(str(output_path), engine="openpyxl") as writer:
            # Google Sheets RawDataì™€ ë™ì¼í•œ ì»¬ëŸ¼ ìˆœì„œ
            RAWDATA_COLUMNS = [
                "snapshot_date", "category_id", "rank", "asin", "product_name",
                "brand", "price", "list_price", "discount_percent", "rating",
                "reviews_count", "badge", "coupon_text", "is_subscribe_save",
                "promo_badges", "product_url"
            ]

            # ========================================
            # Case 1: SQLite/Google Sheetsì—ì„œ ê¸°ê°„ë³„ ë°ì´í„° ë¡œë“œë¨
            # ========================================
            if data_source and all_records:
                source_name = "SQLite" if data_source == "sqlite" else "Google Sheets"
                logging.info(f"Excel export: using {source_name} data ({len(all_records)} records, {start_date} ~ {end_date})")

                df_all = pd.DataFrame(all_records)

                if not df_all.empty:
                    # 1. RawData ì‹œíŠ¸ - ì „ì²´ ë°ì´í„°
                    available_cols = [c for c in RAWDATA_COLUMNS if c in df_all.columns]
                    df_raw = df_all[available_cols].copy()
                    df_raw = df_raw.sort_values(["snapshot_date", "category_id", "rank"])
                    df_raw.to_excel(writer, sheet_name="RawData", index=False)
                    sheets_created.append("RawData")
                    total_rows += len(df_raw)

                    # 2. ë‚ ì§œë³„ ìš”ì•½ ì‹œíŠ¸
                    if "snapshot_date" in df_all.columns:
                        date_summary = []
                        for date in sorted(df_all["snapshot_date"].unique()):
                            df_date = df_all[df_all["snapshot_date"] == date]
                            laneige_count = len(df_date[df_date["brand"].str.upper() == "LANEIGE"]) if "brand" in df_date.columns else 0
                            date_summary.append({
                                "ë‚ ì§œ": date,
                                "ì´ ì œí’ˆ ìˆ˜": len(df_date),
                                "LANEIGE ì œí’ˆ ìˆ˜": laneige_count,
                                "LANEIGE SoS (%)": round(laneige_count / len(df_date) * 100, 1) if len(df_date) > 0 else 0
                            })
                        if date_summary:
                            df_summary = pd.DataFrame(date_summary)
                            df_summary.to_excel(writer, sheet_name="Daily Summary", index=False)
                            sheets_created.append("Daily Summary")
                            total_rows += len(df_summary)

                    # 3. ì¹´í…Œê³ ë¦¬ë³„ ì‹œíŠ¸
                    if "category_id" in df_all.columns:
                        for cat_id in df_all["category_id"].unique():
                            df_cat = df_all[df_all["category_id"] == cat_id].copy()
                            if df_cat.empty:
                                continue

                            display_cols = ["snapshot_date", "rank", "asin", "product_name", "brand",
                                            "price", "rating", "reviews_count", "badge"]
                            available_display = [c for c in display_cols if c in df_cat.columns]
                            df_display = df_cat[available_display].sort_values(["snapshot_date", "rank"])

                            sheet_name = categories_info.get(cat_id, cat_id)[:31]
                            df_display.to_excel(writer, sheet_name=sheet_name, index=False)
                            sheets_created.append(sheet_name)
                            total_rows += len(df_display)

                    # 4. LANEIGE ì œí’ˆ ì „ìš© ì‹œíŠ¸
                    if "brand" in df_all.columns:
                        df_laneige = df_all[df_all["brand"].str.upper() == "LANEIGE"].copy()
                        if not df_laneige.empty:
                            laneige_cols = ["snapshot_date", "category_id", "rank", "asin",
                                            "product_name", "price", "rating", "reviews_count", "badge"]
                            available_laneige = [c for c in laneige_cols if c in df_laneige.columns]
                            df_laneige = df_laneige[available_laneige].sort_values(["snapshot_date", "category_id", "rank"])
                            df_laneige.to_excel(writer, sheet_name="LANEIGE Products", index=False)
                            sheets_created.append("LANEIGE Products")
                            total_rows += len(df_laneige)

            # ========================================
            # Case 2: ëŒ€ì‹œë³´ë“œ ë°ì´í„° í˜•ì‹ (ì§‘ê³„ ë°ì´í„°ë§Œ)
            # ========================================
            elif is_dashboard_data and not is_crawl_data:
                logging.info("Excel export: using dashboard_data.json (aggregated data only)")

                # 1. Overview ì‹œíŠ¸
                metadata = crawl_data.get("metadata", {})
                data_source = crawl_data.get("data_source", {})
                overview_data = [
                    {"í•­ëª©": "ë°ì´í„° ë‚ ì§œ", "ê°’": metadata.get("data_date", "N/A")},
                    {"í•­ëª©": "ìƒì„± ì‹œê°", "ê°’": metadata.get("generated_at", "N/A")},
                    {"í•­ëª©": "ì´ ì œí’ˆ ìˆ˜", "ê°’": metadata.get("total_products", 0)},
                    {"í•­ëª©": "LANEIGE ì œí’ˆ ìˆ˜", "ê°’": metadata.get("laneige_products", 0)},
                    {"í•­ëª©": "í”Œë«í¼", "ê°’": data_source.get("platform", "Amazon US")},
                ]
                df_overview = pd.DataFrame(overview_data)
                df_overview.to_excel(writer, sheet_name="Overview", index=False)
                sheets_created.append("Overview")
                total_rows += len(df_overview)

                # 2. Brand KPIs ì‹œíŠ¸
                brand_kpis = crawl_data.get("brand", {}).get("kpis", {})
                if brand_kpis:
                    kpi_data = [
                        {"KPI": "SoS (Share of Shelf)", "ê°’": f"{brand_kpis.get('sos', 0)}%"},
                        {"KPI": "SoS ë³€í™”", "ê°’": brand_kpis.get("sos_delta", "N/A")},
                        {"KPI": "Top 10 ì œí’ˆ ìˆ˜", "ê°’": brand_kpis.get("top10_count", 0)},
                        {"KPI": "í‰ê·  ìˆœìœ„", "ê°’": brand_kpis.get("avg_rank", 0)},
                        {"KPI": "HHI (ì‹œì¥ ì§‘ì¤‘ë„)", "ê°’": brand_kpis.get("hhi", 0)},
                    ]
                    df_kpis = pd.DataFrame(kpi_data)
                    df_kpis.to_excel(writer, sheet_name="LANEIGE KPIs", index=False)
                    sheets_created.append("LANEIGE KPIs")
                    total_rows += len(df_kpis)

                # 3. Competitors ì‹œíŠ¸
                competitors = crawl_data.get("brand", {}).get("competitors", [])
                if competitors:
                    df_comp = pd.DataFrame(competitors)
                    column_mapping = {
                        "brand": "Brand",
                        "sos": "SoS (%)",
                        "avg_rank": "Avg Rank",
                        "product_count": "Product Count",
                        "avg_price": "Avg Price ($)"
                    }
                    existing_cols = {k: v for k, v in column_mapping.items() if k in df_comp.columns}
                    df_comp = df_comp.rename(columns=existing_cols)
                    df_comp.to_excel(writer, sheet_name="Competitors", index=False)
                    sheets_created.append("Competitors")
                    total_rows += len(df_comp)

                # 4. Action Items ì‹œíŠ¸
                action_items = crawl_data.get("home", {}).get("action_items", [])
                if action_items:
                    df_actions = pd.DataFrame(action_items)
                    df_actions.to_excel(writer, sheet_name="Action Items", index=False)
                    sheets_created.append("Action Items")
                    total_rows += len(df_actions)

                # 5. Category View ì‹œíŠ¸
                category_data = crawl_data.get("category", {})
                if category_data:
                    for cat_id, cat_info in category_data.items():
                        top_products = cat_info.get("top_products", [])
                        if top_products:
                            df_cat = pd.DataFrame(top_products)
                            sheet_name = categories_info.get(cat_id, cat_id)[:31]
                            df_cat.to_excel(writer, sheet_name=sheet_name, index=False)
                            sheets_created.append(sheet_name)
                            total_rows += len(df_cat)

            # ========================================
            # Case 3: ë¡œì»¬ í¬ë¡¤ë§ ì›ë³¸ ë°ì´í„°
            # ========================================
            else:
                logging.info("Excel export: using latest_crawl_result.json (raw crawl data)")

                # ì „ì²´ RawData ìˆ˜ì§‘ (ì¹´í…Œê³ ë¦¬ë³„ rank_records)
                all_records = []
                for cat_id, cat_data in crawl_data.get("categories", {}).items():
                    records = cat_data.get("rank_records", cat_data.get("products", []))
                    for record in records:
                        # category_id ì¶”ê°€ (ì—†ëŠ” ê²½ìš°)
                        if "category_id" not in record:
                            record["category_id"] = cat_id
                        all_records.append(record)

                if not all_records:
                    logging.warning("Excel export: no rank_records found in crawl data")

                if all_records:
                    df_all = pd.DataFrame(all_records)

                    # ë‚ ì§œ í•„í„° ì ìš© (ì„ íƒ ê¸°ê°„)
                    if start_date and "snapshot_date" in df_all.columns:
                        df_all = df_all[df_all["snapshot_date"] >= start_date]
                    if end_date and "snapshot_date" in df_all.columns:
                        df_all = df_all[df_all["snapshot_date"] <= end_date]

                    if not df_all.empty:
                        # 1. RawData ì‹œíŠ¸ - Google Sheetsì™€ ë™ì¼í•œ ì „ì²´ ë°ì´í„°
                        available_cols = [c for c in RAWDATA_COLUMNS if c in df_all.columns]
                        df_raw = df_all[available_cols].copy()
                        df_raw = df_raw.sort_values(["category_id", "rank"])
                        df_raw.to_excel(writer, sheet_name="RawData", index=False)
                        sheets_created.append("RawData")
                        total_rows += len(df_raw)

                        # 2. ì¹´í…Œê³ ë¦¬ë³„ ì‹œíŠ¸ (ìš”ì•½ ë³´ê¸°ìš©)
                        for cat_id in df_all["category_id"].unique():
                            df_cat = df_all[df_all["category_id"] == cat_id].copy()
                            if df_cat.empty:
                                continue

                            # í•µì‹¬ ì»¬ëŸ¼ë§Œ ì„ íƒí•˜ì—¬ ê°€ë…ì„± í–¥ìƒ
                            display_cols = ["rank", "asin", "product_name", "brand",
                                            "price", "rating", "reviews_count", "badge"]
                            available_display = [c for c in display_cols if c in df_cat.columns]
                            df_display = df_cat[available_display].sort_values("rank")

                            # ì‹œíŠ¸ ì´ë¦„ (31ì ì œí•œ)
                            sheet_name = categories_info.get(cat_id, cat_id)[:31]
                            df_display.to_excel(writer, sheet_name=sheet_name, index=False)
                            sheets_created.append(sheet_name)
                            total_rows += len(df_display)

                        # 3. LANEIGE ì œí’ˆ ì „ìš© ì‹œíŠ¸
                        df_laneige = df_all[df_all["brand"].str.upper() == "LANEIGE"].copy()
                        if not df_laneige.empty:
                            laneige_cols = ["snapshot_date", "category_id", "rank", "asin",
                                            "product_name", "price", "rating", "reviews_count", "badge"]
                            available_laneige = [c for c in laneige_cols if c in df_laneige.columns]
                            df_laneige = df_laneige[available_laneige].sort_values(["category_id", "rank"])
                            df_laneige.to_excel(writer, sheet_name="LANEIGE Products", index=False)
                            sheets_created.append("LANEIGE Products")
                            total_rows += len(df_laneige)

                        # 4. Summary ì‹œíŠ¸ - ë¸Œëœë“œë³„ ì§‘ê³„
                        if "brand" in df_all.columns:
                            agg_dict = {"asin": "count"}
                            if "rank" in df_all.columns:
                                agg_dict["rank"] = "mean"
                            if "price" in df_all.columns:
                                agg_dict["price"] = "mean"
                            if "rating" in df_all.columns:
                                agg_dict["rating"] = "mean"

                            summary = df_all.groupby("brand").agg(agg_dict).reset_index()
                            col_names = ["Brand", "Product Count"]
                            if "rank" in agg_dict:
                                col_names.append("Avg Rank")
                            if "price" in agg_dict:
                                col_names.append("Avg Price")
                            if "rating" in agg_dict:
                                col_names.append("Avg Rating")
                            summary.columns = col_names

                            summary = summary.sort_values("Product Count", ascending=False).head(30)
                            for col in ["Avg Rank", "Avg Price", "Avg Rating"]:
                                if col in summary.columns:
                                    summary[col] = summary[col].round(2)

                            summary.to_excel(writer, sheet_name="Summary", index=False)
                            sheets_created.append("Summary")
                            total_rows += len(summary)

            # 4. ì‹œíŠ¸ê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ì•ˆë‚´ ì‹œíŠ¸ ìƒì„±
            if not sheets_created:
                data_source_info = f"SQLite" if data_source == "sqlite" else ("Google Sheets" if data_source == "sheets" else (str(json_path) if json_path else "N/A"))
                no_data_info = [
                    {"í•­ëª©": "ìš”ì²­ ê¸°ê°„", "ê°’": f"{start_date or 'N/A'} ~ {end_date or 'N/A'}"},
                    {"í•­ëª©": "ê²°ê³¼", "ê°’": "í•´ë‹¹ ê¸°ê°„ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"},
                    {"í•­ëª©": "ë°ì´í„° ì†ŒìŠ¤", "ê°’": data_source_info},
                    {"í•­ëª©": "ì•ˆë‚´", "ê°’": "í¬ë¡¤ë§ ì‹¤í–‰ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”"}
                ]
                df_no_data = pd.DataFrame(no_data_info)
                df_no_data.to_excel(writer, sheet_name="No Data", index=False)
                sheets_created.append("No Data")

        logging.info(f"Excel exported: {output_path} ({total_rows} rows, sheets: {sheets_created})")

        # íŒŒì¼ ë°˜í™˜
        return FileResponse(
            path=str(output_path),
            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            headers={"Content-Disposition": f"attachment; filename={output_path.name}"}
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Excel export error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Excel ë‚´ë³´ë‚´ê¸° ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤")


# ============= Competitor Comparison API =============

@app.get("/api/competitors")
async def get_competitor_data(brand: Optional[str] = None):
    """
    ê²½ìŸì‚¬ ì¶”ì  ë°ì´í„° ì¡°íšŒ

    Args:
        brand: ë¸Œëœë“œ í•„í„° (ì˜ˆ: "Summer Fridays")

    Returns:
        ê²½ìŸì‚¬ ì œí’ˆ ëª©ë¡ ë° LANEIGE ë¹„êµ ë°ì´í„°
    """
    try:
        from src.tools.sqlite_storage import get_sqlite_storage
        from pathlib import Path
        import json

        result = {
            "competitors": {},
            "laneige_products": {},
            "comparison": []
        }

        # 1. SQLiteì—ì„œ ê²½ìŸì‚¬ ë°ì´í„° ì¡°íšŒ ì‹œë„
        try:
            sqlite = get_sqlite_storage()
            await sqlite.initialize()
            comp_products = await sqlite.get_competitor_products(brand=brand)

            if comp_products:
                # ë¸Œëœë“œë³„ë¡œ ê·¸ë£¹í™”
                for p in comp_products:
                    brand_name = p.get("brand", "Unknown")
                    if brand_name not in result["competitors"]:
                        result["competitors"][brand_name] = {
                            "brand": brand_name,
                            "products": [],
                            "product_count": 0,
                            "avg_price": 0,
                            "avg_rating": 0
                        }
                    result["competitors"][brand_name]["products"].append(p)

                # ë¸Œëœë“œë³„ ì§‘ê³„
                for brand_name, brand_data in result["competitors"].items():
                    products = brand_data["products"]
                    brand_data["product_count"] = len(products)
                    prices = [p["price"] for p in products if p.get("price")]
                    ratings = [p["rating"] for p in products if p.get("rating")]
                    brand_data["avg_price"] = round(sum(prices) / len(prices), 2) if prices else 0
                    brand_data["avg_rating"] = round(sum(ratings) / len(ratings), 1) if ratings else 0

        except Exception as sqlite_err:
            logging.warning(f"SQLite competitor query failed: {sqlite_err}")

        # 2. JSON íŒŒì¼ì—ì„œ í´ë°±
        if not result["competitors"]:
            json_path = Path("./data/competitor_products.json")
            if json_path.exists():
                with open(json_path, "r", encoding="utf-8") as f:
                    json_data = json.load(f)
                    for p in json_data.get("products", []):
                        brand_name = p.get("brand", "Unknown")
                        if brand and brand_name != brand:
                            continue
                        if brand_name not in result["competitors"]:
                            result["competitors"][brand_name] = {
                                "brand": brand_name,
                                "products": [],
                                "product_count": 0
                            }
                        result["competitors"][brand_name]["products"].append(p)

        # 3. LANEIGE ì œí’ˆ ë°ì´í„° ë¡œë“œ (ìµœì‹  í¬ë¡¤ë§ ë°ì´í„°ì—ì„œ)
        data = load_dashboard_data()
        if data:
            # ì¹´í…Œê³ ë¦¬ë³„ LANEIGE ì œí’ˆ ì¶”ì¶œ
            for cat_id, cat_data in data.get("category", {}).items():
                for product in cat_data.get("top_products", []):
                    if "laneige" in product.get("brand", "").lower():
                        product_type = _detect_product_type(product.get("product_name", ""))
                        if product_type not in result["laneige_products"]:
                            result["laneige_products"][product_type] = []
                        result["laneige_products"][product_type].append({
                            **product,
                            "category_id": cat_id,
                            "product_type": product_type
                        })

        # 4. ì œí’ˆ íƒ€ì…ë³„ ë¹„êµ ë°ì´í„° ìƒì„±
        for brand_name, brand_data in result["competitors"].items():
            for comp_product in brand_data["products"]:
                laneige_match = comp_product.get("laneige_competitor")
                product_type = comp_product.get("product_type", "")

                comparison_item = {
                    "competitor_brand": brand_name,
                    "competitor_product": comp_product.get("product_name", ""),
                    "competitor_price": comp_product.get("price"),
                    "competitor_rating": comp_product.get("rating"),
                    "competitor_reviews": comp_product.get("reviews_count"),
                    "product_type": product_type,
                    "laneige_product": laneige_match,
                    "laneige_price": None,
                    "laneige_rating": None,
                    "laneige_reviews": None,
                    "price_diff": None,
                    "rating_diff": None
                }

                # LANEIGE ë§¤ì¹­ ì œí’ˆ ì°¾ê¸°
                if product_type in result["laneige_products"]:
                    for lp in result["laneige_products"][product_type]:
                        comparison_item["laneige_price"] = lp.get("price")
                        comparison_item["laneige_rating"] = lp.get("rating")
                        comparison_item["laneige_reviews"] = lp.get("reviews_count")

                        # ì°¨ì´ ê³„ì‚°
                        if comparison_item["competitor_price"] and comparison_item["laneige_price"]:
                            comparison_item["price_diff"] = round(
                                comparison_item["laneige_price"] - comparison_item["competitor_price"], 2
                            )
                        if comparison_item["competitor_rating"] and comparison_item["laneige_rating"]:
                            comparison_item["rating_diff"] = round(
                                comparison_item["laneige_rating"] - comparison_item["competitor_rating"], 1
                            )
                        break

                result["comparison"].append(comparison_item)

        return result

    except Exception as e:
        logger.error(f"Competitor data error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="ê²½ìŸì‚¬ ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤")


def _detect_product_type(product_name: str) -> str:
    """ì œí’ˆëª…ì—ì„œ ì œí’ˆ íƒ€ì… ì¶”ì¶œ"""
    name_lower = product_name.lower()

    if "lip sleeping" in name_lower or "lip mask" in name_lower:
        return "lip_balm"
    elif "lip glowy" in name_lower or "lip butter" in name_lower or "lip balm" in name_lower:
        return "lip_balm"
    elif "water sleeping" in name_lower or "sleeping mask" in name_lower:
        return "sleeping_mask"
    elif "water bank" in name_lower or "cream" in name_lower or "moisturizer" in name_lower:
        return "moisturizer"
    elif "toner" in name_lower or "cream skin" in name_lower:
        return "toner"
    elif "serum" in name_lower:
        return "serum"
    else:
        return "other"


@app.get("/api/competitors/brands")
async def get_tracked_brands():
    """ì¶”ì  ì¤‘ì¸ ê²½ìŸì‚¬ ë¸Œëœë“œ ëª©ë¡"""
    try:
        from pathlib import Path
        import json

        config_path = Path("./config/tracked_competitors.json")
        if not config_path.exists():
            return {"brands": []}

        with open(config_path, "r", encoding="utf-8") as f:
            config = json.load(f)

        brands = []
        for brand_name, brand_config in config.get("competitors", {}).items():
            brands.append({
                "name": brand_name,
                "tier": brand_config.get("tier", ""),
                "product_count": len(brand_config.get("products", []))
            })

        return {"brands": brands}

    except Exception as e:
        logging.error(f"Get tracked brands error: {e}")
        return {"brands": []}


# ============= Alert Settings API =============

from src.core.state_manager import StateManager, get_state_manager

# ì‹±ê¸€í†¤ State Manager
_state_manager: Optional[StateManager] = None

def get_app_state_manager() -> StateManager:
    """ì•± ë ˆë²¨ State Manager ë°˜í™˜"""
    global _state_manager
    if _state_manager is None:
        _state_manager = get_state_manager()
    return _state_manager


class AlertSettingsRequest(BaseModel):
    """ì•Œë¦¼ ì„¤ì • ìš”ì²­"""
    email: str
    consent: bool
    alert_types: List[str] = []


class AlertSettingsResponse(BaseModel):
    """ì•Œë¦¼ ì„¤ì • ì‘ë‹µ"""
    email: str
    consent: bool
    alert_types: List[str]
    consent_date: Optional[str] = None


@app.get("/api/v3/alert-settings")
async def get_alert_settings():
    """
    í˜„ì¬ ì•Œë¦¼ ì„¤ì • ì¡°íšŒ

    ì°¸ê³ : í˜„ì¬ëŠ” ë‹¨ì¼ ì‚¬ìš©ì ì„¤ì •ë§Œ ì§€ì› (ì²« ë²ˆì§¸ ë“±ë¡ëœ ì´ë©”ì¼)
    """
    state_manager = get_app_state_manager()
    subscriptions = state_manager.get_all_subscriptions()

    if not subscriptions:
        return {
            "email": "",
            "consent": False,
            "alert_types": [],
            "consent_date": None
        }

    # ì²« ë²ˆì§¸ êµ¬ë… ë°˜í™˜
    email, sub = next(iter(subscriptions.items()))
    return {
        "email": email,
        "consent": sub.consent,
        "alert_types": sub.alert_types,
        "consent_date": sub.consent_date.isoformat() if sub.consent_date else None
    }


@app.post("/api/v3/alert-settings", dependencies=[Depends(verify_api_key)])
async def save_alert_settings(request: AlertSettingsRequest):
    """
    ì•Œë¦¼ ì„¤ì • ì €ì¥ (API Key í•„ìš”)

    ì¤‘ìš”: consentê°€ Trueì¼ ë•Œë§Œ ì´ë©”ì¼ ë“±ë¡
    """
    state_manager = get_app_state_manager()

    if not request.email:
        raise HTTPException(status_code=400, detail="ì´ë©”ì¼ ì£¼ì†Œê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    if request.consent:
        # ì´ë©”ì¼ ë“±ë¡ (ëª…ì‹œì  ë™ì˜)
        success = state_manager.register_email(
            email=request.email,
            consent=True,
            alert_types=request.alert_types
        )

        if not success:
            raise HTTPException(status_code=400, detail="ì´ë©”ì¼ ë“±ë¡ ì‹¤íŒ¨")

        return {"status": "ok", "message": "ì•Œë¦¼ ì„¤ì •ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤."}
    else:
        # ë™ì˜ ì—†ìœ¼ë©´ ì—…ë°ì´íŠ¸ë§Œ (ì•Œë¦¼ ìœ í˜• ë³€ê²½)
        success = state_manager.update_email_subscription(
            email=request.email,
            alert_types=request.alert_types
        )

        return {"status": "ok", "message": "ì„¤ì •ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤."}


@app.post("/api/v3/alert-settings/revoke", dependencies=[Depends(verify_api_key)])
async def revoke_alert_consent():
    """
    ì•Œë¦¼ ë™ì˜ ì² íšŒ (API Key í•„ìš”)

    ì²« ë²ˆì§¸ ë“±ë¡ëœ ì´ë©”ì¼ì˜ ë™ì˜ë¥¼ ì² íšŒí•©ë‹ˆë‹¤.
    """
    state_manager = get_app_state_manager()
    subscriptions = state_manager.get_all_subscriptions()

    if not subscriptions:
        return {"status": "ok", "message": "ì² íšŒí•  ë™ì˜ê°€ ì—†ìŠµë‹ˆë‹¤."}

    # ì²« ë²ˆì§¸ ì´ë©”ì¼ ì² íšŒ
    email = next(iter(subscriptions.keys()))
    state_manager.revoke_email_consent(email)

    return {"status": "ok", "message": "ë™ì˜ê°€ ì² íšŒë˜ì—ˆìŠµë‹ˆë‹¤."}


@app.get("/api/v3/alerts")
async def get_alerts(limit: int = 50, alert_type: Optional[str] = None):
    """
    ì•Œë¦¼ ëª©ë¡ ì¡°íšŒ

    Args:
        limit: ìµœëŒ€ ê°œìˆ˜
        alert_type: í•„í„°í•  ì•Œë¦¼ ìœ í˜•
    """
    from src.agents.alert_agent import AlertAgent

    state_manager = get_app_state_manager()
    alert_agent = AlertAgent(state_manager)

    return {
        "alerts": alert_agent.get_alerts(limit=limit, alert_type=alert_type),
        "pending_count": alert_agent.get_pending_count(),
        "stats": alert_agent.get_stats()
    }


# ============= ëŒ€ì‹œë³´ë“œ HTML ì„œë¹™ =============

@app.get("/dashboard")
async def serve_dashboard():
    """ëŒ€ì‹œë³´ë“œ HTML í˜ì´ì§€ ì„œë¹™"""
    dashboard_path = Path("./dashboard/amore_unified_dashboard_v4.html")
    if not dashboard_path.exists():
        raise HTTPException(status_code=404, detail="Dashboard not found")
    return FileResponse(dashboard_path, media_type="text/html")


@app.get("/api/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}


# ============= Level 4 Brain API (v4) =============

class BrainChatRequest(BaseModel):
    """Brain ì±—ë´‡ ìš”ì²­"""
    message: str
    session_id: Optional[str] = "default"
    skip_cache: bool = False


class BrainChatResponse(BaseModel):
    """Brain ì±—ë´‡ ì‘ë‹µ"""
    text: str
    confidence: float
    sources: List[str]
    reasoning: Optional[str] = None
    tools_used: List[str]
    processing_time_ms: float
    from_cache: bool
    brain_mode: str


@app.post("/api/v4/chat", response_model=BrainChatResponse)
@limiter.limit("30/minute")  # ë¶„ë‹¹ 30íšŒ ì œí•œ
async def chat_v4(request: Request, body: BrainChatRequest):
    """
    Level 4 Brain ê¸°ë°˜ ì±—ë´‡ API (v4)

    LLM-First ì ‘ê·¼:
    - ëª¨ë“  íŒë‹¨ì„ LLMì´ ìˆ˜í–‰
    - ê·œì¹™ ê¸°ë°˜ ë¹ ë¥¸ ê²½ë¡œ ì—†ìŒ
    - RAG + KG í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
    - ììœ¨ ìŠ¤ì¼€ì¤„ëŸ¬ì™€ í†µí•©
    """
    import time
    start_time = time.time()

    message = body.message.strip()
    session_id = body.session_id or "default"

    if not message:
        raise HTTPException(status_code=400, detail="Message is required")

    try:
        # Brain ì¸ìŠ¤í„´ìŠ¤ íšë“
        brain = await get_initialized_brain()

        # í˜„ì¬ ë©”íŠ¸ë¦­ ë°ì´í„° ë¡œë“œ
        data = load_dashboard_data()
        current_metrics = data if data else None

        # Brainìœ¼ë¡œ ì²˜ë¦¬ (LLM-First)
        response = await brain.process_query(
            query=message,
            session_id=session_id,
            current_metrics=current_metrics,
            skip_cache=body.skip_cache
        )

        processing_time = (time.time() - start_time) * 1000

        return BrainChatResponse(
            text=response.content,
            confidence=response.confidence,
            sources=response.sources,
            reasoning=response.reasoning,
            tools_used=response.tools_called if hasattr(response, 'tools_called') else [],
            processing_time_ms=processing_time,
            from_cache=response.from_cache if hasattr(response, 'from_cache') else False,
            brain_mode=brain.mode.value
        )

    except Exception as e:
        logging.error(f"Brain error: {e}")
        return BrainChatResponse(
            text=f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            confidence=0.0,
            sources=[],
            reasoning=None,
            tools_used=[],
            processing_time_ms=(time.time() - start_time) * 1000,
            from_cache=False,
            brain_mode="error"
        )


@app.get("/api/v4/brain/status")
async def get_brain_status():
    """
    Brain ìƒíƒœ ì¡°íšŒ

    Returns:
        - mode: í˜„ì¬ Brain ëª¨ë“œ
        - scheduler: ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ
        - pending_tasks: ëŒ€ê¸° ì¤‘ íƒœìŠ¤í¬
        - stats: í†µê³„
    """
    try:
        brain = await get_initialized_brain()

        return {
            "mode": brain.mode.value,
            "scheduler_running": brain.scheduler.running if brain.scheduler else False,
            "pending_tasks": brain.scheduler.get_pending_count() if brain.scheduler else 0,
            "stats": brain.get_stats(),
            "initialized": True
        }
    except Exception as e:
        return {
            "mode": "uninitialized",
            "scheduler_running": False,
            "pending_tasks": 0,
            "stats": {},
            "initialized": False,
            "error": str(e)
        }


@app.post("/api/v4/brain/scheduler/start", dependencies=[Depends(verify_api_key)])
async def start_brain_scheduler():
    """
    ììœ¨ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ (API Key í•„ìš”)

    - ì¼ì¼ í¬ë¡¤ë§ (09:00)
    - ì£¼ê¸°ì  ì•Œë¦¼ ì²´í¬ (30ë¶„)
    - ë°±ê·¸ë¼ìš´ë“œ ë¶„ì„
    """
    try:
        brain = await get_initialized_brain()

        if brain.scheduler and brain.scheduler.running:
            return {
                "started": False,
                "message": "ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.",
                "status": "running"
            }

        await brain.start_scheduler()

        return {
            "started": True,
            "message": "ììœ¨ ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "status": "running"
        }
    except Exception as e:
        return {
            "started": False,
            "message": f"ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ ì‹¤íŒ¨: {str(e)}",
            "status": "error"
        }


@app.post("/api/v4/brain/scheduler/stop", dependencies=[Depends(verify_api_key)])
async def stop_brain_scheduler():
    """ììœ¨ ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€ (API Key í•„ìš”)"""
    try:
        brain = await get_initialized_brain()

        if brain.scheduler:
            brain.scheduler.stop()

        return {
            "stopped": True,
            "message": "ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "status": "stopped"
        }
    except Exception as e:
        return {
            "stopped": False,
            "message": f"ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€ ì‹¤íŒ¨: {str(e)}",
            "status": "error"
        }


@app.post("/api/v4/brain/autonomous-cycle", dependencies=[Depends(verify_api_key)])
async def run_autonomous_cycle():
    """
    ììœ¨ ì‚¬ì´í´ ìˆ˜ë™ ì‹¤í–‰ (API Key í•„ìš”)

    1. ë°ì´í„° ì‹ ì„ ë„ í™•ì¸
    2. í•„ìš”ì‹œ í¬ë¡¤ë§
    3. ì§€í‘œ ê³„ì‚°
    4. ì•Œë¦¼ ì¡°ê±´ ì²´í¬
    5. ì¸ì‚¬ì´íŠ¸ ìƒì„±
    """
    try:
        brain = await get_initialized_brain()
        result = await brain.run_autonomous_cycle()

        return {
            "success": True,
            "result": result
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }


@app.post("/api/v4/brain/check-alerts")
async def check_brain_alerts():
    """
    ì•Œë¦¼ ì¡°ê±´ ìˆ˜ë™ ì²´í¬

    í˜„ì¬ ë©”íŠ¸ë¦­ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì•Œë¦¼ ì¡°ê±´ì„ ì²´í¬í•©ë‹ˆë‹¤.
    """
    try:
        brain = await get_initialized_brain()
        data = load_dashboard_data()

        if not data:
            return {
                "alerts": [],
                "message": "ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
            }

        alerts = await brain.check_alerts(data)

        return {
            "alerts": alerts,
            "count": len(alerts),
            "checked_at": datetime.now().isoformat()
        }
    except Exception as e:
        return {
            "alerts": [],
            "error": str(e)
        }


@app.get("/api/v4/brain/stats")
async def get_brain_stats():
    """Brain í†µê³„ ì¡°íšŒ"""
    try:
        brain = await get_initialized_brain()
        return brain.get_stats()
    except Exception as e:
        return {"error": str(e)}


@app.post("/api/v4/brain/mode", dependencies=[Depends(verify_api_key)])
async def set_brain_mode(mode: str):
    """
    Brain ëª¨ë“œ ë³€ê²½ (API Key í•„ìš”)

    Args:
        mode: reactive, proactive, autonomous
    """
    try:
        brain = await get_initialized_brain()

        mode_map = {
            "reactive": BrainMode.REACTIVE,
            "proactive": BrainMode.PROACTIVE,
            "autonomous": BrainMode.AUTONOMOUS
        }

        if mode not in mode_map:
            raise HTTPException(
                status_code=400,
                detail=f"Invalid mode. Valid modes: {list(mode_map.keys())}"
            )

        brain.mode = mode_map[mode]

        return {
            "mode": brain.mode.value,
            "message": f"Brain ëª¨ë“œê°€ {mode}(ìœ¼)ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤."
        }
    except HTTPException:
        raise
    except Exception as e:
        return {"error": str(e)}


# ============= Amazon Deals API =============

from src.tools.deals_scraper import AmazonDealsScraper, get_deals_scraper


class DealsRequest(BaseModel):
    """Deals í¬ë¡¤ë§ ìš”ì²­"""
    max_items: int = 50
    beauty_only: bool = True


class DealsResponse(BaseModel):
    """Deals ì‘ë‹µ"""
    success: bool
    count: int
    lightning_count: int
    competitor_count: int
    snapshot_datetime: str
    deals: List[Dict[str, Any]]
    competitor_deals: List[Dict[str, Any]]
    error: Optional[str] = None


@app.get("/api/deals")
async def get_deals_data(
    brand: Optional[str] = None,
    hours: int = 24,
    limit: int = 100
):
    """
    ì €ì¥ëœ Deals ë°ì´í„° ì¡°íšŒ

    Args:
        brand: ë¸Œëœë“œ í•„í„° (ì„ íƒ)
        hours: ìµœê·¼ Nì‹œê°„ ë°ì´í„° (ê¸°ë³¸: 24ì‹œê°„)
        limit: ìµœëŒ€ ê°œìˆ˜

    Returns:
        - deals: ë”œ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        - summary: ìš”ì•½ í†µê³„
    """
    try:
        storage = get_sqlite_storage()
        await storage.initialize()

        # ê²½ìŸì‚¬ ë”œ ì¡°íšŒ
        deals = await storage.get_competitor_deals(brand=brand, hours=hours)

        # ìµœëŒ€ ê°œìˆ˜ ì œí•œ
        deals = deals[:limit] if len(deals) > limit else deals

        # ìš”ì•½ í†µê³„
        summary = await storage.get_deals_summary(days=7)

        return {
            "success": True,
            "deals": deals,
            "count": len(deals),
            "summary": summary,
            "filters": {
                "brand": brand,
                "hours": hours
            }
        }

    except Exception as e:
        logging.error(f"Deals data error: {e}")
        return {
            "success": False,
            "deals": [],
            "count": 0,
            "error": str(e)
        }


@app.get("/api/deals/summary")
async def get_deals_summary(days: int = 7):
    """
    Deals ìš”ì•½ í†µê³„

    Args:
        days: ë¶„ì„ ê¸°ê°„ (ì¼)

    Returns:
        - by_brand: ë¸Œëœë“œë³„ ë”œ í˜„í™©
        - by_date: ì¼ë³„ ì¶”ì´
    """
    try:
        storage = get_sqlite_storage()
        await storage.initialize()

        summary = await storage.get_deals_summary(days=days)

        return {
            "success": True,
            **summary
        }

    except Exception as e:
        logging.error(f"Deals summary error: {e}")
        return {
            "success": False,
            "error": str(e)
        }


@app.post("/api/deals/scrape", dependencies=[Depends(verify_api_key)])
async def scrape_deals(request: DealsRequest):
    """
    Amazon Deals í˜ì´ì§€ í¬ë¡¤ë§ (API Key í•„ìš”)

    ê²½ìŸì‚¬ í• ì¸ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤.

    Args:
        max_items: ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜
        beauty_only: ë·°í‹° ì¹´í…Œê³ ë¦¬ë§Œ í•„í„°ë§

    Returns:
        - deals: ìˆ˜ì§‘ëœ ë”œ ë°ì´í„°
        - competitor_deals: ê²½ìŸì‚¬ ë”œ
        - lightning_count: Lightning Deal ìˆ˜
    """
    try:
        scraper = await get_deals_scraper()

        # í¬ë¡¤ë§ ì‹¤í–‰
        result = await scraper.scrape_deals(
            max_items=request.max_items,
            beauty_only=request.beauty_only
        )

        if result["success"]:
            # SQLiteì— ì €ì¥
            storage = get_sqlite_storage()
            await storage.initialize()

            # ëª¨ë“  ë”œ ì €ì¥
            if result["deals"]:
                await storage.save_deals(result["deals"], is_competitor=False)

            # ê²½ìŸì‚¬ ë”œì€ is_competitor=Trueë¡œ ë³„ë„ ì €ì¥
            if result["competitor_deals"]:
                await storage.save_deals(result["competitor_deals"], is_competitor=True)

                # ì•Œë¦¼ ì„œë¹„ìŠ¤ë¡œ ì•Œë¦¼ ì²˜ë¦¬
                try:
                    alert_service = get_alert_service()
                    alerts = await alert_service.process_deals_for_alerts(result["competitor_deals"])

                    # DBì— ì•Œë¦¼ ì €ì¥
                    for alert in alerts:
                        await storage.save_deal_alert(alert)

                    logging.info(f"Processed {len(alerts)} alerts from {len(result['competitor_deals'])} competitor deals")
                except Exception as alert_err:
                    logging.error(f"Alert processing error: {alert_err}")
                    # ì•Œë¦¼ ì‹¤íŒ¨í•´ë„ í¬ë¡¤ë§ ê²°ê³¼ëŠ” ë°˜í™˜

            logging.info(f"Deals scraped: {result['count']} total, {len(result['competitor_deals'])} competitors")

        return DealsResponse(
            success=result["success"],
            count=result["count"],
            lightning_count=result["lightning_count"],
            competitor_count=len(result["competitor_deals"]),
            snapshot_datetime=result["snapshot_datetime"],
            deals=result["deals"],
            competitor_deals=result["competitor_deals"],
            error=result.get("error")
        )

    except Exception as e:
        logging.error(f"Deals scrape error: {e}")
        return DealsResponse(
            success=False,
            count=0,
            lightning_count=0,
            competitor_count=0,
            snapshot_datetime=datetime.now().isoformat(),
            deals=[],
            competitor_deals=[],
            error=str(e)
        )


@app.get("/api/deals/alerts")
async def get_deals_alerts(
    limit: int = 50,
    unsent_only: bool = False
):
    """
    Deals ì•Œë¦¼ ëª©ë¡ ì¡°íšŒ

    Args:
        limit: ìµœëŒ€ ê°œìˆ˜
        unsent_only: ë¯¸ë°œì†¡ ì•Œë¦¼ë§Œ ì¡°íšŒ

    Returns:
        - alerts: ì•Œë¦¼ ëª©ë¡
        - count: ì´ ê°œìˆ˜
    """
    try:
        storage = get_sqlite_storage()
        await storage.initialize()

        if unsent_only:
            alerts = await storage.get_unsent_alerts(limit=limit)
        else:
            # ëª¨ë“  ì•Œë¦¼ ì¡°íšŒ (ìµœê·¼ 7ì¼)
            with storage.get_connection() as conn:
                cursor = conn.execute("""
                    SELECT * FROM deals_alerts
                    ORDER BY alert_datetime DESC
                    LIMIT ?
                """, (limit,))
                alerts = [dict(row) for row in cursor.fetchall()]

        return {
            "success": True,
            "alerts": alerts,
            "count": len(alerts)
        }

    except Exception as e:
        logging.error(f"Deals alerts error: {e}")
        return {
            "success": False,
            "alerts": [],
            "count": 0,
            "error": str(e)
        }


@app.post("/api/deals/export")
async def export_deals_report(
    days: int = 7,
    format: str = "excel"
):
    """
    Deals ë¦¬í¬íŠ¸ ë‚´ë³´ë‚´ê¸°

    Args:
        days: ë¶„ì„ ê¸°ê°„ (ì¼)
        format: ì¶œë ¥ í˜•ì‹ (excel, json)

    Returns:
        - ì—‘ì…€: íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        - JSON: ë°ì´í„° ë°˜í™˜
    """
    try:
        storage = get_sqlite_storage()
        await storage.initialize()

        if format == "json":
            # JSON í˜•ì‹ ë°˜í™˜
            summary = await storage.get_deals_summary(days=days)

            # ì „ì²´ ë”œ ë°ì´í„°
            with storage.get_connection() as conn:
                cutoff_date = (datetime.now() - timedelta(days=days)).strftime("%Y-%m-%d")
                cursor = conn.execute("""
                    SELECT * FROM deals
                    WHERE DATE(snapshot_datetime) >= ?
                    ORDER BY snapshot_datetime DESC
                """, (cutoff_date,))
                all_deals = [dict(row) for row in cursor.fetchall()]

            return {
                "success": True,
                "summary": summary,
                "deals": all_deals,
                "export_date": datetime.now().isoformat(),
                "period_days": days
            }

        else:  # Excel
            # ì—‘ì…€ íŒŒì¼ ìƒì„±
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = f"./data/exports/Deals_Report_{timestamp}.xlsx"

            result = storage.export_deals_report(
                output_path=output_path,
                days=days
            )

            if not result.get("success"):
                raise HTTPException(status_code=500, detail=result.get("error", "Export failed"))

            file_path = Path(result["file_path"])
            if not file_path.exists():
                raise HTTPException(status_code=500, detail="Generated file not found")

            return FileResponse(
                path=str(file_path),
                media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                headers={"Content-Disposition": f"attachment; filename={file_path.name}"}
            )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Deals export error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Deals ë‚´ë³´ë‚´ê¸° ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤")


# ============= ì•Œë¦¼ ì„œë¹„ìŠ¤ API =============

from src.tools.alert_service import AlertService, get_alert_service


class AlertSendRequest(BaseModel):
    """ì•Œë¦¼ ë°œì†¡ ìš”ì²­"""
    alert_ids: Optional[List[int]] = None  # ë°œì†¡í•  ì•Œë¦¼ ID (ì—†ìœ¼ë©´ ë¯¸ë°œì†¡ ì „ì²´)


@app.get("/api/alerts/status")
async def get_alert_service_status():
    """ì•Œë¦¼ ì„œë¹„ìŠ¤ ìƒíƒœ ì¡°íšŒ"""
    try:
        service = get_alert_service()
        return {
            "success": True,
            **service.get_status()
        }
    except Exception as e:
        logging.error(f"Alert service status error: {e}")
        return {
            "success": False,
            "error": str(e)
        }


@app.post("/api/alerts/send")
async def send_pending_alerts(request: Optional[AlertSendRequest] = None):
    """
    ë¯¸ë°œì†¡ ì•Œë¦¼ ë°œì†¡

    íŠ¹ì • alert_idsë¥¼ ì§€ì •í•˜ë©´ í•´ë‹¹ ì•Œë¦¼ë§Œ, ì—†ìœ¼ë©´ ë¯¸ë°œì†¡ ì „ì²´ ë°œì†¡
    """
    try:
        storage = get_sqlite_storage()
        await storage.initialize()

        alert_service = get_alert_service()

        # ë¯¸ë°œì†¡ ì•Œë¦¼ ì¡°íšŒ
        unsent_alerts = await storage.get_unsent_alerts(limit=50)

        if not unsent_alerts:
            return {
                "success": True,
                "message": "No pending alerts to send",
                "sent_count": 0
            }

        # íŠ¹ì • ID í•„í„°ë§
        if request and request.alert_ids:
            unsent_alerts = [a for a in unsent_alerts if a.get("id") in request.alert_ids]

        if not unsent_alerts:
            return {
                "success": True,
                "message": "No matching alerts found",
                "sent_count": 0
            }

        # ì•Œë¦¼ ë°œì†¡
        sent_count = 0
        for alert in unsent_alerts:
            result = await alert_service.send_single_alert(alert)

            # ì„±ê³µ ì‹œ ë°œì†¡ ì™„ë£Œ í‘œì‹œ
            if result.get("slack") or result.get("email"):
                await storage.mark_alert_sent(alert["id"])
                sent_count += 1

        return {
            "success": True,
            "sent_count": sent_count,
            "total_pending": len(unsent_alerts),
            "channels": {
                "slack": alert_service._slack_enabled,
                "email": alert_service._email_enabled
            }
        }

    except Exception as e:
        logging.error(f"Alert send error: {e}")
        return {
            "success": False,
            "error": str(e),
            "sent_count": 0
        }


@app.post("/api/alerts/test")
async def send_test_alert():
    """í…ŒìŠ¤íŠ¸ ì•Œë¦¼ ë°œì†¡"""
    try:
        alert_service = get_alert_service()

        test_alert = {
            "alert_datetime": datetime.now().isoformat(),
            "brand": "TEST BRAND",
            "asin": "B000TEST01",
            "product_name": "Test Product - Alert System Verification",
            "deal_type": "lightning",
            "discount_percent": 50.0,
            "deal_price": 19.99,
            "original_price": 39.99,
            "time_remaining": "2h 30m",
            "claimed_percent": 45,
            "product_url": "https://amazon.com/dp/B000TEST01",
            "alert_type": "lightning_deal",
            "alert_message": "Test Alert - ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì•Œë¦¼ì…ë‹ˆë‹¤"
        }

        result = await alert_service.send_single_alert(test_alert)

        return {
            "success": True,
            "test_alert": test_alert,
            "send_result": result,
            "message": "Test alert sent successfully" if any(result.values()) else "No channels enabled"
        }

    except Exception as e:
        logging.error(f"Test alert error: {e}")
        return {
            "success": False,
            "error": str(e)
        }


# ============= Category KPI API =============

@app.get("/api/category/kpi")
async def get_category_kpi(
    category_id: str,
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    brand: str = "LANEIGE"
):
    """
    ì¹´í…Œê³ ë¦¬ë³„ KPI ë°ì´í„° ì¡°íšŒ (ê¸°ê°„ í•„í„°ë§ ì§€ì›)

    Args:
        category_id: ì¹´í…Œê³ ë¦¬ ID (beauty_personal_care, skin_care, lip_care, lip_makeup, face_powder)
        start_date: ì‹œì‘ì¼ (YYYY-MM-DD)
        end_date: ì¢…ë£Œì¼ (YYYY-MM-DD)
        brand: íƒ€ê²Ÿ ë¸Œëœë“œ (ê¸°ë³¸ê°’: LANEIGE)

    Returns:
        KPI ë°ì´í„°: sos, best_rank, cpi, new_competitors
    """
    try:
        # ë‚ ì§œ ë²”ìœ„ ì„¤ì •
        if not end_date:
            end_date = datetime.now().strftime("%Y-%m-%d")
        if not start_date:
            start_date = (datetime.now() - timedelta(days=7)).strftime("%Y-%m-%d")

        rows = []

        # SQLiteì—ì„œ ë°ì´í„° ì¡°íšŒ
        try:
            from src.tools.sqlite_storage import get_sqlite_storage
            sqlite = get_sqlite_storage()
            await sqlite.initialize()

            query = """
                SELECT snapshot_date, rank, brand, price
                FROM raw_data
                WHERE snapshot_date BETWEEN ? AND ?
                AND category_id = ?
                ORDER BY snapshot_date DESC, rank ASC
            """
            with sqlite.get_connection() as conn:
                cursor = conn.execute(query, (start_date, end_date, category_id))
                rows = cursor.fetchall()
        except Exception as db_err:
            logging.warning(f"SQLite query failed for category KPI: {db_err}")

        # JSON fallback
        if not rows:
            crawl_data = _load_crawl_data_for_sos()
            if crawl_data and crawl_data.get("categories", {}).get(category_id):
                cat_data = crawl_data["categories"][category_id]
                snapshot_date = crawl_data.get("snapshot_date", end_date)
                for product in cat_data.get("products", []):
                    rows.append((
                        snapshot_date,
                        product.get("rank", 100),
                        product.get("brand", "Unknown"),
                        product.get("price")
                    ))

        if not rows:
            return {
                "success": True,
                "message": f"í•´ë‹¹ ê¸°ê°„({start_date} ~ {end_date})ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.",
                "data": None,
                "period": {"start": start_date, "end": end_date}
            }

        # KPI ê³„ì‚°
        total_products = len(rows)
        brand_products = [r for r in rows if r[2] and brand.lower() in r[2].lower()]
        brand_count = len(brand_products)

        # SoS (Share of Shelf)
        sos = (brand_count / total_products * 100) if total_products > 0 else 0

        # Best Rank
        brand_ranks = [r[1] for r in brand_products if r[1]]
        best_rank = min(brand_ranks) if brand_ranks else None

        # CPI (Competitive Price Index) - ë¸Œëœë“œ í‰ê· ê°€ / ì „ì²´ í‰ê· ê°€ * 100
        brand_prices = [r[3] for r in brand_products if r[3] and r[3] > 0]
        all_prices = [r[3] for r in rows if r[3] and r[3] > 0]

        if brand_prices and all_prices:
            brand_avg_price = sum(brand_prices) / len(brand_prices)
            all_avg_price = sum(all_prices) / len(all_prices)
            cpi = (brand_avg_price / all_avg_price * 100) if all_avg_price > 0 else 100
        else:
            cpi = 100

        # New Competitors (ìµœê·¼ 7ì¼ ë‚´ ì‹ ê·œ ì§„ì… - ê°„ì†Œí™”ëœ ê³„ì‚°)
        # ì‹¤ì œë¡œëŠ” ì´ì „ ê¸°ê°„ ë°ì´í„°ì™€ ë¹„êµ í•„ìš”, ì—¬ê¸°ì„œëŠ” ì¶”ì •ê°’
        new_competitors = max(0, total_products - brand_count - 50)  # ê°„ì†Œí™”ëœ ì¶”ì •

        return {
            "success": True,
            "data": {
                "category_id": category_id,
                "sos": round(sos, 1),
                "best_rank": best_rank,
                "cpi": round(cpi, 0),
                "new_competitors": new_competitors,
                "brand": brand,
                "product_count": brand_count,
                "total_products": total_products
            },
            "period": {"start": start_date, "end": end_date}
        }

    except Exception as e:
        logging.error(f"Category KPI API error: {e}")
        return {
            "success": False,
            "error": str(e),
            "data": None
        }


# ============= SoS (Share of Shelf) API =============

def _load_crawl_data_for_sos():
    """JSON íŒŒì¼ì—ì„œ í¬ë¡¤ë§ ë°ì´í„° ë¡œë“œ (SQLite fallback)"""
    import json
    from pathlib import Path

    # latest_crawl_result.jsonì—ì„œ ë°ì´í„° ë¡œë“œ
    crawl_path = Path("./data/latest_crawl_result.json")
    if crawl_path.exists():
        with open(crawl_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    return None


@app.get("/api/sos/category")
async def get_sos_by_category(
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    compare_brands: Optional[str] = None  # comma-separated brand names
):
    """
    ì¹´í…Œê³ ë¦¬ë³„ SoS (Share of Shelf) ë°ì´í„° ì¡°íšŒ

    SoS = (í•´ë‹¹ ë¸Œëœë“œ ì œí’ˆ ìˆ˜ / Top 100) * 100

    Args:
        start_date: ì‹œì‘ì¼ (YYYY-MM-DD)
        end_date: ì¢…ë£Œì¼ (YYYY-MM-DD)
        compare_brands: ë¹„êµí•  ë¸Œëœë“œ (ì½¤ë§ˆë¡œ êµ¬ë¶„)

    Returns:
        ì¹´í…Œê³ ë¦¬ë³„ SoS ë°ì´í„°
    """
    try:
        # ë¹„êµ ë¸Œëœë“œ íŒŒì‹±
        compare_brand_list = []
        if compare_brands:
            compare_brand_list = [b.strip() for b in compare_brands.split(",") if b.strip()]

        # ë‚ ì§œ ë²”ìœ„ ì„¤ì •
        if not end_date:
            end_date = datetime.now().strftime("%Y-%m-%d")
        if not start_date:
            start_date = end_date

        # SQLite ë¨¼ì € ì‹œë„
        rows = []
        try:
            from src.tools.sqlite_storage import get_sqlite_storage
            sqlite = get_sqlite_storage()
            await sqlite.initialize()

            query = """
                SELECT snapshot_date, category_id, brand, COUNT(*) as product_count
                FROM raw_data
                WHERE snapshot_date BETWEEN ? AND ?
                GROUP BY snapshot_date, category_id, brand
                ORDER BY snapshot_date DESC, category_id, product_count DESC
            """
            with sqlite.get_connection() as conn:
                cursor = conn.execute(query, (start_date, end_date))
                rows = cursor.fetchall()
        except Exception as db_err:
            logging.warning(f"SQLite query failed, using JSON fallback: {db_err}")

        # SQLite ë°ì´í„° ì—†ìœ¼ë©´ JSON fallback
        if not rows:
            crawl_data = _load_crawl_data_for_sos()
            if crawl_data and crawl_data.get("categories"):
                # JSONì—ì„œ ë°ì´í„° ì¶”ì¶œ
                snapshot_date = crawl_data.get("snapshot_date", end_date)
                for cat_id, cat_data in crawl_data.get("categories", {}).items():
                    for product in cat_data.get("products", []):
                        brand = product.get("brand", "Unknown")
                        rows.append((snapshot_date, cat_id, brand, 1))

        if not rows:
            return {
                "success": True,
                "message": f"í•´ë‹¹ ê¸°ê°„({start_date} ~ {end_date})ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.",
                "data": [],
                "period": {"start": start_date, "end": end_date}
            }

        # ë°ì´í„° ì§‘ê³„
        # êµ¬ì¡°: {category_id: {brand: {dates: [count, ...], total_count: N}}}
        category_data = {}
        dates_set = set()

        for row in rows:
            if len(row) == 4:
                snapshot_date, category_id, brand, count = row
            else:
                snapshot_date, category_id, brand, count = row[0], row[1], row[2], row[3]
            dates_set.add(snapshot_date)

            if category_id not in category_data:
                category_data[category_id] = {}
            if brand not in category_data[category_id]:
                category_data[category_id][brand] = {"dates": {}, "total_count": 0}

            category_data[category_id][brand]["dates"][snapshot_date] = count
            category_data[category_id][brand]["total_count"] += count

        # SoS ê³„ì‚° (ê¸°ê°„ í‰ê· )
        num_dates = len(dates_set)
        result_data = []

        # ì¹´í…Œê³ ë¦¬ ê³„ì¸µ êµ¬ì¡° ë¡œë“œ
        hierarchy_path = Path("./config/category_hierarchy.json")
        hierarchy_data = {}
        if hierarchy_path.exists():
            with open(hierarchy_path, 'r', encoding='utf-8') as f:
                hierarchy_data = json.load(f).get('categories', {})

        # ì¹´í…Œê³ ë¦¬ ë©”íƒ€ ì •ë³´ (ê³„ì¸µ êµ¬ì¡° í¬í•¨)
        category_meta = {
            "beauty": {"name": "Beauty & Personal Care", "level": 0, "parent_id": None, "indent": 0, "order": 0},
            "skin_care": {"name": "Skin Care", "level": 1, "parent_id": "beauty", "indent": 1, "order": 1},
            "lip_care": {"name": "Lip Care", "level": 2, "parent_id": "skin_care", "indent": 2, "order": 2},
            "lip_makeup": {"name": "Lip Makeup", "level": 2, "parent_id": "makeup", "indent": 1, "order": 3},
            "face_powder": {"name": "Face Powder", "level": 3, "parent_id": "face_makeup", "indent": 2, "order": 4}
        }

        # hierarchy_dataì—ì„œ ì •ë³´ ì—…ë°ì´íŠ¸
        for cat_id, meta in category_meta.items():
            if cat_id in hierarchy_data:
                meta["name"] = hierarchy_data[cat_id].get("name", meta["name"])
                meta["level"] = hierarchy_data[cat_id].get("level", meta["level"])
                meta["parent_id"] = hierarchy_data[cat_id].get("parent_id", meta["parent_id"])

        for category_id, brands in category_data.items():
            # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ ì´ ì œí’ˆ ìˆ˜ (ê¸°ê°„ í•©ê³„)
            total_products_in_category = sum(b["total_count"] for b in brands.values())

            # LANEIGE SoS
            laneige_count = 0
            laneige_variants = ["LANEIGE", "Laneige", "laneige"]
            for variant in laneige_variants:
                if variant in brands:
                    laneige_count += brands[variant]["total_count"]

            laneige_sos = (laneige_count / total_products_in_category * 100) if total_products_in_category > 0 else 0

            # í‰ê·  SoS (ì „ì²´ ë¸Œëœë“œ ìˆ˜ ê¸°ì¤€)
            num_brands = len(brands)
            avg_sos = (100 / num_brands) if num_brands > 0 else 0

            # ë¹„êµ ë¸Œëœë“œ SoS
            compare_sos = {}
            for compare_brand in compare_brand_list:
                brand_count = 0
                for brand_name, brand_data in brands.items():
                    if compare_brand.lower() in brand_name.lower():
                        brand_count += brand_data["total_count"]
                compare_sos[compare_brand] = (brand_count / total_products_in_category * 100) if total_products_in_category > 0 else 0

            # LANEIGE ê°œë³„ ì œí’ˆ ë°ì´í„° (í•´ë‹¹ ì¹´í…Œê³ ë¦¬ ë‚´)
            laneige_products = []
            # ì œí’ˆë³„ ìƒì„¸ëŠ” ë³„ë„ ì¿¼ë¦¬ í•„ìš” - ì—¬ê¸°ì„œëŠ” ë¸Œëœë“œ ë ˆë²¨ë§Œ

            # ì¹´í…Œê³ ë¦¬ ë©”íƒ€ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            meta = category_meta.get(category_id, {"name": category_id, "level": 0, "parent_id": None, "indent": 0, "order": 99})

            result_data.append({
                "category_id": category_id,
                "category_name": meta["name"],
                "level": meta["level"],
                "parent_id": meta["parent_id"],
                "indent": meta["indent"],
                "order": meta["order"],
                "total_products": total_products_in_category // num_dates if num_dates > 0 else 0,
                "laneige_sos": round(laneige_sos, 2),
                "laneige_count": laneige_count // num_dates if num_dates > 0 else 0,
                "avg_sos": round(avg_sos, 2),
                "compare_brands": compare_sos,
                "num_dates": num_dates
            })

        # ê³„ì¸µ êµ¬ì¡° ìˆœì„œëŒ€ë¡œ ì •ë ¬
        result_data.sort(key=lambda x: x.get("order", 99))

        return {
            "success": True,
            "period": {
                "start": start_date,
                "end": end_date,
                "days": num_dates
            },
            "data": result_data,
            "compare_brands": compare_brand_list,
            "hierarchy_info": {
                "description": "ê° ì¹´í…Œê³ ë¦¬ëŠ” ìì²´ Top 100 ê¸°ì¤€ìœ¼ë¡œ ë…ë¦½ ê³„ì‚°ë©ë‹ˆë‹¤.",
                "note": "ìƒìœ„ ì¹´í…Œê³ ë¦¬ì™€ í•˜ìœ„ ì¹´í…Œê³ ë¦¬ì˜ SoSëŠ” ì„œë¡œ ë‹¤ë¥¸ ë­í‚¹ì—ì„œ ê³„ì‚°ë©ë‹ˆë‹¤."
            }
        }

    except Exception as e:
        logging.error(f"SoS category API error: {e}")
        return {
            "success": False,
            "error": str(e)
        }


@app.get("/api/sos/brands")
async def get_available_brands(
    category_id: Optional[str] = None,
    min_count: int = 1
):
    """
    ë¹„êµ ê°€ëŠ¥í•œ ë¸Œëœë“œ ëª©ë¡ ì¡°íšŒ (Top 100ì— í¬í•¨ëœ ë¸Œëœë“œë“¤)

    Args:
        category_id: íŠ¹ì • ì¹´í…Œê³ ë¦¬ë§Œ ì¡°íšŒ (ì„ íƒ)
        min_count: ìµœì†Œ ì œí’ˆ ìˆ˜ (ê¸°ë³¸: 1)

    Returns:
        ë¸Œëœë“œ ëª©ë¡ (ì œí’ˆ ìˆ˜ ê¸°ì¤€ ì •ë ¬)
    """
    try:
        end_date = datetime.now().strftime("%Y-%m-%d")
        start_date = (datetime.now() - timedelta(days=7)).strftime("%Y-%m-%d")

        rows = []
        # SQLite ë¨¼ì € ì‹œë„
        try:
            from src.tools.sqlite_storage import get_sqlite_storage
            sqlite = get_sqlite_storage()
            await sqlite.initialize()

            if category_id:
                query = """
                    SELECT brand, COUNT(DISTINCT asin) as product_count,
                           COUNT(DISTINCT snapshot_date) as days_present
                    FROM raw_data
                    WHERE snapshot_date BETWEEN ? AND ?
                    AND category_id = ?
                    GROUP BY brand
                    HAVING product_count >= ?
                    ORDER BY product_count DESC
                """
                params = (start_date, end_date, category_id, min_count)
            else:
                query = """
                    SELECT brand, COUNT(DISTINCT asin) as product_count,
                           COUNT(DISTINCT snapshot_date) as days_present
                    FROM raw_data
                    WHERE snapshot_date BETWEEN ? AND ?
                    GROUP BY brand
                    HAVING product_count >= ?
                    ORDER BY product_count DESC
                """
                params = (start_date, end_date, min_count)

            with sqlite.get_connection() as conn:
                cursor = conn.execute(query, params)
                rows = cursor.fetchall()
        except Exception as db_err:
            logging.warning(f"SQLite query failed for brands: {db_err}")

        # SQLite ë°ì´í„° ì—†ìœ¼ë©´ JSON fallback
        brands = []
        if not rows:
            crawl_data = _load_crawl_data_for_sos()
            if crawl_data and crawl_data.get("categories"):
                brand_counts = {}
                for cat_id, cat_data in crawl_data.get("categories", {}).items():
                    if category_id and cat_id != category_id:
                        continue
                    for product in cat_data.get("products", []):
                        brand = product.get("brand", "Unknown")
                        if brand:
                            brand_counts[brand] = brand_counts.get(brand, 0) + 1

                for brand_name, count in sorted(brand_counts.items(), key=lambda x: -x[1]):
                    if count >= min_count and brand_name.strip():
                        brands.append({
                            "name": brand_name,
                            "product_count": count,
                            "days_present": 1,
                            "is_laneige": "laneige" in brand_name.lower()
                        })
        else:
            for row in rows:
                brand_name, product_count, days_present = row
                if brand_name and brand_name.strip():
                    brands.append({
                        "name": brand_name,
                        "product_count": product_count,
                        "days_present": days_present,
                        "is_laneige": "laneige" in brand_name.lower()
                    })

        return {
            "success": True,
            "period": {"start": start_date, "end": end_date},
            "category_id": category_id,
            "brands": brands,
            "total_brands": len(brands)
        }

    except Exception as e:
        logging.error(f"SoS brands API error: {e}")
        return {
            "success": False,
            "error": str(e)
        }


@app.get("/api/sos/trend")
async def get_sos_trend(
    brand: str = "LANEIGE",
    category_id: Optional[str] = None,
    days: int = 7,
    start_date: Optional[str] = None,
    end_date: Optional[str] = None
):
    """
    ë¸Œëœë“œì˜ SoS ì¶”ì„¸ ë°ì´í„° (ì¼ë³„)

    Args:
        brand: ë¸Œëœë“œëª… (ê¸°ë³¸: LANEIGE)
        category_id: ì¹´í…Œê³ ë¦¬ (ì„ íƒ, ì—†ìœ¼ë©´ ì „ì²´)
        days: ì¡°íšŒ ê¸°ê°„ (ê¸°ë³¸: 7ì¼, start_date/end_dateê°€ ì—†ì„ ë•Œë§Œ ì‚¬ìš©)
        start_date: ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)
        end_date: ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)

    Returns:
        ì¼ë³„ SoS ì¶”ì„¸ ë°ì´í„°
    """
    try:
        from src.tools.sqlite_storage import get_sqlite_storage

        sqlite = get_sqlite_storage()
        await sqlite.initialize()

        # start_date/end_dateê°€ ì œê³µë˜ë©´ ì‚¬ìš©, ì•„ë‹ˆë©´ days ê¸°ë°˜ìœ¼ë¡œ ê³„ì‚°
        if start_date and end_date:
            pass  # ê·¸ëŒ€ë¡œ ì‚¬ìš©
        else:
            end_date = datetime.now().strftime("%Y-%m-%d")
            start_date = (datetime.now() - timedelta(days=days)).strftime("%Y-%m-%d")

        # ì¼ë³„ ì „ì²´ ì œí’ˆ ìˆ˜
        if category_id:
            total_query = """
                SELECT snapshot_date, COUNT(*) as total_count
                FROM raw_data
                WHERE snapshot_date BETWEEN ? AND ?
                AND category_id = ?
                GROUP BY snapshot_date
                ORDER BY snapshot_date
            """
            total_params = (start_date, end_date, category_id)

            brand_query = """
                SELECT snapshot_date, COUNT(*) as brand_count
                FROM raw_data
                WHERE snapshot_date BETWEEN ? AND ?
                AND category_id = ?
                AND LOWER(brand) LIKE ?
                GROUP BY snapshot_date
                ORDER BY snapshot_date
            """
            brand_params = (start_date, end_date, category_id, f"%{brand.lower()}%")
        else:
            total_query = """
                SELECT snapshot_date, COUNT(*) as total_count
                FROM raw_data
                WHERE snapshot_date BETWEEN ? AND ?
                GROUP BY snapshot_date
                ORDER BY snapshot_date
            """
            total_params = (start_date, end_date)

            brand_query = """
                SELECT snapshot_date, COUNT(*) as brand_count
                FROM raw_data
                WHERE snapshot_date BETWEEN ? AND ?
                AND LOWER(brand) LIKE ?
                GROUP BY snapshot_date
                ORDER BY snapshot_date
            """
            brand_params = (start_date, end_date, f"%{brand.lower()}%")

        with sqlite.get_connection() as conn:
            # ì „ì²´ ì¹´ìš´íŠ¸
            cursor = conn.execute(total_query, total_params)
            total_rows = cursor.fetchall()
            total_by_date = {row[0]: row[1] for row in total_rows}

            # ë¸Œëœë“œ ì¹´ìš´íŠ¸
            cursor = conn.execute(brand_query, brand_params)
            brand_rows = cursor.fetchall()
            brand_by_date = {row[0]: row[1] for row in brand_rows}

        # SoS ê³„ì‚°
        trend_data = []
        for date, total in sorted(total_by_date.items()):
            brand_count = brand_by_date.get(date, 0)
            sos = (brand_count / total * 100) if total > 0 else 0
            trend_data.append({
                "date": date,
                "total_products": total,
                "brand_count": brand_count,
                "sos": round(sos, 2)
            })

        return {
            "success": True,
            "brand": brand,
            "category_id": category_id,
            "period": {"start": start_date, "end": end_date, "days": days},
            "trend": trend_data
        }

    except Exception as e:
        logging.error(f"SoS trend API error: {e}")
        return {
            "success": False,
            "error": str(e)
        }


# ============= ì„œë²„ ì‹¤í–‰ =============

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)
