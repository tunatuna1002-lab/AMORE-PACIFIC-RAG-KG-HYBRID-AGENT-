"""
Dashboard API Server
ëŒ€ì‹œë³´ë“œìš© FastAPI ë°±ì—”ë“œ ì„œë²„

- ì±—ë´‡ API (ChatGPT + RAG + Ontology ì—°ë™)
- DOCX ì¸ì‚¬ì´íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±
- ëŒ€í™” ë©”ëª¨ë¦¬ ì§€ì›
- Audit Trail ë¡œê¹…
"""

import json
import os
import asyncio
import logging
from datetime import datetime
from typing import Dict, Any, List, Optional
from io import BytesIO
from collections import defaultdict
from pathlib import Path

from fastapi import FastAPI, HTTPException, Depends, Security, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from fastapi.security import APIKeyHeader
from pydantic import BaseModel
from dotenv import load_dotenv
from litellm import acompletion

# Rate Limiting
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded

from docx import Document
from docx.shared import Inches, Pt, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH
from docx.enum.table import WD_TABLE_ALIGNMENT

# RAG ì‹œìŠ¤í…œ ì—°ë™
from src.rag.router import RAGRouter, QueryType
from src.rag.retriever import DocumentRetriever

# Ontology ìŠ¤í‚¤ë§ˆ
from src.ontology.schema import ProductMetrics, BrandMetrics, MarketMetrics

# í†µí•© ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° (ì‹ ê·œ)
from src.core.unified_orchestrator import UnifiedOrchestrator, get_unified_orchestrator
from src.core.crawl_manager import get_crawl_manager, CrawlStatus

# Level 4 Brain (LLM-First Autonomous Agent)
from src.core.brain import UnifiedBrain, get_brain, get_initialized_brain, BrainMode, TaskPriority

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# Rate Limiter ì„¤ì • (IP ê¸°ë°˜)
limiter = Limiter(key_func=get_remote_address)

app = FastAPI(
    title="AMORE Dashboard API",
    description="LANEIGE Amazon ëŒ€ì‹œë³´ë“œ ë°±ì—”ë“œ API (RAG + Ontology í†µí•©)",
    version="2.0.0"
)

# Rate Limit ì´ˆê³¼ ì‹œ ì—ëŸ¬ í•¸ë“¤ëŸ¬
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

# CORS ì„¤ì • (í™˜ê²½ë³€ìˆ˜ë¡œ í—ˆìš© ë„ë©”ì¸ ì„¤ì • ê°€ëŠ¥)
ALLOWED_ORIGINS = os.getenv("ALLOWED_ORIGINS", "http://localhost:8001,http://127.0.0.1:8001").split(",")

app.add_middleware(
    CORSMiddleware,
    allow_origins=ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["GET", "POST", "DELETE", "OPTIONS"],
    allow_headers=["Content-Type", "X-API-Key", "Authorization"],
)

# ============= ì„œë²„ ì‹œì‘ ì‹œ ìë™ ìŠ¤ì¼€ì¤„ëŸ¬ =============

AUTO_START_SCHEDULER = os.getenv("AUTO_START_SCHEDULER", "true").lower() == "true"


@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ìë™ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ ë° ì¦‰ì‹œ í¬ë¡¤ë§ ì²´í¬"""
    # 1. ì¦‰ì‹œ í¬ë¡¤ë§ í•„ìš” ì—¬ë¶€ ì²´í¬ (ì„œë²„ ì¬ì‹œì‘ í›„ ì˜¤ëŠ˜ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ë°”ë¡œ í¬ë¡¤ë§)
    try:
        crawl_manager = get_crawl_manager()
        if crawl_manager.needs_crawl():
            logging.info(f"ì„œë²„ ì‹œì‘: ì˜¤ëŠ˜({crawl_manager.get_kst_today()}) ë°ì´í„° ì—†ìŒ â†’ í¬ë¡¤ë§ ì‹œì‘")
            await crawl_manager.start_crawl()
        else:
            logging.info(f"ì„œë²„ ì‹œì‘: ì˜¤ëŠ˜ ë°ì´í„° ìˆìŒ ë˜ëŠ” í¬ë¡¤ë§ ì¤‘ (data_date={crawl_manager.get_data_date()})")
    except Exception as e:
        logging.error(f"ì„œë²„ ì‹œì‘ í¬ë¡¤ë§ ì²´í¬ ì‹¤íŒ¨: {e}")

    # 2. ììœ¨ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ (ë§¤ì¼ 06:00 ì •ê¸° í¬ë¡¤ë§ìš©)
    if AUTO_START_SCHEDULER:
        try:
            brain = await get_initialized_brain()
            await brain.start_scheduler()
            logging.info("ììœ¨ ìŠ¤ì¼€ì¤„ëŸ¬ ìë™ ì‹œì‘ ì™„ë£Œ (ë§¤ì¼ í•œêµ­ì‹œê°„ 06:00 í¬ë¡¤ë§)")
        except Exception as e:
            logging.error(f"ììœ¨ ìŠ¤ì¼€ì¤„ëŸ¬ ìë™ ì‹œì‘ ì‹¤íŒ¨: {e}")

# ë°ì´í„° ê²½ë¡œ
DATA_PATH = "./data/dashboard_data.json"
DOCS_PATH = "./"  # MD íŒŒì¼ë“¤ì´ ë£¨íŠ¸ì— ìˆìŒ
AUDIT_LOG_DIR = "./logs"

# ============= API Key ì¸ì¦ ì„¤ì • =============

# API_KEY: í™˜ê²½ë³€ìˆ˜ í•„ìˆ˜ - ê¸°ë³¸ê°’ ì—†ìŒ (ë³´ì•ˆìƒ í•˜ë“œì½”ë”© ê¸ˆì§€)
API_KEY = os.getenv("API_KEY")
if not API_KEY:
    logging.warning("âš ï¸ API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë³´í˜¸ëœ ì—”ë“œí¬ì¸íŠ¸ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
api_key_header = APIKeyHeader(name="X-API-Key", auto_error=False)


async def verify_api_key(api_key: str = Security(api_key_header)):
    """
    API Key ê²€ì¦ (ë¯¼ê°í•œ ì—”ë“œí¬ì¸íŠ¸ìš©)

    ì‚¬ìš©ë²•: ì—”ë“œí¬ì¸íŠ¸ì— dependencies=[Depends(verify_api_key)] ì¶”ê°€
    """
    if api_key is None:
        raise HTTPException(
            status_code=401,
            detail="API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤. í—¤ë”ì— X-API-Keyë¥¼ ì¶”ê°€í•˜ì„¸ìš”."
        )
    if api_key != API_KEY:
        raise HTTPException(
            status_code=403,
            detail="ìœ íš¨í•˜ì§€ ì•Šì€ API Keyì…ë‹ˆë‹¤."
        )
    return api_key


# ============= Audit Trail Logger ì„¤ì • =============

def setup_audit_logger():
    """Audit Trail ë¡œê±° ì„¤ì •"""
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    Path(AUDIT_LOG_DIR).mkdir(parents=True, exist_ok=True)

    # ì˜¤ëŠ˜ ë‚ ì§œ ê¸°ë°˜ ë¡œê·¸ íŒŒì¼
    today = datetime.now().strftime("%Y-%m-%d")
    log_file = Path(AUDIT_LOG_DIR) / f"chatbot_audit_{today}.log"

    # ë¡œê±° ìƒì„±
    audit_logger = logging.getLogger("audit_trail")
    audit_logger.setLevel(logging.INFO)

    # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±° (ì¤‘ë³µ ë°©ì§€)
    audit_logger.handlers.clear()

    # íŒŒì¼ í•¸ë“¤ëŸ¬ ì¶”ê°€
    file_handler = logging.FileHandler(log_file, encoding="utf-8")
    file_handler.setLevel(logging.INFO)

    # í¬ë§· ì„¤ì •
    formatter = logging.Formatter(
        '%(asctime)s | %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    file_handler.setFormatter(formatter)
    audit_logger.addHandler(file_handler)

    return audit_logger

audit_logger = setup_audit_logger()

def log_chat_interaction(
    session_id: str,
    user_query: str,
    ai_response: str,
    query_type: str,
    confidence: float,
    entities: Dict,
    sources: List[str],
    response_time_ms: float
):
    """ì±—ë´‡ ëŒ€í™” Audit Trail ê¸°ë¡"""
    audit_entry = {
        "session_id": session_id,
        "timestamp": datetime.now().isoformat(),
        "user_query": user_query,
        "ai_response": ai_response[:500] + "..." if len(ai_response) > 500 else ai_response,
        "query_type": query_type,
        "confidence": round(confidence, 4),
        "entities": entities,
        "sources": sources,
        "response_time_ms": round(response_time_ms, 2)
    }

    # JSON í˜•ì‹ìœ¼ë¡œ ë¡œê·¸ ê¸°ë¡
    audit_logger.info(json.dumps(audit_entry, ensure_ascii=False))

# ============= Global Instances =============

# RAG ì‹œìŠ¤í…œ
rag_router = RAGRouter()
doc_retriever = DocumentRetriever(DOCS_PATH)

# ì„¸ì…˜ë³„ ëŒ€í™” ë©”ëª¨ë¦¬ (TTL ê¸°ë°˜ ìë™ ì •ë¦¬)
conversation_memory: Dict[str, List[Dict[str, str]]] = defaultdict(list)
session_last_activity: Dict[str, datetime] = {}  # ì„¸ì…˜ë³„ ë§ˆì§€ë§‰ í™œë™ ì‹œê°„
MAX_MEMORY_TURNS = 10
SESSION_TTL_HOURS = 1  # ì„¸ì…˜ ë§Œë£Œ ì‹œê°„ (1ì‹œê°„)
MAX_SESSIONS = 1000  # ìµœëŒ€ ì„¸ì…˜ ìˆ˜


def cleanup_expired_sessions() -> int:
    """ë§Œë£Œëœ ì„¸ì…˜ ì •ë¦¬ (TTL ê¸°ë°˜)"""
    now = datetime.now()
    expired = [
        sid for sid, last_time in session_last_activity.items()
        if (now - last_time).total_seconds() > SESSION_TTL_HOURS * 3600
    ]
    for sid in expired:
        if sid in conversation_memory:
            del conversation_memory[sid]
        if sid in session_last_activity:
            del session_last_activity[sid]
    return len(expired)

# í†µí•© ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì¸ìŠ¤í„´ìŠ¤ëŠ” get_unified_orchestrator()ë¡œ ê´€ë¦¬ë¨


# ============= Pydantic Models =============

class ChatRequest(BaseModel):
    """ì±—ë´‡ ìš”ì²­"""
    message: str
    session_id: Optional[str] = "default"
    context: Optional[Dict] = None


class ChatResponse(BaseModel):
    """ì±—ë´‡ ì‘ë‹µ"""
    response: str
    query_type: str
    confidence: float
    sources: List[str]
    suggestions: List[str]
    entities: Dict[str, Any]


class ExportRequest(BaseModel):
    """ë‚´ë³´ë‚´ê¸° ìš”ì²­"""
    start_date: Optional[str] = None
    end_date: Optional[str] = None
    include_strategy: bool = True


# ============= Helper Functions =============

def load_dashboard_data() -> Dict[str, Any]:
    """ëŒ€ì‹œë³´ë“œ ë°ì´í„° ë¡œë“œ"""
    try:
        with open(DATA_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        return {}


def get_conversation_history(session_id: str, limit: int = 5) -> str:
    """ëŒ€í™” ê¸°ë¡ ì¡°íšŒ (ë¬¸ìì—´ í˜•íƒœ)"""
    history = conversation_memory.get(session_id, [])[-limit:]
    if not history:
        return ""

    lines = []
    for turn in history:
        role = "ì‚¬ìš©ì" if turn["role"] == "user" else "AI"
        content = turn["content"][:150] + "..." if len(turn["content"]) > 150 else turn["content"]
        lines.append(f"[{role}]: {content}")

    return "\n".join(lines)


def add_to_memory(session_id: str, role: str, content: str) -> None:
    """ëŒ€í™” ë©”ëª¨ë¦¬ì— ì¶”ê°€"""
    now = datetime.now()

    # ì£¼ê¸°ì  ì •ë¦¬ (ë§¤ 100ë²ˆì§¸ í˜¸ì¶œë§ˆë‹¤ ë˜ëŠ” ì„¸ì…˜ ìˆ˜ ì´ˆê³¼ ì‹œ)
    if len(session_last_activity) > MAX_SESSIONS or len(session_last_activity) % 100 == 0:
        cleanup_expired_sessions()

    # ë§ˆì§€ë§‰ í™œë™ ì‹œê°„ ì—…ë°ì´íŠ¸
    session_last_activity[session_id] = now

    conversation_memory[session_id].append({
        "role": role,
        "content": content,
        "timestamp": now.isoformat()
    })
    # ìµœëŒ€ ê°œìˆ˜ ìœ ì§€
    if len(conversation_memory[session_id]) > MAX_MEMORY_TURNS * 2:
        conversation_memory[session_id] = conversation_memory[session_id][-MAX_MEMORY_TURNS * 2:]


def build_data_context(data: Dict, query_type: QueryType, entities: Dict) -> str:
    """
    ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (Ontology ê¸°ë°˜)

    ì§ˆë¬¸ ìœ í˜•ê³¼ ì¶”ì¶œëœ ì—”í‹°í‹°ì— ë”°ë¼ í•„ìš”í•œ ë°ì´í„°ë§Œ ì„ íƒ
    """
    if not data:
        return "í˜„ì¬ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

    context_parts = []

    # ë©”íƒ€ë°ì´í„° (í•­ìƒ í¬í•¨)
    metadata = data.get("metadata", {})
    context_parts.append(f"""[ë°ì´í„° í˜„í™©]
- ê¸°ì¤€ì¼: {metadata.get('data_date', 'N/A')}
- ì´ ì œí’ˆ ìˆ˜: {metadata.get('total_products', 0)}ê°œ
- LANEIGE ì œí’ˆ ìˆ˜: {metadata.get('laneige_products', 0)}ê°œ""")

    # ì§ˆë¬¸ ìœ í˜•ë³„ ë°ì´í„° ì„ íƒ
    brand_kpis = data.get("brand", {}).get("kpis", {})

    # ì‹œì¥/ë¸Œëœë“œ ì§€í‘œ (DEFINITION, INTERPRETATION, ANALYSIS)
    if query_type in [QueryType.DEFINITION, QueryType.INTERPRETATION, QueryType.ANALYSIS, QueryType.COMBINATION]:
        if brand_kpis:
            context_parts.append(f"""
[LANEIGE ë¸Œëœë“œ KPI] (Ontology: BrandMetrics)
- SoS (Share of Shelf): {brand_kpis.get('sos', 0)}% {brand_kpis.get('sos_delta', '')}
- Top 10 ì œí’ˆ ìˆ˜: {brand_kpis.get('top10_count', 0)}ê°œ
- í‰ê·  ìˆœìœ„: {brand_kpis.get('avg_rank', 0)}ìœ„
- HHI (ì‹œì¥ ì§‘ì¤‘ë„): {brand_kpis.get('hhi', 0)}""")

    # ê²½ìŸì‚¬ ì •ë³´ (ANALYSIS, DATA_QUERYì—ì„œ ê²½ìŸì‚¬ ì–¸ê¸‰ ì‹œ)
    competitors = data.get("brand", {}).get("competitors", [])
    brands_mentioned = entities.get("brands", [])

    if query_type == QueryType.ANALYSIS or any(b for b in brands_mentioned if b.lower() != "laneige"):
        if competitors:
            top_comps = competitors[:5]
            comp_lines = [f"  - {c['brand']}: SoS {c['sos']}%, í‰ê·  ìˆœìœ„ {c['avg_rank']}ìœ„, ì œí’ˆ {c['product_count']}ê°œ" for c in top_comps]
            context_parts.append("[ê²½ìŸì‚¬ í˜„í™©]\n" + "\n".join(comp_lines))

    # ì œí’ˆ ì •ë³´ (DATA_QUERY, íŠ¹ì • ì œí’ˆ ì–¸ê¸‰ ì‹œ)
    products = data.get("products", {})
    products_mentioned = entities.get("products", [])

    if query_type == QueryType.DATA_QUERY or products_mentioned:
        if products:
            prod_lines = []
            for asin, p in list(products.items())[:5]:
                prod_lines.append(f"""  - {p['name'][:40]}
    ìˆœìœ„: #{p['rank']} ({p['rank_delta']}), í‰ì : {p['rating']}, ë³€ë™ì„±: {p.get('volatility_status', 'N/A')}""")
            context_parts.append("[LANEIGE ì œí’ˆ í˜„í™©] (Ontology: ProductMetrics)\n" + "\n".join(prod_lines))

    # ì¹´í…Œê³ ë¦¬ ì •ë³´
    categories = data.get("categories", {})
    categories_mentioned = entities.get("categories", [])

    if categories_mentioned or query_type in [QueryType.ANALYSIS, QueryType.INTERPRETATION]:
        if categories:
            cat_lines = []
            for cat_id, cat in categories.items():
                cat_lines.append(f"  - {cat['name']}: SoS {cat['sos']}%, ìµœê³  ìˆœìœ„ #{cat['best_rank']}, CPI {cat.get('cpi', 100)}")
            context_parts.append("[ì¹´í…Œê³ ë¦¬ í˜„í™©] (Ontology: MarketMetrics)\n" + "\n".join(cat_lines))

    # ì•¡ì…˜ ì•„ì´í…œ (ì „ëµ ì§ˆë¬¸)
    if query_type == QueryType.ANALYSIS:
        action_items = data.get("home", {}).get("action_items", [])
        if action_items:
            action_lines = [f"  - [{a['priority']}] {a['product_name']}: {a['signal']} â†’ {a['action_tag']}" for a in action_items[:4]]
            context_parts.append("[í˜„ì¬ ì•¡ì…˜ ì•„ì´í…œ]\n" + "\n".join(action_lines))

    return "\n\n".join(context_parts)


async def get_rag_context(query: str, query_type: QueryType) -> tuple[str, List[str]]:
    """
    RAG ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰

    Returns:
        (ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´, ì¶œì²˜ ëª©ë¡)
    """
    # DocumentRetriever ì´ˆê¸°í™” (ì²˜ìŒ í˜¸ì¶œ ì‹œ)
    if not doc_retriever._initialized:
        await doc_retriever.initialize()

    # ì§ˆë¬¸ ìœ í˜•ì— ë§ëŠ” ë¬¸ì„œ ê²€ìƒ‰
    target_doc = rag_router.get_target_document(query_type)

    # ê²€ìƒ‰ ì‹¤í–‰
    results = await doc_retriever.search(query, top_k=3, doc_filter=target_doc)

    if not results:
        return "", []

    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context_parts = []
    sources = []

    for result in results:
        metadata = result.get("metadata", {})
        content = result.get("content", "")
        title = metadata.get("title", "Unknown")
        doc_id = metadata.get("doc_id", "")

        context_parts.append(f"[{title}]\n{content}")

        # ì¶œì²˜ ì¶”ê°€
        doc_name_map = {
            "strategic_indicators": "Strategic Indicators Definition",
            "metric_interpretation": "Metric Interpretation Guide",
            "indicator_combination": "Indicator Combination Playbook",
            "home_insight_rules": "Home Page Insight Rules"
        }
        if doc_id in doc_name_map and doc_name_map[doc_id] not in sources:
            sources.append(doc_name_map[doc_id])

    return "\n\n---\n\n".join(context_parts), sources


def get_dynamic_suggestions(query_type: QueryType, entities: Dict, response: str) -> List[str]:
    """
    ë™ì  í›„ì† ì§ˆë¬¸ ì œì•ˆ

    ì‘ë‹µ ë‚´ìš©ê³¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë§ì¶¤í˜• ì œì•ˆ ìƒì„±
    """
    suggestions = []

    # ì—”í‹°í‹° ê¸°ë°˜ ì œì•ˆ
    brands = entities.get("brands", [])
    indicators = entities.get("indicators", [])
    categories = entities.get("categories", [])

    if query_type == QueryType.DEFINITION:
        # ì •ì˜ ì§ˆë¬¸ â†’ í•´ì„/í™œìš© ì§ˆë¬¸ ì œì•ˆ
        if indicators:
            ind = indicators[0].upper()
            suggestions.append(f"{ind}ê°€ ë†’ìœ¼ë©´ ì–´ë–¤ ì˜ë¯¸ì¸ê°€ìš”?")
            suggestions.append(f"{ind} ê°œì„ ì„ ìœ„í•œ ì „ëµì€?")
        suggestions.append("ë‹¤ë¥¸ ì§€í‘œì™€ í•¨ê»˜ í•´ì„í•˜ë©´ ì–´ë–¤ê°€ìš”?")

    elif query_type == QueryType.INTERPRETATION:
        # í•´ì„ ì§ˆë¬¸ â†’ ì•¡ì…˜/ì¡°í•© ì œì•ˆ
        suggestions.append("í˜„ì¬ LANEIGE ìˆ˜ì¹˜ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”")
        suggestions.append("ê²½ìŸì‚¬ì™€ ë¹„êµí•˜ë©´ ì–´ë–¤ê°€ìš”?")
        suggestions.append("ê°œì„ ì„ ìœ„í•œ ì•¡ì…˜ ì•„ì´í…œì€?")

    elif query_type == QueryType.DATA_QUERY:
        # ë°ì´í„° ì¡°íšŒ â†’ ì‹¬í™” ë¶„ì„ ì œì•ˆ
        suggestions.append("ì´ ìˆ˜ì¹˜ê°€ ì¢‹ì€ ê±´ê°€ìš”?")
        suggestions.append("ìµœê·¼ 7ì¼ ì¶”ì´ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”")
        suggestions.append("ê²½ìŸì‚¬ ëŒ€ë¹„ ì–´ë–¤ê°€ìš”?")

    elif query_type == QueryType.ANALYSIS:
        # ë¶„ì„ â†’ ì „ëµ/ì•¡ì…˜ ì œì•ˆ
        suggestions.append("ê°€ì¥ ì‹œê¸‰í•œ ì•¡ì…˜ì€ ë¬´ì—‡ì¸ê°€ìš”?")
        suggestions.append("Top 10 ì§„ì…ì„ ìœ„í•œ ì „ëµì€?")
        suggestions.append("ë¦¬ìŠ¤í¬ ìš”ì¸ì´ ìˆë‚˜ìš”?")

    elif query_type == QueryType.COMBINATION:
        # ì¡°í•© ì§ˆë¬¸ â†’ ë‹¤ë¥¸ ì¡°í•© ì œì•ˆ
        suggestions.append("ë‹¤ë¥¸ ì‹œë‚˜ë¦¬ì˜¤ë„ ë¶„ì„í•´ì£¼ì„¸ìš”")
        suggestions.append("í˜„ì¬ ë°ì´í„°ì—ì„œ í•´ë‹¹ ìƒí™©ì´ ìˆë‚˜ìš”?")

    else:
        # ê¸°ë³¸ ì œì•ˆ
        suggestions = [
            "SoS(ì ìœ ìœ¨)ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”",
            "í˜„ì¬ LANEIGE ìˆœìœ„ëŠ”?",
            "ì „ëµì  ê¶Œê³ ì‚¬í•­ì´ ìˆë‚˜ìš”?"
        ]

    return suggestions[:3]


# ============= API Endpoints =============

@app.get("/")
async def root():
    """í—¬ìŠ¤ ì²´í¬"""
    return {
        "status": "ok",
        "message": "AMORE Dashboard API v2.0 (RAG + Ontology)",
        "features": ["chatbot", "rag", "ontology", "memory", "docx_export"]
    }


@app.get("/api/data")
async def get_data():
    """ëŒ€ì‹œë³´ë“œ ë°ì´í„° ì¡°íšŒ"""
    data = load_dashboard_data()
    if not data:
        raise HTTPException(status_code=404, detail="Dashboard data not found")
    return data


@app.post("/api/chat", response_model=ChatResponse)
@limiter.limit("30/minute")  # ë¶„ë‹¹ 30íšŒ ì œí•œ
async def chat(request: ChatRequest, req: Request):
    """
    ChatGPT + RAG + Ontology í†µí•© ì±—ë´‡ API

    1. ì§ˆë¬¸ ë¶„ì„ (RAGRouter)
    2. ì—”í‹°í‹° ì¶”ì¶œ (Ontology ê¸°ë°˜)
    3. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ (RAG)
    4. ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    5. ëŒ€í™” ê¸°ë¡ ì°¸ì¡°
    6. LLM ì‘ë‹µ ìƒì„±
    7. Audit Trail ë¡œê¹…
    """
    import time
    start_time = time.time()

    message = request.message.strip()
    session_id = request.session_id or "default"

    if not message:
        raise HTTPException(status_code=400, detail="Message is required")

    # 1. ì§ˆë¬¸ ë¶„ë¥˜ (RAGRouter ì‚¬ìš©)
    route_result = rag_router.route(message)
    query_type = route_result["query_type"]
    confidence = route_result["confidence"]

    # 2. ì—”í‹°í‹° ì¶”ì¶œ (Ontology ê¸°ë°˜)
    entities = rag_router.extract_entities(message)

    # 3. ëª…í™•í™” í•„ìš” ì—¬ë¶€ í™•ì¸
    clarification = rag_router.needs_clarification(route_result, entities)
    if clarification and confidence < 0.5:
        # ëª…í™•í™” ìš”ì²­
        add_to_memory(session_id, "user", message)
        add_to_memory(session_id, "assistant", clarification)

        return ChatResponse(
            response=clarification,
            query_type=query_type.value if hasattr(query_type, 'value') else str(query_type),
            confidence=confidence,
            sources=[],
            suggestions=["ì˜ˆ, ì „ì²´ ë¸Œëœë“œ ë¶„ì„í•´ì£¼ì„¸ìš”", "LANEIGEë§Œ ë¶„ì„í•´ì£¼ì„¸ìš”", "Lip Care ì¹´í…Œê³ ë¦¬ë§Œ"],
            entities=entities
        )

    # 4. RAG ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
    rag_context, sources = await get_rag_context(message, query_type)

    # 5. ë°ì´í„° ë¡œë“œ ë° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    data = load_dashboard_data()
    data_context = build_data_context(data, query_type, entities)

    # 6. ëŒ€í™” ê¸°ë¡ ì¡°íšŒ
    conversation_history = get_conversation_history(session_id)

    # 7. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    system_prompt = """ë‹¹ì‹ ì€ AMORE Pacificì˜ LANEIGE ë¸Œëœë“œ Amazon ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì—­í• :
- Amazon US ë² ìŠ¤íŠ¸ì…€ëŸ¬ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì¸ì‚¬ì´íŠ¸ ì œê³µ
- LANEIGE ë¸Œëœë“œì˜ ì‹œì¥ í¬ì§€ì…˜ ë¶„ì„
- ê²½ìŸì‚¬ ëŒ€ë¹„ ì „ëµì  ê¶Œê³  ì œê³µ
- ì§€í‘œ ì •ì˜ ë° í•´ì„ ê°€ì´ë“œ ì œê³µ

Ontology ì—”í‹°í‹° ì´í•´:
- Brand: ë¸Œëœë“œ ì •ë³´ (LANEIGE, ê²½ìŸì‚¬ ë“±)
- Product: ì œí’ˆ ì •ë³´ (ASIN, ìˆœìœ„, í‰ì , ê°€ê²© ë“±)
- Category: ì¹´í…Œê³ ë¦¬ (Lip Care, Skin Care ë“±)
- BrandMetrics: SoS, í‰ê· ìˆœìœ„, ì œí’ˆìˆ˜ ë“±
- ProductMetrics: ìˆœìœ„ë³€ë™ì„±, ì—°ì†ì²´ë¥˜ì¼, í‰ì ì¶”ì„¸ ë“±
- MarketMetrics: HHI(ì‹œì¥ì§‘ì¤‘ë„), êµì²´ìœ¨ ë“±

ì‘ë‹µ ê°€ì´ë“œë¼ì¸:
1. ë°ì´í„°ì— ê¸°ë°˜í•œ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ ì¸ìš©
2. RAG ë¬¸ì„œì˜ ì •ì˜/í•´ì„ ê¸°ì¤€ í™œìš©
3. ì´ì „ ëŒ€í™” ë§¥ë½ ê³ ë ¤
4. ê°„ê²°í•˜ê³  ì•¡ì…˜ ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸ ì œê³µ
5. ë¶ˆí™•ì‹¤í•œ ê²½ìš° ëª…í™•íˆ ë°í ê²ƒ
6. ë‹¨ì •ì  í‘œí˜„ í”¼í•˜ê¸°
7. í•œêµ­ì–´ë¡œ ì‘ë‹µ

ì§ˆë¬¸ ìœ í˜•ë³„ ì‘ë‹µ ìŠ¤íƒ€ì¼:
- ì •ì˜(DEFINITION): ì§€í‘œì˜ ì •ì˜, ì‚°ì¶œì‹, ì˜ë¯¸ë¥¼ ì„¤ëª…
- í•´ì„(INTERPRETATION): ìˆ˜ì¹˜ì˜ ì˜ë¯¸, ì¢‹ê³  ë‚˜ì¨ì˜ ê¸°ì¤€ ì„¤ëª…
- ì¡°í•©(COMBINATION): ì—¬ëŸ¬ ì§€í‘œë¥¼ í•¨ê»˜ í•´ì„, ì‹œë‚˜ë¦¬ì˜¤ë³„ ì•¡ì…˜ ì œì•ˆ
- ë°ì´í„°ì¡°íšŒ(DATA_QUERY): í˜„ì¬ ìˆ˜ì¹˜ì™€ ë³€ë™ í˜„í™© ì•ˆë‚´
- ë¶„ì„(ANALYSIS): ì¢…í•© ë¶„ì„ê³¼ ì „ëµì  ê¶Œê³  ì œê³µ
"""

    # 8. ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    user_prompt = f"""## ì‚¬ìš©ì ì§ˆë¬¸
{message}

## ì§ˆë¬¸ ìœ í˜•
{query_type.value if hasattr(query_type, 'value') else str(query_type)} (ì‹ ë¢°ë„: {confidence:.1%})

## ì¶”ì¶œëœ ì—”í‹°í‹°
- ë¸Œëœë“œ: {', '.join(entities.get('brands', [])) or 'ì—†ìŒ'}
- ì¹´í…Œê³ ë¦¬: {', '.join(entities.get('categories', [])) or 'ì—†ìŒ'}
- ì§€í‘œ: {', '.join(entities.get('indicators', [])) or 'ì—†ìŒ'}
- ê¸°ê°„: {entities.get('time_range') or 'ì—†ìŒ'}

## RAG ì°¸ì¡° ë¬¸ì„œ
{rag_context if rag_context else 'ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ'}

## í˜„ì¬ ë°ì´í„°
{data_context}

## ì´ì „ ëŒ€í™”
{conversation_history if conversation_history else 'ì´ì „ ëŒ€í™” ì—†ìŒ'}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
- ì§ˆë¬¸ ìœ í˜•ì— ë§ëŠ” ì‘ë‹µ ìŠ¤íƒ€ì¼ì„ ì‚¬ìš©í•˜ì„¸ìš”.
- RAG ë¬¸ì„œì— ê´€ë ¨ ì •ì˜/í•´ì„ì´ ìˆìœ¼ë©´ ì¸ìš©í•˜ì„¸ìš”.
- ì´ì „ ëŒ€í™” ë§¥ë½ì´ ìˆìœ¼ë©´ ê³ ë ¤í•˜ì„¸ìš”.
"""

    try:
        response = await acompletion(
            model="gpt-4.1-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.3,
            max_tokens=1000
        )

        answer = response.choices[0].message.content

        # 9. ëŒ€í™” ë©”ëª¨ë¦¬ì— ì €ì¥
        add_to_memory(session_id, "user", message)
        add_to_memory(session_id, "assistant", answer)

        # 10. ë™ì  í›„ì† ì§ˆë¬¸ ì œì•ˆ
        suggestions = get_dynamic_suggestions(query_type, entities, answer)

        # 11. Audit Trail ë¡œê¹…
        response_time_ms = (time.time() - start_time) * 1000
        log_chat_interaction(
            session_id=session_id,
            user_query=message,
            ai_response=answer,
            query_type=query_type.value if hasattr(query_type, 'value') else str(query_type),
            confidence=confidence,
            entities=entities,
            sources=sources,
            response_time_ms=response_time_ms
        )

        return ChatResponse(
            response=answer,
            query_type=query_type.value if hasattr(query_type, 'value') else str(query_type),
            confidence=confidence,
            sources=sources,
            suggestions=suggestions,
            entities=entities
        )

    except Exception as e:
        print(f"LLM Error: {e}")

        # Fallback ì‘ë‹µ
        fallback = route_result.get("fallback_message") or rag_router.get_fallback_response("unknown")

        # ë°ì´í„° ê¸°ë°˜ ê¸°ë³¸ ì‘ë‹µ ì¶”ê°€
        if data and query_type == QueryType.DATA_QUERY:
            brand_kpis = data.get("brand", {}).get("kpis", {})
            fallback = f"""í˜„ì¬ LANEIGE í˜„í™©:
- SoS: {brand_kpis.get('sos', 0)}%
- Top 10 ì œí’ˆ: {brand_kpis.get('top10_count', 0)}ê°œ
- í‰ê·  ìˆœìœ„: {brand_kpis.get('avg_rank', 0)}ìœ„

(ìƒì„¸ ë¶„ì„ì„ ìœ„í•´ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”)"""

        # Fallback ì‘ë‹µë„ Audit Trail ê¸°ë¡
        response_time_ms = (time.time() - start_time) * 1000
        log_chat_interaction(
            session_id=session_id,
            user_query=message,
            ai_response=f"[ERROR] {str(e)[:100]} | Fallback: {fallback[:200]}",
            query_type=query_type.value if hasattr(query_type, 'value') else str(query_type),
            confidence=0.0,
            entities=entities,
            sources=["fallback"],
            response_time_ms=response_time_ms
        )

        return ChatResponse(
            response=fallback,
            query_type=query_type.value if hasattr(query_type, 'value') else str(query_type),
            confidence=0.0,
            sources=[],
            suggestions=["ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”", "SoSê°€ ë­”ê°€ìš”?", "í˜„ì¬ ìˆœìœ„ ì•Œë ¤ì£¼ì„¸ìš”"],
            entities=entities
        )


@app.delete("/api/chat/memory/{session_id}")
async def clear_memory(session_id: str):
    """ì„¸ì…˜ ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"""
    if session_id in conversation_memory:
        del conversation_memory[session_id]
    return {"status": "ok", "message": f"Session {session_id} memory cleared"}


# ============= Simple Chat API (v3 - ë‹¨ìˆœí™”) =============

from src.core.simple_chat import get_chat_service


class SimpleChatRequest(BaseModel):
    """Simple Chat ìš”ì²­"""
    message: str
    session_id: Optional[str] = "default"


class SimpleChatResponse(BaseModel):
    """Simple Chat ì‘ë‹µ"""
    text: str
    suggestions: List[str]
    tools_used: List[str]
    sources: List[Dict[str, Any]] = []  # AI ì¶œì²˜ ì •ë³´ ì¶”ê°€
    data_date: str
    processing_time_ms: float


@app.post("/api/v3/chat", response_model=SimpleChatResponse)
@limiter.limit("30/minute")  # ë¶„ë‹¹ 30íšŒ ì œí•œ
async def chat_v3(request: SimpleChatRequest, req: Request):
    """
    Simple LLM Chat API (v3)

    ë‹¨ìˆœí™”ëœ êµ¬ì¡°:
    - LLMì´ ëª¨ë“  íŒë‹¨ ë‹´ë‹¹
    - Function Callingìœ¼ë¡œ ë„êµ¬ ì‚¬ìš©
    - ë¶ˆí•„ìš”í•œ ë ˆì´ì–´ ì œê±°
    """
    message = request.message.strip()
    session_id = request.session_id or "default"

    if not message:
        raise HTTPException(status_code=400, detail="Message is required")

    # í¬ë¡¤ë§ ìƒíƒœ ì²´í¬
    crawl_manager = get_crawl_manager()
    crawl_notification = None
    crawl_started = False

    if crawl_manager.needs_crawl():
        crawl_started = await crawl_manager.start_crawl()

    if crawl_manager.should_notify(session_id):
        crawl_notification = crawl_manager.get_notification_message()
        crawl_manager.mark_notified(session_id)

    # Simple Chat Serviceë¡œ ì²˜ë¦¬
    chat_service = get_chat_service()
    result = await chat_service.chat(message, session_id)

    # í¬ë¡¤ë§ ì•Œë¦¼ ì¶”ê°€
    response_text = result["text"]
    if crawl_notification:
        response_text = f"{crawl_notification}\n\n---\n\n{response_text}"
    elif crawl_started:
        data_date = crawl_manager.get_data_date() or "ì—†ìŒ"
        response_text = (
            f"ğŸ“¡ **ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì˜¤ëŠ˜ ë°ì´í„° ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤.**\n"
            f"í˜„ì¬ ë°ì´í„°: {data_date}\n"
            f"ìˆ˜ì§‘ì´ ì™„ë£Œë˜ë©´ ì•Œë ¤ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n\n---\n\n{response_text}"
        )

    return SimpleChatResponse(
        text=response_text,
        suggestions=result.get("suggestions", []),
        tools_used=result.get("tools_used", []),
        sources=result.get("sources", []),  # AI ì¶œì²˜ ì •ë³´ ì „ë‹¬
        data_date=result.get("data_date", "N/A"),
        processing_time_ms=result.get("processing_time_ms", 0)
    )


# ============= LLM Orchestrator API (v2 - ê¸°ì¡´, deprecated) =============

class OrchestratorChatRequest(BaseModel):
    """LLM Orchestrator ì±—ë´‡ ìš”ì²­"""
    message: str
    session_id: Optional[str] = "default"
    skip_cache: bool = False


class OrchestratorChatResponse(BaseModel):
    """LLM Orchestrator ì±—ë´‡ ì‘ë‹µ"""
    text: str
    query_type: str
    confidence_level: str
    confidence_score: float
    sources: List[str]
    entities: Dict[str, Any]
    tools_called: List[str]
    suggestions: List[str]
    is_fallback: bool
    is_clarification: bool
    processing_time_ms: float


@app.post("/api/v2/chat", response_model=OrchestratorChatResponse)
@limiter.limit("30/minute")  # ë¶„ë‹¹ 30íšŒ ì œí•œ
async def chat_v2(request: OrchestratorChatRequest, req: Request):
    """
    í†µí•© ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ê¸°ë°˜ ì±—ë´‡ API (v2)

    ë™ì‘ íë¦„:
    1. ì§ˆë¬¸ ìˆ˜ì‹ 
    2. ì‹œìŠ¤í…œ ìƒíƒœ ì ê²€ (ë°ì´í„° ì‹ ì„ ë„, ì‚¬ìš© ê°€ëŠ¥ ì—ì´ì „íŠ¸)
    3. LLMì´ ìƒí™© íŒë‹¨ â†’ ì—ì´ì „íŠ¸ ì„ íƒ
    4. ì—ì´ì „íŠ¸ ì‹¤í–‰ (ì—ëŸ¬ ì‹œ ì „ëµì— ë”°ë¼ ì²˜ë¦¬)
    5. ì‘ë‹µ ìƒì„±

    ì—ëŸ¬ ì „ëµ:
    - RETRY: ì¬ì‹œë„ (ìµœëŒ€ 2íšŒ)
    - FALLBACK: ìºì‹œ ë°ì´í„° ì‚¬ìš©
    - SKIP: ê±´ë„ˆë›°ê³  ê³„ì†
    - ABORT: ì¤‘ë‹¨ + ì‚¬ìš©ì ì•Œë¦¼
    """
    import time
    start_time = time.time()

    message = request.message.strip()
    session_id = request.session_id or "default"

    if not message:
        raise HTTPException(status_code=400, detail="Message is required")

    # === í¬ë¡¤ë§ ìƒíƒœ ì²´í¬ ===
    crawl_manager = get_crawl_manager()
    crawl_notification = None
    crawl_started = False

    # ì˜¤ëŠ˜ ë°ì´í„°ê°€ ì—†ê³ , í¬ë¡¤ë§ ì¤‘ì´ ì•„ë‹ˆë©´ ì‹œì‘
    if crawl_manager.needs_crawl():
        crawl_started = await crawl_manager.start_crawl()
        if crawl_started:
            logging.info("Background crawl started for today's data")

    # í¬ë¡¤ë§ ì™„ë£Œ ì•Œë¦¼ ì²´í¬ (ì´ ì„¸ì…˜ì—ì„œ ì•„ì§ ì•ˆ ì•Œë ¸ìœ¼ë©´)
    if crawl_manager.should_notify(session_id):
        crawl_notification = crawl_manager.get_notification_message()
        crawl_manager.mark_notified(session_id)

    try:
        # í†µí•© ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ë¡œ ì²˜ë¦¬
        orchestrator = get_unified_orchestrator()

        # í˜„ì¬ ë©”íŠ¸ë¦­ ë°ì´í„° ë¡œë“œ
        data = load_dashboard_data()
        current_metrics = data if data else None

        # ì²˜ë¦¬
        response = await orchestrator.process(
            query=message,
            session_id=session_id,
            current_metrics=current_metrics,
            skip_cache=request.skip_cache
        )

        # ì‘ë‹µ í…ìŠ¤íŠ¸ êµ¬ì„±
        response_text = response.text

        # í¬ë¡¤ë§ ì•Œë¦¼ ì¶”ê°€
        if crawl_notification:
            response_text = f"{crawl_notification}\n\n---\n\n{response_text}"
        elif crawl_started:
            # í¬ë¡¤ë§ ì‹œì‘ ì•Œë¦¼
            data_date = crawl_manager.get_data_date() or "ì—†ìŒ"
            response_text = (
                f"ğŸ“¡ **ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì˜¤ëŠ˜ ë°ì´í„° ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤.**\n"
                f"í˜„ì¬ ë°ì´í„°: {data_date}\n"
                f"ìˆ˜ì§‘ì´ ì™„ë£Œë˜ë©´ ì•Œë ¤ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n\n---\n\n{response_text}"
            )

        # ì‘ë‹µ ë³€í™˜
        return OrchestratorChatResponse(
            text=response_text,
            query_type=response.query_type,
            confidence_level=response.confidence_level.value,
            confidence_score=response.confidence_score,
            sources=response.sources,
            entities=response.entities,
            tools_called=response.tools_called,
            suggestions=response.suggestions,
            is_fallback=response.is_fallback,
            is_clarification=response.is_clarification,
            processing_time_ms=response.processing_time_ms
        )

    except Exception as e:
        logging.error(f"Orchestrator error: {e}")
        return OrchestratorChatResponse(
            text=f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            query_type="error",
            confidence_level="unknown",
            confidence_score=0.0,
            sources=[],
            entities={},
            tools_called=[],
            suggestions=["ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”"],
            is_fallback=True,
            is_clarification=False,
            processing_time_ms=(time.time() - start_time) * 1000
        )


@app.get("/api/v2/stats")
async def get_orchestrator_stats():
    """í†µí•© ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° í†µê³„ ì¡°íšŒ"""
    orchestrator = get_unified_orchestrator()
    return orchestrator.get_stats()


@app.get("/api/v2/state")
async def get_orchestrator_state():
    """í†µí•© ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ìƒíƒœ ì¡°íšŒ"""
    orchestrator = get_unified_orchestrator()
    return {
        "summary": orchestrator.get_state_summary(),
        "state": orchestrator.state.to_dict()
    }


@app.get("/api/v2/errors")
async def get_orchestrator_errors():
    """í†µí•© ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ìµœê·¼ ì—ëŸ¬ ì¡°íšŒ"""
    orchestrator = get_unified_orchestrator()
    return {
        "recent_errors": orchestrator.get_recent_errors(limit=20),
        "stats": orchestrator.get_stats()
    }


@app.post("/api/v2/reset-errors")
async def reset_orchestrator_errors():
    """ì‹¤íŒ¨í•œ ì—ì´ì „íŠ¸ ëª©ë¡ ì´ˆê¸°í™”"""
    orchestrator = get_unified_orchestrator()
    orchestrator.reset_failed_agents()
    return {"status": "ok", "message": "Failed agents list cleared"}


@app.get("/api/crawl/status")
async def get_crawl_status():
    """
    í¬ë¡¤ë§ ìƒíƒœ ì¡°íšŒ

    Returns:
        - status: idle/running/completed/failed
        - date: í¬ë¡¤ë§ ëŒ€ìƒ ë‚ ì§œ
        - progress: ì§„í–‰ë¥  (0-100)
        - data_date: í˜„ì¬ ë°ì´í„° ë‚ ì§œ
        - needs_crawl: í¬ë¡¤ë§ í•„ìš” ì—¬ë¶€
    """
    crawl_manager = get_crawl_manager()
    return {
        **crawl_manager.state.to_dict(),
        "data_date": crawl_manager.get_data_date(),
        "needs_crawl": crawl_manager.needs_crawl(),
        "is_today_available": crawl_manager.is_today_data_available(),
        "status_message": crawl_manager.get_status_message()
    }


@app.post("/api/crawl/start", dependencies=[Depends(verify_api_key)])
async def start_crawl():
    """
    ìˆ˜ë™ìœ¼ë¡œ í¬ë¡¤ë§ ì‹œì‘ (API Key í•„ìš”)

    Returns:
        - started: í¬ë¡¤ë§ ì‹œì‘ ì—¬ë¶€
        - message: ìƒíƒœ ë©”ì‹œì§€
    """
    crawl_manager = get_crawl_manager()

    if crawl_manager.is_crawling():
        return {
            "started": False,
            "message": "í¬ë¡¤ë§ì´ ì´ë¯¸ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤.",
            "status": crawl_manager.state.to_dict()
        }

    if crawl_manager.is_today_data_available():
        return {
            "started": False,
            "message": "ì˜¤ëŠ˜ ë°ì´í„°ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.",
            "status": crawl_manager.state.to_dict()
        }

    started = await crawl_manager.start_crawl()
    return {
        "started": started,
        "message": "í¬ë¡¤ë§ì„ ì‹œì‘í–ˆìŠµë‹ˆë‹¤." if started else "í¬ë¡¤ë§ ì‹œì‘ ì‹¤íŒ¨",
        "status": crawl_manager.state.to_dict()
    }


# ============= Historical Data API =============

from src.tools.sheets_writer import SheetsWriter
from datetime import timedelta

# SheetsWriter ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_sheets_writer: Optional[SheetsWriter] = None


def get_sheets_writer() -> SheetsWriter:
    """SheetsWriter ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _sheets_writer
    if _sheets_writer is None:
        _sheets_writer = SheetsWriter()
    return _sheets_writer


@app.get("/api/historical")
async def get_historical_data(
    start_date: str,
    end_date: str,
    category_id: Optional[str] = None,
    brand: Optional[str] = "LANEIGE"
):
    """
    íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„° ì¡°íšŒ (Google Sheetsì—ì„œ)

    Args:
        start_date: ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)
        end_date: ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)
        category_id: ì¹´í…Œê³ ë¦¬ í•„í„° (ì„ íƒ)
        brand: ë¸Œëœë“œ í•„í„° (ê¸°ë³¸ê°’: LANEIGE)

    Returns:
        - data: ë‚ ì§œë³„ ì§€í‘œ ë°ì´í„°
        - sos_history: SoS ì¶”ì´ ë°ì´í„°
        - rank_history: ìˆœìœ„ ì¶”ì´ ë°ì´í„°
    """
    try:
        sheets_writer = get_sheets_writer()
        if not sheets_writer._initialized:
            await sheets_writer.initialize()

        # ë‚ ì§œ ë²”ìœ„ ê³„ì‚°
        start_dt = datetime.strptime(start_date, "%Y-%m-%d")
        end_dt = datetime.strptime(end_date, "%Y-%m-%d")
        days = (end_dt - start_dt).days + 1

        # Google Sheetsì—ì„œ íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„° ì¡°íšŒ
        records = await sheets_writer.get_rank_history(
            category_id=category_id,
            brand=brand,
            days=days
        )

        if not records:
            # Google Sheetsì— ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ë¡œì»¬ JSON íŒŒì¼ì—ì„œ ì‹œë„
            return await _get_historical_from_local(start_date, end_date, brand)

        # ë‚ ì§œë³„ ë°ì´í„° ì§‘ê³„
        daily_data = {}
        for record in records:
            snapshot_date = record.get("snapshot_date", "")
            if not snapshot_date or snapshot_date < start_date or snapshot_date > end_date:
                continue

            if snapshot_date not in daily_data:
                daily_data[snapshot_date] = {
                    "date": snapshot_date,
                    "products": [],
                    "total_count": 0,
                    "top10_count": 0
                }

            rank = int(record.get("rank", 0)) if record.get("rank") else 0
            daily_data[snapshot_date]["products"].append({
                "asin": record.get("asin", ""),
                "product_name": record.get("product_name", ""),
                "rank": rank,
                "price": record.get("price", ""),
                "rating": record.get("rating", "")
            })
            daily_data[snapshot_date]["total_count"] += 1
            if rank <= 10:
                daily_data[snapshot_date]["top10_count"] += 1

        # SoS ì¶”ì´ ê³„ì‚° (Top 100 ê¸°ì¤€)
        sos_history = []
        rank_history = []
        for date_str in sorted(daily_data.keys()):
            day_data = daily_data[date_str]
            products = day_data["products"]

            # SoS = (LANEIGE ì œí’ˆ ìˆ˜ / 100) * 100
            sos = round(len(products) / 100 * 100, 1) if products else 0
            sos_history.append({
                "date": date_str,
                "sos": sos,
                "product_count": len(products),
                "top10_count": day_data["top10_count"]
            })

            # í‰ê·  ìˆœìœ„ (ìˆëŠ” ê²½ìš°)
            if products:
                avg_rank = round(sum(p["rank"] for p in products) / len(products), 1)
                rank_history.append({
                    "date": date_str,
                    "rank": avg_rank,
                    "best_rank": min(p["rank"] for p in products),
                    "worst_rank": max(p["rank"] for p in products)
                })

        # available_dates ê³„ì‚°
        available_dates = sorted(daily_data.keys())

        # brand_metrics ê³„ì‚° (ì „ì²´ ê¸°ê°„ í†µí•© - ëª¨ë“  ë¸Œëœë“œ í¬í•¨)
        brand_metrics = await _calculate_brand_metrics_for_period(records, daily_data, brand)

        return {
            "success": True,
            "available_dates": available_dates,
            "brand_metrics": brand_metrics,
            "data": {
                "sos_history": sos_history,
                "rank_history": rank_history,
                "daily_data": list(daily_data.values()),
                "period": {
                    "start": start_date,
                    "end": end_date,
                    "days": days
                },
                "brand": brand
            }
        }

    except Exception as e:
        logging.error(f"Historical data error: {e}")
        # í´ë°±: ë¡œì»¬ ë°ì´í„°ì—ì„œ ì‹œë„
        return await _get_historical_from_local(start_date, end_date, brand)


async def _calculate_brand_metrics_for_period(
    records: List[Dict],
    daily_data: Dict,
    target_brand: str
) -> List[Dict]:
    """
    ê¸°ê°„ ë‚´ ëª¨ë“  ë¸Œëœë“œì˜ ë©”íŠ¸ë¦­ ê³„ì‚° (SoS Ã— Avg Rank ì°¨íŠ¸ìš©)

    Returns:
        ë¸Œëœë“œë³„ SoS, í‰ê·  ìˆœìœ„, ì œí’ˆ ìˆ˜ ë“±
    """
    # ì „ì²´ ì œí’ˆ ë°ì´í„° ì§‘ê³„ (ëª¨ë“  ë¸Œëœë“œ)
    brand_data = {}

    for record in records:
        brand_name = record.get("brand", "Unknown")
        rank = int(record.get("rank", 0)) if record.get("rank") else 0

        if not brand_name or rank == 0:
            continue

        if brand_name not in brand_data:
            brand_data[brand_name] = {
                "brand": brand_name,
                "ranks": [],
                "product_count": 0
            }

        brand_data[brand_name]["ranks"].append(rank)
        brand_data[brand_name]["product_count"] += 1

    # ì´ ì œí’ˆ ìˆ˜ (ëª¨ë“  ë¸Œëœë“œ)
    total_products = sum(b["product_count"] for b in brand_data.values())

    # ë©”íŠ¸ë¦­ ê³„ì‚°
    brand_metrics = []
    for brand_name, data in brand_data.items():
        if not data["ranks"]:
            continue

        sos = round(data["product_count"] / max(total_products, 100) * 100, 2)
        avg_rank = round(sum(data["ranks"]) / len(data["ranks"]), 1)

        # ë²„ë¸” í¬ê¸°: ì œí’ˆ ìˆ˜ ê¸°ë°˜ (ìµœì†Œ 5, ìµœëŒ€ 25)
        bubble_size = max(5, min(25, data["product_count"] * 2))

        brand_metrics.append({
            "brand": brand_name,
            "sos": sos,
            "avg_rank": avg_rank,
            "product_count": data["product_count"],
            "bubble_size": bubble_size,
            "is_laneige": target_brand.upper() in brand_name.upper()
        })

    # SoS ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬, ìƒìœ„ 10ê°œë§Œ
    brand_metrics.sort(key=lambda x: x["sos"], reverse=True)
    return brand_metrics[:10]


def _get_brand_metrics_from_dashboard(
    dashboard_data: Optional[Dict],
    target_brand: str
) -> List[Dict]:
    """
    ëŒ€ì‹œë³´ë“œ ë°ì´í„°ì—ì„œ ë¸Œëœë“œ ë©”íŠ¸ë¦­ ì¶”ì¶œ (ë¡œì»¬ í´ë°±ìš©)
    """
    if not dashboard_data:
        return []

    # ëŒ€ì‹œë³´ë“œì˜ brand_matrix ë°ì´í„° ì‚¬ìš©
    brand_matrix = dashboard_data.get("charts", {}).get("brand_matrix", [])
    if brand_matrix:
        return brand_matrix

    # ê²½ìŸì‚¬ ë°ì´í„°ì—ì„œ ìƒì„±
    competitors = dashboard_data.get("brand", {}).get("competitors", [])
    if not competitors:
        return []

    brand_metrics = []
    for comp in competitors:
        brand_metrics.append({
            "brand": comp.get("brand", "Unknown"),
            "sos": comp.get("sos", 0),
            "avg_rank": comp.get("avg_rank", 50),
            "product_count": comp.get("product_count", 0),
            "bubble_size": max(5, min(25, comp.get("product_count", 0) * 2)),
            "is_laneige": target_brand.upper() in comp.get("brand", "").upper()
        })

    return brand_metrics


async def _get_historical_from_local(
    start_date: str,
    end_date: str,
    brand: str = "LANEIGE"
) -> Dict[str, Any]:
    """
    ë¡œì»¬ JSON íŒŒì¼ì—ì„œ íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„° ì¡°íšŒ (í´ë°±)

    data/ í´ë”ì˜ ë‚ ì§œë³„ JSON íŒŒì¼ì´ë‚˜ dashboard_data.jsonì˜ íˆìŠ¤í† ë¦¬ ë°ì´í„° í™œìš©
    """
    try:
        # ë©”ì¸ ëŒ€ì‹œë³´ë“œ ë°ì´í„° ë¡œë“œ
        data = load_dashboard_data()
        sos_history = []
        rank_history = []

        # 1. ëŒ€ì‹œë³´ë“œ ë°ì´í„°ì—ì„œ í˜„ì¬ SoS/ìˆœìœ„ ì •ë³´ ì¶”ì¶œ
        if data:
            brand_kpis = data.get("brand", {}).get("kpis", {})
            current_sos = brand_kpis.get("sos", 0)
            data_date = data.get("metadata", {}).get("data_date", datetime.now().strftime("%Y-%m-%d"))

            # í˜„ì¬ ë‚ ì§œê°€ ìš”ì²­ ë²”ìœ„ì— í¬í•¨ë˜ë©´ ì¶”ê°€
            if start_date <= data_date <= end_date:
                sos_history.append({
                    "date": data_date,
                    "sos": current_sos,
                    "product_count": brand_kpis.get("product_count", 0),
                    "top10_count": brand_kpis.get("top10_count", 0)
                })

                avg_rank = brand_kpis.get("avg_rank", 0)
                if avg_rank:
                    rank_history.append({
                        "date": data_date,
                        "rank": avg_rank,
                        "best_rank": brand_kpis.get("best_rank", avg_rank),
                        "worst_rank": brand_kpis.get("worst_rank", avg_rank)
                    })

        # 2. latest_crawl_result.jsonì—ì„œ ë°ì´í„° ì¶”ì¶œ
        latest_crawl_path = Path("./data/latest_crawl_result.json")
        if latest_crawl_path.exists():
            try:
                with open(latest_crawl_path, "r", encoding="utf-8") as f:
                    crawl_data = json.load(f)

                # ëª¨ë“  ì¹´í…Œê³ ë¦¬ì—ì„œ ë¸Œëœë“œ ì œí’ˆ ì°¾ê¸°
                brand_products = []
                crawl_date = None

                for cat_id, cat_data in crawl_data.get("categories", {}).items():
                    for product in cat_data.get("products", []):
                        product_brand = product.get("brand", "")
                        product_name = product.get("product_name", "")

                        # ë¸Œëœë“œ ë§¤ì¹­ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ, ë¶€ë¶„ ë§¤ì¹­)
                        if brand.upper() in product_brand.upper() or brand.upper() in product_name.upper():
                            brand_products.append(product)
                            if not crawl_date:
                                crawl_date = product.get("snapshot_date")

                if brand_products and crawl_date and start_date <= crawl_date <= end_date:
                    # ì¤‘ë³µ ì œê±° í™•ì¸
                    if not any(h["date"] == crawl_date for h in sos_history):
                        # ì¹´í…Œê³ ë¦¬ë³„ ì´ ì œí’ˆ ìˆ˜ (Top 100 ê¸°ì¤€)
                        total_products = sum(
                            len(cat.get("products", []))
                            for cat in crawl_data.get("categories", {}).values()
                        )

                        sos = round(len(brand_products) / max(total_products, 100) * 100, 2)
                        avg_rank = round(sum(p.get("rank", 0) for p in brand_products) / len(brand_products), 1)

                        sos_history.append({
                            "date": crawl_date,
                            "sos": sos,
                            "product_count": len(brand_products),
                            "top10_count": sum(1 for p in brand_products if p.get("rank", 100) <= 10)
                        })
                        rank_history.append({
                            "date": crawl_date,
                            "rank": avg_rank,
                            "best_rank": min(p.get("rank", 100) for p in brand_products),
                            "worst_rank": max(p.get("rank", 100) for p in brand_products)
                        })

            except (json.JSONDecodeError, ValueError) as e:
                logging.warning(f"Failed to parse latest_crawl_result.json: {e}")

        # 3. raw_products í´ë”ì—ì„œ ë‚ ì§œë³„ ë°ì´í„° ê²€ìƒ‰ (ê¸°ì¡´ ë¡œì§)
        raw_data_dir = Path("./data/raw_products")
        if raw_data_dir.exists():
            for json_file in raw_data_dir.glob("*.json"):
                try:
                    file_date = json_file.stem  # íŒŒì¼ëª…ì´ YYYY-MM-DD í˜•ì‹ì´ë¼ê³  ê°€ì •
                    if start_date <= file_date <= end_date:
                        with open(json_file, "r", encoding="utf-8") as f:
                            daily_raw = json.load(f)

                        # ë¸Œëœë“œ ì œí’ˆë§Œ í•„í„°ë§
                        brand_products = [
                            p for p in daily_raw
                            if brand.upper() in p.get("brand", "").upper() or brand.upper() in p.get("product_name", "").upper()
                        ]

                        if brand_products:
                            sos = round(len(brand_products) / 100 * 100, 1)
                            avg_rank = round(sum(p.get("rank", 0) for p in brand_products) / len(brand_products), 1)

                            # ì¤‘ë³µ ì œê±°
                            if not any(h["date"] == file_date for h in sos_history):
                                sos_history.append({
                                    "date": file_date,
                                    "sos": sos,
                                    "product_count": len(brand_products),
                                    "top10_count": sum(1 for p in brand_products if p.get("rank", 100) <= 10)
                                })
                                rank_history.append({
                                    "date": file_date,
                                    "rank": avg_rank,
                                    "best_rank": min(p.get("rank", 100) for p in brand_products),
                                    "worst_rank": max(p.get("rank", 100) for p in brand_products)
                                })
                except (json.JSONDecodeError, ValueError):
                    continue

        # ë‚ ì§œìˆœ ì •ë ¬
        sos_history.sort(key=lambda x: x["date"])
        rank_history.sort(key=lambda x: x["date"])

        # available_dates ê³„ì‚°
        available_dates = [h["date"] for h in sos_history]

        # brand_metrics ê³„ì‚° (í˜„ì¬ ëŒ€ì‹œë³´ë“œ ë°ì´í„°ì—ì„œ)
        brand_metrics = _get_brand_metrics_from_dashboard(data, brand)

        if not sos_history:
            return {
                "success": False,
                "error": "No historical data found for the specified period",
                "available_dates": [],
                "brand_metrics": [],
                "data": None
            }

        return {
            "success": True,
            "available_dates": available_dates,
            "brand_metrics": brand_metrics,
            "data": {
                "sos_history": sos_history,
                "rank_history": rank_history,
                "period": {
                    "start": start_date,
                    "end": end_date
                },
                "brand": brand,
                "source": "local"
            }
        }

    except Exception as e:
        logging.error(f"Local historical data error: {e}")
        return {
            "success": False,
            "error": str(e),
            "available_dates": [],
            "brand_metrics": [],
            "data": None
        }


@app.post("/api/export/docx")
async def export_docx(request: ExportRequest):
    """
    ì¸ì‚¬ì´íŠ¸ ë¦¬í¬íŠ¸ DOCX ìƒì„± ë° ë‹¤ìš´ë¡œë“œ
    """
    data = load_dashboard_data()
    if not data:
        raise HTTPException(status_code=404, detail="Dashboard data not found")

    # DOCX ë¬¸ì„œ ìƒì„±
    doc = Document()

    # ìŠ¤íƒ€ì¼ ì„¤ì •
    style = doc.styles['Normal']
    font = style.font
    font.name = 'Arial'
    font.size = Pt(11)

    # ===== í‘œì§€ =====
    title = doc.add_heading('AMORE INSIGHT Report', 0)
    title.alignment = WD_ALIGN_PARAGRAPH.CENTER

    subtitle = doc.add_paragraph('LANEIGE Amazon US ë¶„ì„ ë¦¬í¬íŠ¸')
    subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER

    # ë‚ ì§œ
    metadata = data.get("metadata", {})
    date_para = doc.add_paragraph()
    date_para.alignment = WD_ALIGN_PARAGRAPH.CENTER
    date_para.add_run(f"ë¶„ì„ ê¸°ì¤€ì¼: {metadata.get('data_date', datetime.now().strftime('%Y-%m-%d'))}")
    date_para.add_run(f"\nìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M')}")

    doc.add_page_break()

    # ===== 1. Executive Summary =====
    doc.add_heading('1. Executive Summary', level=1)

    brand_kpis = data.get("brand", {}).get("kpis", {})
    home_status = data.get("home", {}).get("status", {})

    summary_text = f"""
LANEIGE ë¸Œëœë“œëŠ” Amazon US ì‹œì¥ì—ì„œ {home_status.get('exposure', 'N/A')} ìƒíƒœì…ë‹ˆë‹¤.

â€¢ Share of Shelf (SoS): {brand_kpis.get('sos', 0)}%
â€¢ Top 10 ì§„ì… ì œí’ˆ: {brand_kpis.get('top10_count', 0)}ê°œ
â€¢ í‰ê·  ìˆœìœ„: {brand_kpis.get('avg_rank', 0)}ìœ„
â€¢ ì‹œì¥ ì§‘ì¤‘ë„ (HHI): {brand_kpis.get('hhi', 0)}

í˜„ì¬ ì‹œì¥ í¬ì§€ì…˜: {home_status.get('position', 'N/A')}
ì£¼ì˜ í•„ìš” ì œí’ˆ: {home_status.get('warning_count', 0)}ê°œ
"""
    doc.add_paragraph(summary_text)

    # ===== 2. ì œí’ˆë³„ í˜„í™© =====
    doc.add_heading('2. LANEIGE ì œí’ˆ í˜„í™©', level=1)

    products = data.get("products", {})
    if products:
        # í…Œì´ë¸” ìƒì„±
        table = doc.add_table(rows=1, cols=5)
        table.style = 'Table Grid'
        table.alignment = WD_TABLE_ALIGNMENT.CENTER

        # í—¤ë”
        header_cells = table.rows[0].cells
        headers = ['ì œí’ˆëª…', 'ìˆœìœ„', 'ë³€ë™', 'í‰ì ', 'ë³€ë™ì„±']
        for i, header in enumerate(headers):
            header_cells[i].text = header
            header_cells[i].paragraphs[0].runs[0].bold = True

        # ë°ì´í„° í–‰
        for asin, product in products.items():
            row = table.add_row().cells
            row[0].text = product.get('name', '')[:40]
            row[1].text = f"#{product.get('rank', 'N/A')}"
            row[2].text = product.get('rank_delta', '-')
            row[3].text = str(product.get('rating', '-'))
            row[4].text = product.get('volatility_status', '-')

    doc.add_paragraph()

    # ===== 3. ê²½ìŸì‚¬ ë¶„ì„ =====
    doc.add_heading('3. ê²½ìŸì‚¬ ë¶„ì„', level=1)

    competitors = data.get("brand", {}).get("competitors", [])
    if competitors:
        table = doc.add_table(rows=1, cols=4)
        table.style = 'Table Grid'

        header_cells = table.rows[0].cells
        headers = ['ë¸Œëœë“œ', 'SoS (%)', 'í‰ê·  ìˆœìœ„', 'ì œí’ˆ ìˆ˜']
        for i, header in enumerate(headers):
            header_cells[i].text = header
            header_cells[i].paragraphs[0].runs[0].bold = True

        for comp in competitors[:10]:
            row = table.add_row().cells
            row[0].text = comp.get('brand', '')
            row[1].text = str(comp.get('sos', 0))
            row[2].text = str(comp.get('avg_rank', '-'))
            row[3].text = str(comp.get('product_count', 0))

    doc.add_paragraph()

    # ===== 4. ì•¡ì…˜ ì•„ì´í…œ =====
    doc.add_heading('4. ì•¡ì…˜ ì•„ì´í…œ', level=1)

    action_items = data.get("home", {}).get("action_items", [])
    if action_items:
        for item in action_items:
            priority_marker = "ğŸ”´" if item.get('priority') == 'P1' else "ğŸŸ "
            para = doc.add_paragraph()
            para.add_run(f"{priority_marker} [{item.get('priority')}] ").bold = True
            para.add_run(f"{item.get('product_name', '')}\n")
            para.add_run(f"   ì‹ í˜¸: {item.get('signal', '')}\n")
            para.add_run(f"   ê¶Œì¥ ì•¡ì…˜: {item.get('action_tag', '')}")
    else:
        doc.add_paragraph("í˜„ì¬ íŠ¹ë³„í•œ ì•¡ì…˜ ì•„ì´í…œì´ ì—†ìŠµë‹ˆë‹¤.")

    # ===== 5. ì „ëµì  ê¶Œê³ ì‚¬í•­ =====
    if request.include_strategy:
        doc.add_heading('5. ì „ëµì  ê¶Œê³ ì‚¬í•­', level=1)

        # ChatGPTë¡œ ì „ëµ ìƒì„± (RAG ì»¨í…ìŠ¤íŠ¸ í™œìš©)
        try:
            # RAGì—ì„œ ì „ëµ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
            strategy_context, _ = await get_rag_context("ì „ëµ ì•¡ì…˜ ê¶Œê³ ", QueryType.ANALYSIS)

            strategy_prompt = f"""ë‹¤ìŒ ë°ì´í„°ì™€ ê°€ì´ë“œë¼ì¸ì„ ë°”íƒ•ìœ¼ë¡œ LANEIGE ë¸Œëœë“œì˜ ì „ëµì  ê¶Œê³ ì‚¬í•­ 3ê°€ì§€ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

ë°ì´í„°:
- SoS: {brand_kpis.get('sos', 0)}%
- Top 10 ì œí’ˆ: {brand_kpis.get('top10_count', 0)}ê°œ
- í‰ê·  ìˆœìœ„: {brand_kpis.get('avg_rank', 0)}ìœ„
- ì£¼ìš” ê²½ìŸì‚¬: {', '.join([c['brand'] for c in competitors[:3]])}

ì°¸ê³  ê°€ì´ë“œë¼ì¸:
{strategy_context if strategy_context else 'ê¸°ë³¸ ì „ëµ ê¸°ì¤€ ì ìš©'}

ê° ê¶Œê³ ì‚¬í•­ì€ 1-2ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
"""
            response = await acompletion(
                model="gpt-4.1-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ë·°í‹° ì´ì»¤ë¨¸ìŠ¤ ì „ë¬¸ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤. ê°„ê²°í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì „ëµì„ ì œì•ˆí•©ë‹ˆë‹¤."},
                    {"role": "user", "content": strategy_prompt}
                ],
                temperature=0.3,
                max_tokens=500
            )

            strategy_text = response.choices[0].message.content
            doc.add_paragraph(strategy_text)

        except Exception as e:
            # í´ë°± ì „ëµ
            doc.add_paragraph("""
1. Top 10 ìœ ì§€ ì „ëµ: í˜„ì¬ ìƒìœ„ê¶Œ ì œí’ˆì˜ ë¦¬ë·° ê´€ë¦¬ ë° ì¬ê³  í™•ë³´ë¥¼ í†µí•œ í¬ì§€ì…˜ ìœ ì§€

2. ê²½ìŸì‚¬ ëª¨ë‹ˆí„°ë§: e.l.f., Maybelline ë“± ì£¼ìš” ê²½ìŸì‚¬ì˜ ê°€ê²© ë° í”„ë¡œëª¨ì…˜ ë™í–¥ íŒŒì•…

3. ì‹ ê·œ ì§„ì… ê¸°íšŒ: Lip Care ì¹´í…Œê³ ë¦¬ ì™¸ Face Powder, Toner ë“± í™•ì¥ ê°€ëŠ¥ì„± ê²€í† 
""")

    # ===== í‘¸í„° =====
    doc.add_paragraph()
    footer = doc.add_paragraph()
    footer.alignment = WD_ALIGN_PARAGRAPH.CENTER
    footer.add_run("Â© 2025 AMORE Pacific - Confidential").italic = True

    # BytesIOë¡œ ì €ì¥
    buffer = BytesIO()
    doc.save(buffer)
    buffer.seek(0)

    # íŒŒì¼ëª… ìƒì„±
    filename = f"AMORE_Insight_Report_{datetime.now().strftime('%Y%m%d_%H%M')}.docx"

    return StreamingResponse(
        buffer,
        media_type="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
        headers={"Content-Disposition": f"attachment; filename={filename}"}
    )


# ============= Alert Settings API =============

from src.core.state_manager import StateManager, get_state_manager

# ì‹±ê¸€í†¤ State Manager
_state_manager: Optional[StateManager] = None

def get_app_state_manager() -> StateManager:
    """ì•± ë ˆë²¨ State Manager ë°˜í™˜"""
    global _state_manager
    if _state_manager is None:
        _state_manager = get_state_manager()
    return _state_manager


class AlertSettingsRequest(BaseModel):
    """ì•Œë¦¼ ì„¤ì • ìš”ì²­"""
    email: str
    consent: bool
    alert_types: List[str] = []


class AlertSettingsResponse(BaseModel):
    """ì•Œë¦¼ ì„¤ì • ì‘ë‹µ"""
    email: str
    consent: bool
    alert_types: List[str]
    consent_date: Optional[str] = None


@app.get("/api/v3/alert-settings")
async def get_alert_settings():
    """
    í˜„ì¬ ì•Œë¦¼ ì„¤ì • ì¡°íšŒ

    ì°¸ê³ : í˜„ì¬ëŠ” ë‹¨ì¼ ì‚¬ìš©ì ì„¤ì •ë§Œ ì§€ì› (ì²« ë²ˆì§¸ ë“±ë¡ëœ ì´ë©”ì¼)
    """
    state_manager = get_app_state_manager()
    subscriptions = state_manager.get_all_subscriptions()

    if not subscriptions:
        return {
            "email": "",
            "consent": False,
            "alert_types": [],
            "consent_date": None
        }

    # ì²« ë²ˆì§¸ êµ¬ë… ë°˜í™˜
    email, sub = next(iter(subscriptions.items()))
    return {
        "email": email,
        "consent": sub.consent,
        "alert_types": sub.alert_types,
        "consent_date": sub.consent_date.isoformat() if sub.consent_date else None
    }


@app.post("/api/v3/alert-settings", dependencies=[Depends(verify_api_key)])
async def save_alert_settings(request: AlertSettingsRequest):
    """
    ì•Œë¦¼ ì„¤ì • ì €ì¥ (API Key í•„ìš”)

    ì¤‘ìš”: consentê°€ Trueì¼ ë•Œë§Œ ì´ë©”ì¼ ë“±ë¡
    """
    state_manager = get_app_state_manager()

    if not request.email:
        raise HTTPException(status_code=400, detail="ì´ë©”ì¼ ì£¼ì†Œê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    if request.consent:
        # ì´ë©”ì¼ ë“±ë¡ (ëª…ì‹œì  ë™ì˜)
        success = state_manager.register_email(
            email=request.email,
            consent=True,
            alert_types=request.alert_types
        )

        if not success:
            raise HTTPException(status_code=400, detail="ì´ë©”ì¼ ë“±ë¡ ì‹¤íŒ¨")

        return {"status": "ok", "message": "ì•Œë¦¼ ì„¤ì •ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤."}
    else:
        # ë™ì˜ ì—†ìœ¼ë©´ ì—…ë°ì´íŠ¸ë§Œ (ì•Œë¦¼ ìœ í˜• ë³€ê²½)
        success = state_manager.update_email_subscription(
            email=request.email,
            alert_types=request.alert_types
        )

        return {"status": "ok", "message": "ì„¤ì •ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤."}


@app.post("/api/v3/alert-settings/revoke", dependencies=[Depends(verify_api_key)])
async def revoke_alert_consent():
    """
    ì•Œë¦¼ ë™ì˜ ì² íšŒ (API Key í•„ìš”)

    ì²« ë²ˆì§¸ ë“±ë¡ëœ ì´ë©”ì¼ì˜ ë™ì˜ë¥¼ ì² íšŒí•©ë‹ˆë‹¤.
    """
    state_manager = get_app_state_manager()
    subscriptions = state_manager.get_all_subscriptions()

    if not subscriptions:
        return {"status": "ok", "message": "ì² íšŒí•  ë™ì˜ê°€ ì—†ìŠµë‹ˆë‹¤."}

    # ì²« ë²ˆì§¸ ì´ë©”ì¼ ì² íšŒ
    email = next(iter(subscriptions.keys()))
    state_manager.revoke_email_consent(email)

    return {"status": "ok", "message": "ë™ì˜ê°€ ì² íšŒë˜ì—ˆìŠµë‹ˆë‹¤."}


@app.get("/api/v3/alerts")
async def get_alerts(limit: int = 50, alert_type: Optional[str] = None):
    """
    ì•Œë¦¼ ëª©ë¡ ì¡°íšŒ

    Args:
        limit: ìµœëŒ€ ê°œìˆ˜
        alert_type: í•„í„°í•  ì•Œë¦¼ ìœ í˜•
    """
    from src.agents.alert_agent import AlertAgent

    state_manager = get_app_state_manager()
    alert_agent = AlertAgent(state_manager)

    return {
        "alerts": alert_agent.get_alerts(limit=limit, alert_type=alert_type),
        "pending_count": alert_agent.get_pending_count(),
        "stats": alert_agent.get_stats()
    }


# ============= ëŒ€ì‹œë³´ë“œ HTML ì„œë¹™ =============

@app.get("/dashboard")
async def serve_dashboard():
    """ëŒ€ì‹œë³´ë“œ HTML í˜ì´ì§€ ì„œë¹™"""
    dashboard_path = Path("./dashboard/amore_unified_dashboard_v4.html")
    if not dashboard_path.exists():
        raise HTTPException(status_code=404, detail="Dashboard not found")
    return FileResponse(dashboard_path, media_type="text/html")


@app.get("/api/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}


# ============= Level 4 Brain API (v4) =============

class BrainChatRequest(BaseModel):
    """Brain ì±—ë´‡ ìš”ì²­"""
    message: str
    session_id: Optional[str] = "default"
    skip_cache: bool = False


class BrainChatResponse(BaseModel):
    """Brain ì±—ë´‡ ì‘ë‹µ"""
    text: str
    confidence: float
    sources: List[str]
    reasoning: Optional[str] = None
    tools_used: List[str]
    processing_time_ms: float
    from_cache: bool
    brain_mode: str


@app.post("/api/v4/chat", response_model=BrainChatResponse)
@limiter.limit("30/minute")  # ë¶„ë‹¹ 30íšŒ ì œí•œ
async def chat_v4(request: BrainChatRequest, req: Request):
    """
    Level 4 Brain ê¸°ë°˜ ì±—ë´‡ API (v4)

    LLM-First ì ‘ê·¼:
    - ëª¨ë“  íŒë‹¨ì„ LLMì´ ìˆ˜í–‰
    - ê·œì¹™ ê¸°ë°˜ ë¹ ë¥¸ ê²½ë¡œ ì—†ìŒ
    - RAG + KG í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
    - ììœ¨ ìŠ¤ì¼€ì¤„ëŸ¬ì™€ í†µí•©
    """
    import time
    start_time = time.time()

    message = request.message.strip()
    session_id = request.session_id or "default"

    if not message:
        raise HTTPException(status_code=400, detail="Message is required")

    try:
        # Brain ì¸ìŠ¤í„´ìŠ¤ íšë“
        brain = await get_initialized_brain()

        # í˜„ì¬ ë©”íŠ¸ë¦­ ë°ì´í„° ë¡œë“œ
        data = load_dashboard_data()
        current_metrics = data if data else None

        # Brainìœ¼ë¡œ ì²˜ë¦¬ (LLM-First)
        response = await brain.process_query(
            query=message,
            session_id=session_id,
            current_metrics=current_metrics,
            skip_cache=request.skip_cache
        )

        processing_time = (time.time() - start_time) * 1000

        return BrainChatResponse(
            text=response.content,
            confidence=response.confidence,
            sources=response.sources,
            reasoning=response.reasoning,
            tools_used=response.tools_called if hasattr(response, 'tools_called') else [],
            processing_time_ms=processing_time,
            from_cache=response.from_cache if hasattr(response, 'from_cache') else False,
            brain_mode=brain.mode.value
        )

    except Exception as e:
        logging.error(f"Brain error: {e}")
        return BrainChatResponse(
            text=f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            confidence=0.0,
            sources=[],
            reasoning=None,
            tools_used=[],
            processing_time_ms=(time.time() - start_time) * 1000,
            from_cache=False,
            brain_mode="error"
        )


@app.get("/api/v4/brain/status")
async def get_brain_status():
    """
    Brain ìƒíƒœ ì¡°íšŒ

    Returns:
        - mode: í˜„ì¬ Brain ëª¨ë“œ
        - scheduler: ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ
        - pending_tasks: ëŒ€ê¸° ì¤‘ íƒœìŠ¤í¬
        - stats: í†µê³„
    """
    try:
        brain = await get_initialized_brain()

        return {
            "mode": brain.mode.value,
            "scheduler_running": brain.scheduler.running if brain.scheduler else False,
            "pending_tasks": brain.scheduler.get_pending_count() if brain.scheduler else 0,
            "stats": brain.get_stats(),
            "initialized": True
        }
    except Exception as e:
        return {
            "mode": "uninitialized",
            "scheduler_running": False,
            "pending_tasks": 0,
            "stats": {},
            "initialized": False,
            "error": str(e)
        }


@app.post("/api/v4/brain/scheduler/start", dependencies=[Depends(verify_api_key)])
async def start_brain_scheduler():
    """
    ììœ¨ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ (API Key í•„ìš”)

    - ì¼ì¼ í¬ë¡¤ë§ (09:00)
    - ì£¼ê¸°ì  ì•Œë¦¼ ì²´í¬ (30ë¶„)
    - ë°±ê·¸ë¼ìš´ë“œ ë¶„ì„
    """
    try:
        brain = await get_initialized_brain()

        if brain.scheduler and brain.scheduler.running:
            return {
                "started": False,
                "message": "ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.",
                "status": "running"
            }

        await brain.start_scheduler()

        return {
            "started": True,
            "message": "ììœ¨ ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "status": "running"
        }
    except Exception as e:
        return {
            "started": False,
            "message": f"ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ ì‹¤íŒ¨: {str(e)}",
            "status": "error"
        }


@app.post("/api/v4/brain/scheduler/stop", dependencies=[Depends(verify_api_key)])
async def stop_brain_scheduler():
    """ììœ¨ ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€ (API Key í•„ìš”)"""
    try:
        brain = await get_initialized_brain()

        if brain.scheduler:
            brain.scheduler.stop()

        return {
            "stopped": True,
            "message": "ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "status": "stopped"
        }
    except Exception as e:
        return {
            "stopped": False,
            "message": f"ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€ ì‹¤íŒ¨: {str(e)}",
            "status": "error"
        }


@app.post("/api/v4/brain/autonomous-cycle", dependencies=[Depends(verify_api_key)])
async def run_autonomous_cycle():
    """
    ììœ¨ ì‚¬ì´í´ ìˆ˜ë™ ì‹¤í–‰ (API Key í•„ìš”)

    1. ë°ì´í„° ì‹ ì„ ë„ í™•ì¸
    2. í•„ìš”ì‹œ í¬ë¡¤ë§
    3. ì§€í‘œ ê³„ì‚°
    4. ì•Œë¦¼ ì¡°ê±´ ì²´í¬
    5. ì¸ì‚¬ì´íŠ¸ ìƒì„±
    """
    try:
        brain = await get_initialized_brain()
        result = await brain.run_autonomous_cycle()

        return {
            "success": True,
            "result": result
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }


@app.post("/api/v4/brain/check-alerts")
async def check_brain_alerts():
    """
    ì•Œë¦¼ ì¡°ê±´ ìˆ˜ë™ ì²´í¬

    í˜„ì¬ ë©”íŠ¸ë¦­ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì•Œë¦¼ ì¡°ê±´ì„ ì²´í¬í•©ë‹ˆë‹¤.
    """
    try:
        brain = await get_initialized_brain()
        data = load_dashboard_data()

        if not data:
            return {
                "alerts": [],
                "message": "ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
            }

        alerts = await brain.check_alerts(data)

        return {
            "alerts": alerts,
            "count": len(alerts),
            "checked_at": datetime.now().isoformat()
        }
    except Exception as e:
        return {
            "alerts": [],
            "error": str(e)
        }


@app.get("/api/v4/brain/stats")
async def get_brain_stats():
    """Brain í†µê³„ ì¡°íšŒ"""
    try:
        brain = await get_initialized_brain()
        return brain.get_stats()
    except Exception as e:
        return {"error": str(e)}


@app.post("/api/v4/brain/mode", dependencies=[Depends(verify_api_key)])
async def set_brain_mode(mode: str):
    """
    Brain ëª¨ë“œ ë³€ê²½ (API Key í•„ìš”)

    Args:
        mode: reactive, proactive, autonomous
    """
    try:
        brain = await get_initialized_brain()

        mode_map = {
            "reactive": BrainMode.REACTIVE,
            "proactive": BrainMode.PROACTIVE,
            "autonomous": BrainMode.AUTONOMOUS
        }

        if mode not in mode_map:
            raise HTTPException(
                status_code=400,
                detail=f"Invalid mode. Valid modes: {list(mode_map.keys())}"
            )

        brain.mode = mode_map[mode]

        return {
            "mode": brain.mode.value,
            "message": f"Brain ëª¨ë“œê°€ {mode}(ìœ¼)ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤."
        }
    except HTTPException:
        raise
    except Exception as e:
        return {"error": str(e)}


# ============= ì„œë²„ ì‹¤í–‰ =============

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)
